{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the pdf\n",
    "2. Chunk that pdf (split that into pieces)\n",
    "3. Embed each piece\n",
    "4. Create the vector database, index\n",
    "5. Query (retrieving from that vector database using a llama2 model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "\n",
    "folder_path = \"./pdfs/instruction-tune-llama2-extended-guide.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"07/04/2024, 12:41 Extended Guide: Instruction-tune Llama 2\\nhttps://www.philschmid.de/instruction-tune-llama-2 1/17philschmidBlogNewsletterTagsProjectsAbout MeContact\\nExtended Guide: Instruction-tune Llama\\n2\\n#GENERATIVEAI#HUGGINGFACE#LLM#LLAMA\\nJuly 26, 2023\\n13 min read\\nView CodeThis blog post is an extended guide on instruction-tuning Llama 2 from\\nMeta AI. The idea of the blog post is to focus on creating the\\ninstruction dataset, which we can then use to fine-tune the base model\\nof Llama 2 to follow our instructions.\\nThe goal is to create a model which can create instructions based on\\ninput. The idea behind this is that this can then be used for others to\\ncreate instruction data from inputs. That's especially helpful if you\\nwant to personalize models for, e.g., tweeting, email writing, etc,\\nwhich means that you would be able to generate an instruction dataset\\nfrom your emails to then train a model to mimic your email writing.\\nOkay, so can we get started on this? In the blog, we are going to:\\n1. Define the use case and create a prompt template for instructions\", metadata={'source': './pdfs/instruction-tune-llama2-extended-guide.pdf', 'page': 0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_docs = PyPDFLoader(folder_path).load_and_split()\n",
    "pdf_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding and Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OllamaEmbeddings(model=\"llama2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = Chroma.from_documents(pdf_docs, embedding=embedding, persist_directory=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"llama2\", chat_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?\", response_metadata={'model': 'llama2', 'created_at': '2024-04-09T18:52:59.617943Z', 'message': {'role': 'assistant', 'content': ''}, 'done': True, 'total_duration': 2315489250, 'load_duration': 2863625, 'prompt_eval_count': 20, 'prompt_eval_duration': 1633189000, 'eval_count': 26, 'eval_duration': 676573000}, id='run-532abd7a-4afb-4966-9ee6-30f340c092f1-0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=llm, retriever=vector_db.as_retriever(), return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = qa.invoke(\"What is an instruction in the context of LLMs? Cite a passage from the uploaded document related to instructions.\", return_sources=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is an instruction in the context of LLMs? Cite a passage from the uploaded document related to instructions.',\n",
       " 'result': 'According to the provided document, an instruction in the context of LLMs refers to a set of parameters or configuration options that are used to train and fine-tune a language model (LLM). The document provides examples of instructions for various tasks, including creating an instruction dataset and preparing a model for kbit training.\\n\\nOne passage from the document that relates to instructions is:\\n\\n\"The SFTTrainer supports a native integration with peft, which makes it super easy to efficiently instruction tune LLMs.\"\\n\\nThis passage highlights the ability of the SFTTrainer to integrate with peft (Packing and Efficient Training) to perform efficient instruction tuning of LLMs. The term \"instruction\" in this context refers to the configuration options or parameters that are used to train and fine-tune the LLM using the SFTTrainer.',\n",
       " 'source_documents': [Document(page_content='07/04/2024, 12:41 Extended Guide: Instruction-tune Llama 2\\nhttps://www.philschmid.de/instruction-tune-llama-2 17/17Thanks for reading! If you have any questions, feel free to contact me\\non Twitter or LinkedIn.\\nDiscuss on Twitter \\x00 View on GitHub\\nPhilipp Schmid•© 2024•philschmid blog••Imprint', metadata={'page': 16, 'source': './pdfs/instruction-tune-llama2-extended-guide.pdf'}),\n",
       "  Document(page_content='07/04/2024, 12:41 Extended Guide: Instruction-tune Llama 2\\nhttps://www.philschmid.de/instruction-tune-llama-2 12/17We now have every building block we need to create our SFTTrainer\\nto start then training our model.    save_strategy =\"epoch\",\\n    learning_rate =2e-4,\\n    bf16 =True,\\n    tf32 =True,\\n    max_grad_norm =0.3,\\n    warmup_ratio =0.03,\\n    lr_scheduler_type =\"constant\" ,\\n    disable_tqdm =True # disable tqdm since with packing values are in\\n)\\nfrom trl import SFTTrainer\\nmax_seq_length = 2048 # max sequence length for model and packing of \\ntrainer = SFTTrainer (\\n    model =model,\\n    train_dataset =dataset,\\n    peft_config =peft_config ,\\n    max_seq_length =max_seq_length ,\\n    tokenizer =tokenizer ,\\n    packing =True,\\n    formatting_func =format_instruction ,\\n    args =args,\\n)', metadata={'page': 11, 'source': './pdfs/instruction-tune-llama2-extended-guide.pdf'}),\n",
       "  Document(page_content='07/04/2024, 12:41 Extended Guide: Instruction-tune Llama 2\\nhttps://www.philschmid.de/instruction-tune-llama-2 10/17The SFTTrainer supports a native integration with peft, which\\nmakes it super easy to efficiently instruction tune LLMs. We only need\\nto create our LoRAConfig and provide it to the trainer.# model_id = \"meta-llama/Llama-2-7b-hf\" # gated\\n# BitsAndBytesConfig int-4 config\\nbnb_config = BitsAndBytesConfig (\\n    load_in_4bit =True, bnb_4bit_use_double_quant =True, bnb_4bit_quant\\n)\\n# Load model and tokenizer\\nmodel = AutoModelForCausalLM .from_pretrained (\\n    model_id ,\\n    quantization_config =bnb_config ,\\n    use_cache =False,\\n    use_flash_attention_2 =use_flash_attention ,\\n    device_map =\"auto\",\\n)\\nmodel.config.pretraining_tp = 1\\ntokenizer = AutoTokenizer .from_pretrained (model_id )\\ntokenizer .pad_token = tokenizer .eos_token\\ntokenizer .padding_side = \"right\"\\nfrom peft import LoraConfig , prepare_model_for_kbit_training , get_pef', metadata={'page': 9, 'source': './pdfs/instruction-tune-llama2-extended-guide.pdf'}),\n",
       "  Document(page_content=\"07/04/2024, 12:41 Extended Guide: Instruction-tune Llama 2\\nhttps://www.philschmid.de/instruction-tune-llama-2 4/17Converting the idea into a basic prompt template following the Alpaca\\nformat we get.\\n2. Create an instruction dataset### Instruction:\\nUse the Input below to create an instruction , which could have been u\\n### Input:\\nDear [boss name ],\\nI'm writing to request next week, August 1st through August 4th ,\\noff as paid time off .\\nI have some personal matters to attend to that week that require\\nme to be out of the office . I wanted to give you as much advance\\nnotice as possible so you can plan accordingly while I am away .\\nPlease let me know if you need any additional information from me\\nor have any concerns with me taking next week off . I appreciate you\\nconsidering this request .\\nThank you , [Your name ]\\n### Response:\\nWrite an email to my boss that I need next week 08/01 - 08/04 off.\", metadata={'page': 3, 'source': './pdfs/instruction-tune-llama2-extended-guide.pdf'})]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do a simple semi-manual benchmark for the quality of this rag setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean latency in seconds: 11.501888925378973\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "queries = [\n",
    "\"What is the definition of an instruction for LLM models?\",\n",
    "\"How can instructions guide and constrain the output of LLM models?\",\n",
    "\"What are some examples of instructions for different capabilities of LLMs, such as Brainstorming,\",\n",
    "\"What is the goal of fine-tuning a model to generate instructions based on input?\",\n",
    "\"How can synthetic instruction datasets be created for personalizing LLMs and agents?\",\n",
    "\"How to define a use case and create a prompt template for instructions?\",\n",
    "\"What are the steps to create an instruction dataset, including defining the input and output formats?\",\n",
    "\"What research suggests about creating high-quality instruction datasets?\",\n",
    "\"What are some methods to create an instruction dataset, such as using existing datasets or synthetically generating new ones?\",\n",
    "\"How to use existing LLMs to create synthetic instruction datasets?\"\n",
    "]\n",
    "\n",
    "outputs = []\n",
    "latencies = []\n",
    "for query in queries:\n",
    "    start = time.time()\n",
    "    output = qa.invoke(query)\n",
    "    outputs.append(output)\n",
    "    end = time.time()\n",
    "    latencies.append(end - start)\n",
    "\n",
    "mean_latency = np.mean(latencies)\n",
    "print(f\"Mean latency in seconds: {mean_latency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to use existing LLMs to create synthetic instruction datasets?\n",
      " To create synthetic instruction datasets using existing LLMs, you can follow these general steps:\n",
      "\n",
      "1. Define the use case and create a prompt template for instructions.\n",
      "2. Create an instruction dataset by providing inputs and corresponding desired outputs in the form of instructions. For instance, you can use the email request example provided in the guide as a template and modify it to generate different types of instructions such as writing a letter of recommendation or composing a business proposal.\n",
      "3. Instantiate the LLM and fine-tune it using the instruction dataset, creating an adapter for the model with the help of libraries like Triggers (trl) or the SFTTrainer.\n",
      "4. Test the model's performance on generating relevant instructions based on new inputs.\n",
      "5. Merge the adapter weights into the base model and save the merged model for further use.\n",
      "6. Optionally, push the merged model to a shared hub for public access.\n",
      "\n",
      "By following these steps, you can create synthetic instruction datasets using existing LLMs, enabling more personalized and productive interactions with various models.\n",
      "[Document(page_content=\"07/04/2024, 12:41 Extended Guide: Instruction-tune Llama 2\\nhttps://www.philschmid.de/instruction-tune-llama-2 3/17Capability Example Instruction\\nBrainstorming Provide a diverse set of creative ideas for new flavors of ice\\ncream.\\nClassification Categorize these movies as either comedy, drama, or\\nhorror based on the plot summa ry.\\nClosed QA Answer the question 'What is the capital of France?' with a\\nsingle word.\\nGeneration Write a poem in the style of Robert Frost about nature and\\nthe changing seasons.\\nInformation\\nExtractionExtract the names of the main characters from this short\\nstory.\\nOpen QA Why do leaves change color in autumn? Explain the\\nscientific reasons.\\nSumma rization Summa rize this article on recent advancements in\\nrenewable energy in 2-3 sentences.\\nAs described in the beginning, we want to fine-tune a model to be able\\nto generate instructions based on input. (output). We want to use this\\nas a way to create synthetic datasets to personalize LLMs and Agents.\", metadata={'page': 2, 'source': './pdfs/instruction-tune-llama2-extended-guide.pdf'}), Document(page_content=\"07/04/2024, 12:41 Extended Guide: Instruction-tune Llama 2\\nhttps://www.philschmid.de/instruction-tune-llama-2 2/172. Create an instruction dataset\\n3. Instruction-tune Llama 2 using trl and the SFTTrainer\\n4. Test the Model and run Inference\\nNote: This tutorial was created and run on a g5.2xlarge AWS EC2\\nInstance, including an NVIDIA A10G GPU.\\n1. Define the use case and create a prompt\\ntemplate for instructions\\nBefore we describe our use case, we need to better understand what\\neven is an instruction.\\n“An instruction is a piece of text or prompt that is provided to an\\nLLM, like Llama, GPT-4, or Claude, to guide it to generate a\\nresponse. Instructions allow humans to steer the conversation and\\nconstrain the language model's output to be more natural, useful,\\nand aligned with the user's goals. Crafting clear, well-formulated\\ninstructions is key to productive conversations.”\\nExamples of instructions are listed below in the table.\", metadata={'page': 1, 'source': './pdfs/instruction-tune-llama2-extended-guide.pdf'}), Document(page_content=\"07/04/2024, 12:41 Extended Guide: Instruction-tune Llama 2\\nhttps://www.philschmid.de/instruction-tune-llama-2 4/17Converting the idea into a basic prompt template following the Alpaca\\nformat we get.\\n2. Create an instruction dataset### Instruction:\\nUse the Input below to create an instruction , which could have been u\\n### Input:\\nDear [boss name ],\\nI'm writing to request next week, August 1st through August 4th ,\\noff as paid time off .\\nI have some personal matters to attend to that week that require\\nme to be out of the office . I wanted to give you as much advance\\nnotice as possible so you can plan accordingly while I am away .\\nPlease let me know if you need any additional information from me\\nor have any concerns with me taking next week off . I appreciate you\\nconsidering this request .\\nThank you , [Your name ]\\n### Response:\\nWrite an email to my boss that I need next week 08/01 - 08/04 off.\", metadata={'page': 3, 'source': './pdfs/instruction-tune-llama2-extended-guide.pdf'}), Document(page_content='07/04/2024, 12:41 Extended Guide: Instruction-tune Llama 2\\nhttps://www.philschmid.de/instruction-tune-llama-2 16/17Nice! our model works! If want to accelerate our model we can deploy\\nit with Text Generation Inference. Therefore we would need to merge\\nour adapter weights into the base model.\\nfrom peft import AutoPeftModelForCausalLM\\nmodel = AutoPeftModelForCausalLM .from_pretrained (\\n    args .output_dir ,\\n    low_cpu_mem_usage =True,\\n)\\n# Merge LoRA and base model\\nmerged_model = model.merge_and_unload ()\\n# Save the merged model\\nmerged_model .save_pretrained (\"merged_model\" ,safe_serialization =True)\\ntokenizer .save_pretrained (\"merged_model\" )\\n# push merged model to the hub\\n# merged_model.push_to_hub(\"user/repo\")\\n# tokenizer.push_to_hub(\"user/repo\")', metadata={'page': 15, 'source': './pdfs/instruction-tune-llama2-extended-guide.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "print(outputs[i][\"query\"])\n",
    "print(outputs[i][\"result\"])\n",
    "print(outputs[i][\"source_documents\"])\n",
    "i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-llama2",
   "language": "python",
   "name": "oreilly-llama2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
