{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "\n",
    "folder_path = \"./pdfs/instruction-tune-llama2-extended-guide.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"07/04/2024, 12:41 Extended Guide: Instruction-tune Llama 2\\nhttps://www.philschmid.de/instruction-tune-llama-2 1/17philschmidBlogNewsletterTagsProjectsAbout MeContact\\nExtended Guide: Instruction-tune Llama\\n2\\n#GENERATIVEAI#HUGGINGFACE#LLM#LLAMA\\nJuly 26, 2023\\n13 min read\\nView CodeThis blog post is an extended guide on instruction-tuning Llama 2 from\\nMeta AI. The idea of the blog post is to focus on creating the\\ninstruction dataset, which we can then use to fine-tune the base model\\nof Llama 2 to follow our instructions.\\nThe goal is to create a model which can create instructions based on\\ninput. The idea behind this is that this can then be used for others to\\ncreate instruction data from inputs. That's especially helpful if you\\nwant to personalize models for, e.g., tweeting, email writing, etc,\\nwhich means that you would be able to generate an instruction dataset\\nfrom your emails to then train a model to mimic your email writing.\\nOkay, so can we get started on this? In the blog, we are going to:\\n1. Define the use case and create a prompt template for instructions\", metadata={'source': './pdfs/instruction-tune-llama2-extended-guide.pdf', 'page': 0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_docs = PyPDFLoader(folder_path).load_and_split()\n",
    "pdf_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding and Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OllamaEmbeddings(model=\"mistral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = Chroma.from_documents(pdf_docs, embedding=embedding, persist_directory=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"mistral\", chat_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\" Hello! How can I help you today? If you have any questions or need assistance with something, please don't hesitate to ask. I'm here to help!\\n\\nHere are a few things I can do:\\n- Answer general knowledge questions\\n- Help you find information on a specific topic\\n- Provide explanations on complex concepts\\n- Assist with math problems and equations\\n- Generate creative writing ideas or give feedback on your own writing\\n- And much more! Let me know what you need.\\n\\nIf you just want to chat, that's cool too! I don't get tired or bored, so we can talk about anything you'd like for as long as you'd like. Just let me know what's on your mind. :)\", response_metadata={'model': 'mistral', 'created_at': '2024-04-07T11:26:39.734102Z', 'message': {'role': 'assistant', 'content': ''}, 'done': True, 'total_duration': 7862984584, 'load_duration': 3256444792, 'prompt_eval_count': 11, 'prompt_eval_duration': 130104000, 'eval_count': 163, 'eval_duration': 4473791000}, id='run-72b0fd30-b52a-477b-919f-9059d31b2ebc-0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"HI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=llm, retriever=vector_db.as_retriever(), return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' An instruction is a piece of text or prompt provided to an LLM (such as Llama, GPT-4, or Claude) to guide it to generate a response. Instructions allow humans to steer the conversation and constrain the language model\\'s output to be more natural, useful, and aligned with the user\\'s goals. Here is a passage from the uploaded document explaining instructions:\\n\\n\"An instruction is a piece of text or prompt that is provided to an LLM, like Llama, GPT-4, or Claude, to guide it to generate a response. Instructions allow humans to steer the conversation and constrain the language model\\'s output to be more natural, useful, and aligned with the user\\'s goals. Crafting clear, well-formulated instructions is key to productive conversations.\" (07/04/2024, 12:41 Extended Guide: Instruction-tune Llama 2)'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = qa.invoke(\"What is an instruction in the context of LLMs? Cite a passage from the uploaded document related to instructions.\", return_sources=True)\n",
    "output[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do a simple semi-manual benchmark for the quality of this rag setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean latency in seconds: 11.501888925378973\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "queries = [\n",
    "\"What is the definition of an instruction for LLM models?\",\n",
    "\"How can instructions guide and constrain the output of LLM models?\",\n",
    "\"What are some examples of instructions for different capabilities of LLMs, such as Brainstorming,\",\n",
    "\"What is the goal of fine-tuning a model to generate instructions based on input?\",\n",
    "\"How can synthetic instruction datasets be created for personalizing LLMs and agents?\",\n",
    "\"How to define a use case and create a prompt template for instructions?\",\n",
    "\"What are the steps to create an instruction dataset, including defining the input and output formats?\",\n",
    "\"What research suggests about creating high-quality instruction datasets?\",\n",
    "\"What are some methods to create an instruction dataset, such as using existing datasets or synthetically generating new ones?\",\n",
    "\"How to use existing LLMs to create synthetic instruction datasets?\"\n",
    "]\n",
    "\n",
    "outputs = []\n",
    "latencies = []\n",
    "for query in queries:\n",
    "    start = time.time()\n",
    "    output = qa.invoke(query)\n",
    "    outputs.append(output)\n",
    "    end = time.time()\n",
    "    latencies.append(end - start)\n",
    "\n",
    "mean_latency = np.mean(latencies)\n",
    "print(f\"Mean latency in seconds: {mean_latency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to use existing LLMs to create synthetic instruction datasets?\n",
      " To create synthetic instruction datasets using existing LLMs, you can follow these general steps:\n",
      "\n",
      "1. Define the use case and create a prompt template for instructions.\n",
      "2. Create an instruction dataset by providing inputs and corresponding desired outputs in the form of instructions. For instance, you can use the email request example provided in the guide as a template and modify it to generate different types of instructions such as writing a letter of recommendation or composing a business proposal.\n",
      "3. Instantiate the LLM and fine-tune it using the instruction dataset, creating an adapter for the model with the help of libraries like Triggers (trl) or the SFTTrainer.\n",
      "4. Test the model's performance on generating relevant instructions based on new inputs.\n",
      "5. Merge the adapter weights into the base model and save the merged model for further use.\n",
      "6. Optionally, push the merged model to a shared hub for public access.\n",
      "\n",
      "By following these steps, you can create synthetic instruction datasets using existing LLMs, enabling more personalized and productive interactions with various models.\n",
      "[Document(page_content=\"07/04/2024, 12:41 Extended Guide: Instruction-tune Llama 2\\nhttps://www.philschmid.de/instruction-tune-llama-2 3/17Capability Example Instruction\\nBrainstorming Provide a diverse set of creative ideas for new flavors of ice\\ncream.\\nClassification Categorize these movies as either comedy, drama, or\\nhorror based on the plot summa ry.\\nClosed QA Answer the question 'What is the capital of France?' with a\\nsingle word.\\nGeneration Write a poem in the style of Robert Frost about nature and\\nthe changing seasons.\\nInformation\\nExtractionExtract the names of the main characters from this short\\nstory.\\nOpen QA Why do leaves change color in autumn? Explain the\\nscientific reasons.\\nSumma rization Summa rize this article on recent advancements in\\nrenewable energy in 2-3 sentences.\\nAs described in the beginning, we want to fine-tune a model to be able\\nto generate instructions based on input. (output). We want to use this\\nas a way to create synthetic datasets to personalize LLMs and Agents.\", metadata={'page': 2, 'source': './pdfs/instruction-tune-llama2-extended-guide.pdf'}), Document(page_content=\"07/04/2024, 12:41 Extended Guide: Instruction-tune Llama 2\\nhttps://www.philschmid.de/instruction-tune-llama-2 2/172. Create an instruction dataset\\n3. Instruction-tune Llama 2 using trl and the SFTTrainer\\n4. Test the Model and run Inference\\nNote: This tutorial was created and run on a g5.2xlarge AWS EC2\\nInstance, including an NVIDIA A10G GPU.\\n1. Define the use case and create a prompt\\ntemplate for instructions\\nBefore we describe our use case, we need to better understand what\\neven is an instruction.\\n“An instruction is a piece of text or prompt that is provided to an\\nLLM, like Llama, GPT-4, or Claude, to guide it to generate a\\nresponse. Instructions allow humans to steer the conversation and\\nconstrain the language model's output to be more natural, useful,\\nand aligned with the user's goals. Crafting clear, well-formulated\\ninstructions is key to productive conversations.”\\nExamples of instructions are listed below in the table.\", metadata={'page': 1, 'source': './pdfs/instruction-tune-llama2-extended-guide.pdf'}), Document(page_content=\"07/04/2024, 12:41 Extended Guide: Instruction-tune Llama 2\\nhttps://www.philschmid.de/instruction-tune-llama-2 4/17Converting the idea into a basic prompt template following the Alpaca\\nformat we get.\\n2. Create an instruction dataset### Instruction:\\nUse the Input below to create an instruction , which could have been u\\n### Input:\\nDear [boss name ],\\nI'm writing to request next week, August 1st through August 4th ,\\noff as paid time off .\\nI have some personal matters to attend to that week that require\\nme to be out of the office . I wanted to give you as much advance\\nnotice as possible so you can plan accordingly while I am away .\\nPlease let me know if you need any additional information from me\\nor have any concerns with me taking next week off . I appreciate you\\nconsidering this request .\\nThank you , [Your name ]\\n### Response:\\nWrite an email to my boss that I need next week 08/01 - 08/04 off.\", metadata={'page': 3, 'source': './pdfs/instruction-tune-llama2-extended-guide.pdf'}), Document(page_content='07/04/2024, 12:41 Extended Guide: Instruction-tune Llama 2\\nhttps://www.philschmid.de/instruction-tune-llama-2 16/17Nice! our model works! If want to accelerate our model we can deploy\\nit with Text Generation Inference. Therefore we would need to merge\\nour adapter weights into the base model.\\nfrom peft import AutoPeftModelForCausalLM\\nmodel = AutoPeftModelForCausalLM .from_pretrained (\\n    args .output_dir ,\\n    low_cpu_mem_usage =True,\\n)\\n# Merge LoRA and base model\\nmerged_model = model.merge_and_unload ()\\n# Save the merged model\\nmerged_model .save_pretrained (\"merged_model\" ,safe_serialization =True)\\ntokenizer .save_pretrained (\"merged_model\" )\\n# push merged model to the hub\\n# merged_model.push_to_hub(\"user/repo\")\\n# tokenizer.push_to_hub(\"user/repo\")', metadata={'page': 15, 'source': './pdfs/instruction-tune-llama2-extended-guide.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "print(outputs[i][\"query\"])\n",
    "print(outputs[i][\"result\"])\n",
    "print(outputs[i][\"source_documents\"])\n",
    "i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-llama2",
   "language": "python",
   "name": "oreilly-llama2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
