{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: unrecognized arguments: --sterror\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stderr\n",
    "# !pip install sentence-transformers\n",
    "# !pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "# Step 1: Load the document from a web url\n",
    "loader = WebBaseLoader([\"https://huggingface.co/blog/llama3\"])\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "Welcome Llama 3 - Meta's new open LLM\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Hugging Face\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t\tModels\n",
      "\n",
      "\t\t\t\t\tDatasets\n",
      "\n",
      "\t\t\t\t\tSpaces\n",
      "\n",
      "\t\t\t\t\tPosts\n",
      "\n",
      "\t\t\t\t\tDocs\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\tSolutions\n",
      "\t\t\n",
      "\n",
      "Pricing\n",
      "\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Log In\n",
      "\t\t\t\t\n",
      "Sign Up\n",
      "\t\t\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t\t\tBack to Articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tWelcome Llama 3 - Metaâ€™s new open LLM\n",
      "\t\n",
      "\n",
      "\n",
      "Published\n",
      "\t\t\t\tApril 18, 2024\n",
      "Update on GitHub\n",
      "\n",
      "\n",
      "\t\tUpvote\n",
      "\n",
      "\t\t248\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "+242\n",
      "\n",
      "\n",
      "\n",
      "philschmid\n",
      "Philipp Schmid\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "osanseviero\n",
      "Omar Sanseviero\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "pcuenq\n",
      "Pedro Cuenca\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ybelkada\n",
      "Younes Belkada\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "lvwerra\n",
      "Leandro von Werra\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Introduction\n",
      "\n",
      "Table of contents\n",
      "\n",
      "Whatâ€™s new with Llama 3?\n",
      "\n",
      "Llama 3 evaluation\n",
      "\n",
      "How to prompt Llama 3\n",
      "\n",
      "Demo\n",
      "\n",
      "Using ðŸ¤—Â Transformers\n",
      "\n",
      "Inference Integrations\n",
      "Integration with Inference Endpoints\n",
      "\n",
      "Integration with Google Cloud\n",
      "\n",
      "Integration with Amazon SageMaker\n",
      "\n",
      "\n",
      "Fine-tuning with ðŸ¤—Â TRL\n",
      "\n",
      "Additional Resources\n",
      "\n",
      "Acknowledgments\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tIntroduction\n",
      "\t\n",
      "\n",
      "Metaâ€™s Llama 3, the next iteration of the open-access Llama family, is now released and available at Hugging Face. It's great to see Meta continuing its commitment to open AI, and weâ€™re excited to fully support the launch with comprehensive integration in the Hugging Face ecosystem.\n",
      "Llama 3 comes in two sizes: 8B for efficient deployment and development on consumer-size GPU, and 70B for large-scale AI native applications. Both come in base and instruction-tuned variants. In addition to the 4 models, a new version of Llama Guard was fine-tuned on Llama 3 8B and is released as Llama Guard 2 (safety fine-tune).\n",
      "Weâ€™ve collaborated with Meta to ensure the best integration into the Hugging Face ecosystem. You can find all 5 open-access models (2 base models, 2 fine-tuned & Llama Guard) on the Hub. Among the features and integrations being released, we have:\n",
      "\n",
      "Models on the Hub, with their model cards and licenses\n",
      "ðŸ¤— Transformers integration\n",
      "Hugging Chat integration for Meta Llama 3 70b\n",
      "Inference Integration into Inference Endpoints, Google Cloud & Amazon SageMaker\n",
      "An example of fine-tuning Llama 3 8B on a single GPU with ðŸ¤—Â TRL\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tTable of contents\n",
      "\t\n",
      "\n",
      "\n",
      "Whatâ€™s new with Llama 3?\n",
      "Llama 3 evaluation\n",
      "How to prompt Llama 3\n",
      "Demo\n",
      "Using ðŸ¤—Â Transformers\n",
      "Inference Integrations\n",
      "Fine-tuning with ðŸ¤—Â TRL\n",
      "Additional Resources\n",
      "Acknowledgments\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tWhatâ€™s new with Llama 3?\n",
      "\t\n",
      "\n",
      "The Llama 3 release introduces 4 new open LLM models by Meta based on the Llama 2 architecture. They come in two sizes: 8B and 70B parameters, each with base (pre-trained) and instruct-tuned versions. All the variants can be run on various types of consumer hardware and have a context length of 8K tokens. \n",
      "\n",
      "Meta-Llama-3-8b: Base 8B model\n",
      "Meta-Llama-3-8b-instruct: Instruct fine-tuned version of the base 8b model\n",
      "Meta-Llama-3-70b: Base 70B model\n",
      "Meta-Llama-3-70b-instruct: Instruct fine-tuned version of the base 70b model\n",
      "\n",
      "In addition to these 4 base models, Llama Guard 2 was also released. Fine-tuned on Llama 3 8B, itâ€™s the latest iteration in the Llama Guard family. Llama Guard 2, built for production use cases, is designed to classify LLM inputs (prompts) as well as LLM responses in order to detect content that would be considered unsafe in a risk taxonomy.\n",
      "A big change in Llama 3 compared to Llama 2 is the use of a new tokenizer that expands the vocabulary size to 128,256 (from 32K tokens in the previous version). This larger vocabulary can encode text more efficiently (both for input and output) and potentially yield stronger multilingualism. This comes at a cost, though: the embedding input and output matrices are larger, which accounts for a good portion of the parameter count increase of the small model: it goes from 7B in Llama 2 to 8B in Llama 3. In addition, the 8B version of the model now uses Grouped-Query Attention (GQA), which is an efficient representation that should help with longer contexts. \n",
      "The Llama 3 models were trained ~8x more data on over 15 trillion tokens on a new mix of publicly available online data on two clusters with 24,000 GPUs. We donâ€™t know the exact details of the training mix, and we can only guess that bigger and more careful data curation was a big factor in the improved performance. Llama 3 Instruct has been optimized for dialogue applications and was trained on over 10 Million human-annotated data samples with combination of supervised fine-tuning (SFT), rejection sampling, proximal policy optimization (PPO), and direct policy optimization (DPO). \n",
      "Regarding the licensing terms, Llama 3 comes with a permissive license that allows redistribution, fine-tuning, and derivative works. The requirement for explicit attribution is new in the Llama 3 license and was not present in Llama 2. Derived models, for instance, need to include \"Llama 3\" at the beginning of their name, and you also need to mention \"Built with Meta Llama 3\" in derivative works or services. For full details, please make sure to read the official license.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tLlama 3 evaluation\n",
      "\t\n",
      "\n",
      "Note: We are currently evaluating Meta Llama 3 individually and will update this section as soon as we get the results. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tHow to prompt Llama 3\n",
      "\t\n",
      "\n",
      "The base models have no prompt format. Like other base models, they can be used to continue an input sequence with a plausible continuation or for zero-shot/few-shot inference. They are also a great foundation for fine-tuning your own use cases. The Instruct versions use the following conversation structure:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "{{ system_prompt }}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "{{ user_msg_1 }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "{{ model_answer_1 }}<|eot_id|>\n",
      "\n",
      "This format has to be exactly reproduced for effective use. Weâ€™ll later show how easy it is to reproduce the instruct prompt with the chat template available in transformers. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tDemo\n",
      "\t\n",
      "\n",
      "You can chat with the Llama 3 70B instruct on Hugging Chat! Check out the link here: https://huggingface.co/chat/models/meta-llama/Meta-Llama-3-70B-instruct\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tUsing ðŸ¤—Â Transformers\n",
      "\t\n",
      "\n",
      "With TransformersÂ release 4.40, you can use Llama 3 and leverage all the tools within the Hugging Face ecosystem, such as:\n",
      "\n",
      "training and inference scripts and examples\n",
      "safe file format (safetensors)\n",
      "integrations with tools such as bitsandbytes (4-bit quantization), PEFT (parameter efficient fine-tuning), and Flash Attention 2\n",
      "utilities and helpers to run generation with the model\n",
      "mechanisms to export the models to deploy\n",
      "\n",
      "In addition, Llama 3 models are compatible with torch.compile() with CUDA graphs, giving them a ~4x speedup at inference time!\n",
      "To use Llama 3 models with transformers, make sure to install a recent version of transformers:\n",
      "pip install --upgrade transformers\n",
      "\n",
      "The following snippet shows how to use Llama-3-8b-instruct with transformers. It requires about 16 GB of RAM, which includes consumer GPUs such as 3090 or 4090.\n",
      "from transformers import pipeline\n",
      "import torch\n",
      "\n",
      "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
      "\n",
      "pipe = pipeline(\n",
      "    \"text-generation\",\n",
      "    model=model_id,\n",
      "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
      "    device=\"cuda\",\n",
      ")\n",
      "\n",
      "messages = [\n",
      "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
      "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
      "]\n",
      "\n",
      "terminators = [\n",
      "    pipe.tokenizer.eos_token_id,\n",
      "    pipe.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
      "]\n",
      "\n",
      "outputs = pipe(\n",
      "    messages,\n",
      "    max_new_tokens=256,\n",
      "    eos_token_id=terminators,\n",
      "    do_sample=True,\n",
      "    temperature=0.6,\n",
      "    top_p=0.9,\n",
      ")\n",
      "assistant_response = outputs[0][\"generated_text\"][-1][\"content\"]\n",
      "print(assistant_response)\n",
      "\n",
      "\n",
      "Arrrr, me hearty! Me name be Captain Chat, the scurviest pirate chatbot to ever sail the Seven Seas! Me be here to swab the decks o' yer mind with me trusty responses, savvy? I be ready to hoist the Jolly Roger and set sail fer a swashbucklin' good time, matey! So, what be bringin' ye to these fair waters?\n",
      "\n",
      "A couple of details:\n",
      "\n",
      "We loaded the model in bfloat16. This is the type used by the original checkpoint published by Meta, so itâ€™s the recommended way to run to ensure the best precision or to conduct evaluations. For real world use, itâ€™s also safe to use float16, which may be faster depending on your hardware.\n",
      "Assistant responses may end with the special token <|eot_id|>, but we must also stop generation if the regular EOS token is found. We can stop generation early by providing a list of terminators in the eos_token_id parameter.\n",
      "We used the default sampling parameters (temperature and top_p) taken from the original meta codebase. We havenâ€™t had time to conduct extensive tests yet, feel free to explore!\n",
      "\n",
      "You can also automatically quantize the model, loading it in 8-bit or even 4-bit mode. 4-bit loading takes about 7 GB of memory to run, making it compatible with a lot of consumer cards and all the GPUs in Google Colab. This is how youâ€™d load the generation pipeline in 4-bit:\n",
      "pipeline = transformers.pipeline(\n",
      "    \"text-generation\",\n",
      "    model=model_id,\n",
      "    model_kwargs={\n",
      "        \"torch_dtype\": torch.float16,\n",
      "        \"quantization_config\": {\"load_in_4bit\": True},\n",
      "        \"low_cpu_mem_usage\": True,\n",
      "    },\n",
      ")\n",
      "\n",
      "For more details on using the models with transformers, please check the model cards.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tInference Integrations\n",
      "\t\n",
      "\n",
      "In this section, weâ€™ll go through different approaches to running inference of the Llama 3 models. Before using these models, make sure you have requested access to one of the models in the officialÂ Meta Llama 3Â repositories.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tIntegration with Inference Endpoints\n",
      "\t\n",
      "\n",
      "You can deploy Llama 3 on Hugging Face'sÂ Inference Endpoints, which uses Text Generation Inference as the backend. Text Generation InferenceÂ is a production-ready inference container developed by Hugging Face to enable easy deployment of large language models. It has features such as continuous batching, token streaming, tensor parallelism for fast inference on multiple GPUs, and production-ready logging and tracing.\n",
      "To deploy Llama 3, go to theÂ model pageÂ and click on theÂ Deploy -> Inference Endpoints widget. You can learn more about Deploying LLMs with Hugging Face Inference Endpoints in a previous blog post. Inference Endpoints supports Messages API through Text Generation Inference, which allows you to switch from another closed model to an open one by simply changing the URL.\n",
      "from openai import OpenAI\n",
      "\n",
      "# initialize the client but point it to TGI\n",
      "client = OpenAI(\n",
      "    base_url=\"<ENDPOINT_URL>\" + \"/v1/\",  # replace with your endpoint url\n",
      "    api_key=\"<HF_API_TOKEN>\",  # replace with your token\n",
      ")\n",
      "chat_completion = client.chat.completions.create(\n",
      "    model=\"tgi\",\n",
      "    messages=[\n",
      "        {\"role\": \"user\", \"content\": \"Why is open-source software important?\"},\n",
      "    ],\n",
      "    stream=True,\n",
      "    max_tokens=500\n",
      ")\n",
      "\n",
      "# iterate and print stream\n",
      "for message in chat_completion:\n",
      "    print(message.choices[0].delta.content, end=\"\")\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tIntegration with Google Cloud\n",
      "\t\n",
      "\n",
      "You can deploy Llama 3 on Google Cloud through Vertex AI or Google Kubernetes Engine (GKE), using Text Generation Inference. \n",
      "To deploy the Llama 3 model from Hugging Face, go to theÂ model pageÂ and click on Deploy -> Google Cloud. This will bring you to the Google Cloud Console, where you can 1-click deploy Llama 3 on Vertex AI or GKE.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tIntegration with Amazon SageMaker\n",
      "\t\n",
      "\n",
      "You can deploy and train Llama 3 on Amazon SageMaker through AWS Jumpstart or using the Hugging Face LLM Container. \n",
      "To deploy the Llama 3 model from Hugging Face, go to theÂ model pageÂ and click on Deploy -> Amazon SageMaker. This will display a code snippet you can copy and execute in your environment. Amazon SageMaker will now create a dedicated inference endpoint you can use to send requests. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tFine-tuning with ðŸ¤—Â TRL\n",
      "\t\n",
      "\n",
      "Training LLMs can be technically and computationally challenging. In this section, weâ€™ll look at the tools available in the Hugging Face ecosystem to efficiently train Llama 3 on consumer-size GPUs. Below is an example command to fine-tune Llama 3 on the No Robots dataset. We use 4-bit quantization, and QLoRA and TRLâ€™s SFTTrainer will automatically format the dataset into chatml format. Letâ€™s get started!\n",
      "First, install the latest version of ðŸ¤— TRL. \n",
      "pip install -U transformers trl accelerate\n",
      "\n",
      "If you just want to chat with the model in the terminal you can use the chat command of the TRL CLI (for more info see the docs):\n",
      "trl chat \\\n",
      "--model_name_or_path meta-llama/Meta-Llama-3-8B-Instruct \\\n",
      "--device cuda \\\n",
      "--eos_tokens \"<|end_of_text|>,<|eod_id|>\"\n",
      "\n",
      "You can also use TRL CLI to supervise fine-tuning (SFT) Llama 3 on your own, custom dataset. Use the trl sft command and pass your training arguments as CLI argument. Make sure you are logged in and have access the Llama 3 checkpoint. You can do this with huggingface-cli login.\n",
      "trl sft \\\n",
      "--model_name_or_path meta-llama/Meta-Llama-3-8B \\\n",
      "--dataset_name HuggingFaceH4/no_robots \\\n",
      "--learning_rate 0.0001 \\\n",
      "--per_device_train_batch_size 4 \\\n",
      "--max_seq_length 2048 \\\n",
      "--output_dir ./llama3-sft \\\n",
      "--use_peft \\\n",
      "--load_in_4bit \\\n",
      "--log_with wandb \\\n",
      "--gradient_checkpointing \\\n",
      "--logging_steps 10\n",
      "\n",
      "This will run the fine-tuning from your terminal and takes about 4 hours to train on a single A10G, but can be easily parallelized by tweaking --num_processes to the number of GPUs you have available.\n",
      "Note: You can also replace the CLI arguments with a yaml file. Learn more about the TRL CLI here. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tAdditional Resources\n",
      "\t\n",
      "\n",
      "\n",
      "Models on the Hub\n",
      "Open LLM Leaderboard\n",
      "Chat demo on Hugging Chat\n",
      "Meta Blog\n",
      "Google Cloud Vertex AI model garden\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tAcknowledgments\n",
      "\t\n",
      "\n",
      "Releasing such models with support and evaluations in the ecosystem would not be possible without the contributions of many community members, including\n",
      "\n",
      "ClÃ©mentine Fourrier, Nathan Habib,Â andÂ Eleuther Evaluation HarnessÂ for LLM evaluations\n",
      "Olivier Dehaene and Nicolas Patry for Text Generation Inference Support\n",
      "Arthur Zucker and Lysandre Debut for adding Llama 3 support in transformers and tokenizers\n",
      "Nathan Sarrazin, Victor Mustar, and Kevin Cathaly for making Llama 3 available in Hugging Chat.\n",
      "Yuvraj Sharma for the Gradio demo.\n",
      "Xenova and Vaibhav Srivastav for debugging and experimentation with quantization and prompt templates.\n",
      "Brigitte Tousignant, Florent Daudens, Morgan Funtowicz, and Simon Brandeis for different items during the launch!\n",
      "Thank you to the whole Meta team, including Samuel Selvan, Eleonora Presani, Hamid Shojanazeri, Azadeh Yazdan, Aiman Farooq, Ruan Silva, Ashley Gabriel, Eissa Jamil, Binh Tang, Matthias Reso, Lovish Madaan, Joe Spisak, and Sergey Edunov.\n",
      "\n",
      "Thank you to the Meta Team for releasing Llama 3 and making it available to the open-source AI community!\n",
      "\n",
      "More Articles from our Blog\n",
      "\n",
      "\n",
      "Training and Finetuning Embedding Models with Sentence Transformers v3\n",
      "\n",
      "\t\t\t\tByÂ \n",
      "tomaarsen\n",
      "\n",
      "\n",
      "May 28, 2024\n",
      "â€¢\n",
      "\n",
      "\n",
      "\t\t\t\t87\n",
      "\n",
      "Falcon 2: An 11B parameter pretrained language model and VLM, trained on over 5000B tokens tokens and 11 languages\n",
      "\n",
      "\t\t\t\tByÂ \n",
      "Quent-01\n",
      "\n",
      "\n",
      "May 24, 2024\n",
      "\n",
      "guest\n",
      "â€¢\n",
      "\n",
      "\n",
      "\t\t\t\t14\n",
      "\n",
      "\n",
      "\t\tUpvote\n",
      "\n",
      "\t\t248\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "+236\n",
      "\n",
      "Company\n",
      "Â© Hugging Face\n",
      "TOS\n",
      "Privacy\n",
      "About\n",
      "Jobs\n",
      "\n",
      "Website\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Pricing\n",
      "Docs\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in documents:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\nWelcome Llama 3 - Meta\\'s new open LLM\\n\\n\\n\\n\\n\\n\\n\\nHugging Face\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\tModels\\n\\n\\t\\t\\t\\t\\tDatasets\\n\\n\\t\\t\\t\\t\\tSpaces\\n\\n\\t\\t\\t\\t\\tPosts\\n\\n\\t\\t\\t\\t\\tDocs\\n\\n\\n\\n\\n\\t\\t\\tSolutions\\n\\t\\t\\n\\nPricing\\n\\t\\t\\t\\n\\n\\n\\n\\n\\n\\nLog In\\n\\t\\t\\t\\t\\nSign Up\\n\\t\\t\\t\\t\\t\\n\\n\\n\\n\\t\\t\\t\\t\\t\\tBack to Articles\\n\\n\\n\\n\\n\\n\\t\\tWelcome Llama 3 - Metaâ€™s new open LLM\\n\\t\\n\\n\\nPublished\\n\\t\\t\\t\\tApril 18, 2024\\nUpdate on GitHub\\n\\n\\n\\t\\tUpvote\\n\\n\\t\\t248\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n+242\\n\\n\\n\\nphilschmid\\nPhilipp Schmid\\n\\n\\n\\n\\n\\nosanseviero\\nOmar Sanseviero\\n\\n\\n\\n\\n\\npcuenq\\nPedro Cuenca\\n\\n\\n\\n\\n\\nybelkada\\nYounes Belkada\\n\\n\\n\\n\\n\\nlvwerra\\nLeandro von Werra\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIntroduction\\n\\nTable of contents\\n\\nWhatâ€™s new with Llama 3?\\n\\nLlama 3 evaluation\\n\\nHow to prompt Llama 3\\n\\nDemo\\n\\nUsing ðŸ¤—\\xa0Transformers\\n\\nInference Integrations\\nIntegration with Inference Endpoints\\n\\nIntegration with Google Cloud\\n\\nIntegration with Amazon SageMaker\\n\\n\\nFine-tuning with ðŸ¤—\\xa0TRL\\n\\nAdditional Resources\\n\\nAcknowledgments\\n\\n\\n\\n\\n\\n\\n\\t\\tIntroduction\\n\\t\\n\\nMetaâ€™s Llama 3, the next iteration of the open-access Llama family, is now released and available at Hugging Face. It\\'s great to see Meta continuing its commitment to open AI, and weâ€™re excited to fully support the launch with comprehensive integration in the Hugging Face ecosystem.\\nLlama 3 comes in two sizes: 8B for efficient deployment and development on consumer-size GPU, and 70B for large-scale AI native applications. Both come in base and instruction-tuned variants. In addition to the 4 models, a new version of Llama Guard was fine-tuned on Llama 3 8B and is released as Llama Guard 2 (safety fine-tune).\\nWeâ€™ve collaborated with Meta to ensure the best integration into the Hugging Face ecosystem. You can find all 5 open-access models (2 base models, 2 fine-tuned & Llama Guard) on the Hub. Among the features and integrations being released, we have:\\n\\nModels on the Hub, with their model cards and licenses\\nðŸ¤— Transformers integration\\nHugging Chat integration for Meta Llama 3 70b\\nInference Integration into Inference Endpoints, Google Cloud & Amazon SageMaker\\nAn example of fine-tuning Llama 3 8B on a single GPU with ðŸ¤—\\xa0TRL\\n\\n\\n\\n\\n\\n\\n\\t\\tTable of contents\\n\\t\\n\\n\\nWhatâ€™s new with Llama 3?\\nLlama 3 evaluation\\nHow to prompt Llama 3\\nDemo\\nUsing ðŸ¤—\\xa0Transformers\\nInference Integrations\\nFine-tuning with ðŸ¤—\\xa0TRL\\nAdditional Resources\\nAcknowledgments\\n\\n\\n\\n\\n\\n\\n\\t\\tWhatâ€™s new with Llama 3?\\n\\t\\n\\nThe Llama 3 release introduces 4 new open LLM models by Meta based on the Llama 2 architecture. They come in two sizes: 8B and 70B parameters, each with base (pre-trained) and instruct-tuned versions. All the variants can be run on various types of consumer hardware and have a context length of 8K tokens. \\n\\nMeta-Llama-3-8b: Base 8B model\\nMeta-Llama-3-8b-instruct: Instruct fine-tuned version of the base 8b model\\nMeta-Llama-3-70b: Base 70B model\\nMeta-Llama-3-70b-instruct: Instruct fine-tuned version of the base 70b model\\n\\nIn addition to these 4 base models, Llama Guard 2 was also released. Fine-tuned on Llama 3 8B, itâ€™s the latest iteration in the Llama Guard family. Llama Guard 2, built for production use cases, is designed to classify LLM inputs (prompts) as well as LLM responses in order to detect content that would be considered unsafe in a risk taxonomy.\\nA big change in Llama 3 compared to Llama 2 is the use of a new tokenizer that expands the vocabulary size to 128,256 (from 32K tokens in the previous version). This larger vocabulary can encode text more efficiently (both for input and output) and potentially yield stronger multilingualism. This comes at a cost, though: the embedding input and output matrices are larger, which accounts for a good portion of the parameter count increase of the small model: it goes from 7B in Llama 2 to 8B in Llama 3. In addition, the 8B version of the model now uses Grouped-Query Attention (GQA), which is an efficient representation that should help with longer contexts. \\nThe Llama 3 models were trained ~8x more data on over 15 trillion tokens on a new mix of publicly available online data on two clusters with 24,000 GPUs. We donâ€™t know the exact details of the training mix, and we can only guess that bigger and more careful data curation was a big factor in the improved performance. Llama 3 Instruct has been optimized for dialogue applications and was trained on over 10 Million human-annotated data samples with combination of supervised fine-tuning (SFT), rejection sampling, proximal policy optimization (PPO), and direct policy optimization (DPO). \\nRegarding the licensing terms, Llama 3 comes with a permissive license that allows redistribution, fine-tuning, and derivative works. The requirement for explicit attribution is new in the Llama 3 license and was not present in Llama 2. Derived models, for instance, need to include \"Llama 3\" at the beginning of their name, and you also need to mention \"Built with Meta Llama 3\" in derivative works or services. For full details, please make sure to read the official license.\\n\\n\\n\\n\\n\\n\\t\\tLlama 3 evaluation\\n\\t\\n\\nNote: We are currently evaluating Meta Llama 3 individually and will update this section as soon as we get the results. \\n\\n\\n\\n\\n\\n\\t\\tHow to prompt Llama 3\\n\\t\\n\\nThe base models have no prompt format. Like other base models, they can be used to continue an input sequence with a plausible continuation or for zero-shot/few-shot inference. They are also a great foundation for fine-tuning your own use cases. The Instruct versions use the following conversation structure:\\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{{ system_prompt }}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{{ user_msg_1 }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n{{ model_answer_1 }}<|eot_id|>\\n\\nThis format has to be exactly reproduced for effective use. Weâ€™ll later show how easy it is to reproduce the instruct prompt with the chat template available in transformers. \\n\\n\\n\\n\\n\\n\\t\\tDemo\\n\\t\\n\\nYou can chat with the Llama 3 70B instruct on Hugging Chat! Check out the link here: https://huggingface.co/chat/models/meta-llama/Meta-Llama-3-70B-instruct\\n\\n\\n\\n\\n\\n\\t\\tUsing ðŸ¤—\\xa0Transformers\\n\\t\\n\\nWith Transformers\\xa0release 4.40, you can use Llama 3 and leverage all the tools within the Hugging Face ecosystem, such as:\\n\\ntraining and inference scripts and examples\\nsafe file format (safetensors)\\nintegrations with tools such as bitsandbytes (4-bit quantization), PEFT (parameter efficient fine-tuning), and Flash Attention 2\\nutilities and helpers to run generation with the model\\nmechanisms to export the models to deploy\\n\\nIn addition, Llama 3 models are compatible with torch.compile() with CUDA graphs, giving them a ~4x speedup at inference time!\\nTo use Llama 3 models with transformers, make sure to install a recent version of transformers:\\npip install --upgrade transformers\\n\\nThe following snippet shows how to use Llama-3-8b-instruct with transformers. It requires about 16 GB of RAM, which includes consumer GPUs such as 3090 or 4090.\\nfrom transformers import pipeline\\nimport torch\\n\\nmodel_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\\n\\npipe = pipeline(\\n    \"text-generation\",\\n    model=model_id,\\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\\n    device=\"cuda\",\\n)\\n\\nmessages = [\\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\\n]\\n\\nterminators = [\\n    pipe.tokenizer.eos_token_id,\\n    pipe.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\\n]\\n\\noutputs = pipe(\\n    messages,\\n    max_new_tokens=256,\\n    eos_token_id=terminators,\\n    do_sample=True,\\n    temperature=0.6,\\n    top_p=0.9,\\n)\\nassistant_response = outputs[0][\"generated_text\"][-1][\"content\"]\\nprint(assistant_response)\\n\\n\\nArrrr, me hearty! Me name be Captain Chat, the scurviest pirate chatbot to ever sail the Seven Seas! Me be here to swab the decks o\\' yer mind with me trusty responses, savvy? I be ready to hoist the Jolly Roger and set sail fer a swashbucklin\\' good time, matey! So, what be bringin\\' ye to these fair waters?\\n\\nA couple of details:\\n\\nWe loaded the model in bfloat16. This is the type used by the original checkpoint published by Meta, so itâ€™s the recommended way to run to ensure the best precision or to conduct evaluations. For real world use, itâ€™s also safe to use float16, which may be faster depending on your hardware.\\nAssistant responses may end with the special token <|eot_id|>, but we must also stop generation if the regular EOS token is found. We can stop generation early by providing a list of terminators in the eos_token_id parameter.\\nWe used the default sampling parameters (temperature and top_p) taken from the original meta codebase. We havenâ€™t had time to conduct extensive tests yet, feel free to explore!\\n\\nYou can also automatically quantize the model, loading it in 8-bit or even 4-bit mode. 4-bit loading takes about 7 GB of memory to run, making it compatible with a lot of consumer cards and all the GPUs in Google Colab. This is how youâ€™d load the generation pipeline in 4-bit:\\npipeline = transformers.pipeline(\\n    \"text-generation\",\\n    model=model_id,\\n    model_kwargs={\\n        \"torch_dtype\": torch.float16,\\n        \"quantization_config\": {\"load_in_4bit\": True},\\n        \"low_cpu_mem_usage\": True,\\n    },\\n)\\n\\nFor more details on using the models with transformers, please check the model cards.\\n\\n\\n\\n\\n\\n\\t\\tInference Integrations\\n\\t\\n\\nIn this section, weâ€™ll go through different approaches to running inference of the Llama 3 models. Before using these models, make sure you have requested access to one of the models in the official\\xa0Meta Llama 3\\xa0repositories.\\n\\n\\n\\n\\n\\n\\t\\tIntegration with Inference Endpoints\\n\\t\\n\\nYou can deploy Llama 3 on Hugging Face\\'s\\xa0Inference Endpoints, which uses Text Generation Inference as the backend. Text Generation Inference\\xa0is a production-ready inference container developed by Hugging Face to enable easy deployment of large language models. It has features such as continuous batching, token streaming, tensor parallelism for fast inference on multiple GPUs, and production-ready logging and tracing.\\nTo deploy Llama 3, go to the\\xa0model page\\xa0and click on the\\xa0Deploy -> Inference Endpoints widget. You can learn more about Deploying LLMs with Hugging Face Inference Endpoints in a previous blog post. Inference Endpoints supports Messages API through Text Generation Inference, which allows you to switch from another closed model to an open one by simply changing the URL.\\nfrom openai import OpenAI\\n\\n# initialize the client but point it to TGI\\nclient = OpenAI(\\n    base_url=\"<ENDPOINT_URL>\" + \"/v1/\",  # replace with your endpoint url\\n    api_key=\"<HF_API_TOKEN>\",  # replace with your token\\n)\\nchat_completion = client.chat.completions.create(\\n    model=\"tgi\",\\n    messages=[\\n        {\"role\": \"user\", \"content\": \"Why is open-source software important?\"},\\n    ],\\n    stream=True,\\n    max_tokens=500\\n)\\n\\n# iterate and print stream\\nfor message in chat_completion:\\n    print(message.choices[0].delta.content, end=\"\")\\n\\n\\n\\n\\n\\n\\n\\t\\tIntegration with Google Cloud\\n\\t\\n\\nYou can deploy Llama 3 on Google Cloud through Vertex AI or Google Kubernetes Engine (GKE), using Text Generation Inference. \\nTo deploy the Llama 3 model from Hugging Face, go to the\\xa0model page\\xa0and click on Deploy -> Google Cloud. This will bring you to the Google Cloud Console, where you can 1-click deploy Llama 3 on Vertex AI or GKE.\\n\\n\\n\\n\\n\\n\\t\\tIntegration with Amazon SageMaker\\n\\t\\n\\nYou can deploy and train Llama 3 on Amazon SageMaker through AWS Jumpstart or using the Hugging Face LLM Container. \\nTo deploy the Llama 3 model from Hugging Face, go to the\\xa0model page\\xa0and click on Deploy -> Amazon SageMaker. This will display a code snippet you can copy and execute in your environment. Amazon SageMaker will now create a dedicated inference endpoint you can use to send requests. \\n\\n\\n\\n\\n\\n\\t\\tFine-tuning with ðŸ¤—\\xa0TRL\\n\\t\\n\\nTraining LLMs can be technically and computationally challenging. In this section, weâ€™ll look at the tools available in the Hugging Face ecosystem to efficiently train Llama 3 on consumer-size GPUs. Below is an example command to fine-tune Llama 3 on the No Robots dataset. We use 4-bit quantization, and QLoRA and TRLâ€™s SFTTrainer will automatically format the dataset into chatml format. Letâ€™s get started!\\nFirst, install the latest version of ðŸ¤— TRL. \\npip install -U transformers trl accelerate\\n\\nIf you just want to chat with the model in the terminal you can use the chat command of the TRL CLI (for more info see the docs):\\ntrl chat \\\\\\n--model_name_or_path meta-llama/Meta-Llama-3-8B-Instruct \\\\\\n--device cuda \\\\\\n--eos_tokens \"<|end_of_text|>,<|eod_id|>\"\\n\\nYou can also use TRL CLI to supervise fine-tuning (SFT) Llama 3 on your own, custom dataset. Use the trl sft command and pass your training arguments as CLI argument. Make sure you are logged in and have access the Llama 3 checkpoint. You can do this with huggingface-cli login.\\ntrl sft \\\\\\n--model_name_or_path meta-llama/Meta-Llama-3-8B \\\\\\n--dataset_name HuggingFaceH4/no_robots \\\\\\n--learning_rate 0.0001 \\\\\\n--per_device_train_batch_size 4 \\\\\\n--max_seq_length 2048 \\\\\\n--output_dir ./llama3-sft \\\\\\n--use_peft \\\\\\n--load_in_4bit \\\\\\n--log_with wandb \\\\\\n--gradient_checkpointing \\\\\\n--logging_steps 10\\n\\nThis will run the fine-tuning from your terminal and takes about 4 hours to train on a single A10G, but can be easily parallelized by tweaking --num_processes to the number of GPUs you have available.\\nNote: You can also replace the CLI arguments with a yaml file. Learn more about the TRL CLI here. \\n\\n\\n\\n\\n\\n\\t\\tAdditional Resources\\n\\t\\n\\n\\nModels on the Hub\\nOpen LLM Leaderboard\\nChat demo on Hugging Chat\\nMeta Blog\\nGoogle Cloud Vertex AI model garden\\n\\n\\n\\n\\n\\n\\n\\t\\tAcknowledgments\\n\\t\\n\\nReleasing such models with support and evaluations in the ecosystem would not be possible without the contributions of many community members, including\\n\\nClÃ©mentine Fourrier, Nathan Habib,\\xa0and\\xa0Eleuther Evaluation Harness\\xa0for LLM evaluations\\nOlivier Dehaene and Nicolas Patry for Text Generation Inference Support\\nArthur Zucker and Lysandre Debut for adding Llama 3 support in transformers and tokenizers\\nNathan Sarrazin, Victor Mustar, and Kevin Cathaly for making Llama 3 available in Hugging Chat.\\nYuvraj Sharma for the Gradio demo.\\nXenova and Vaibhav Srivastav for debugging and experimentation with quantization and prompt templates.\\nBrigitte Tousignant, Florent Daudens, Morgan Funtowicz, and Simon Brandeis for different items during the launch!\\nThank you to the whole Meta team, including Samuel Selvan, Eleonora Presani, Hamid Shojanazeri, Azadeh Yazdan, Aiman Farooq, Ruan Silva, Ashley Gabriel, Eissa Jamil, Binh Tang, Matthias Reso, Lovish Madaan, Joe Spisak, and Sergey Edunov.\\n\\nThank you to the Meta Team for releasing Llama 3 and making it available to the open-source AI community!\\n\\nMore Articles from our Blog\\n\\n\\nTraining and Finetuning Embedding Models with Sentence Transformers v3\\n\\n\\t\\t\\t\\tBy\\xa0\\ntomaarsen\\n\\n\\nMay 28, 2024\\nâ€¢\\n\\n\\n\\t\\t\\t\\t87\\n\\nFalcon 2: An 11B parameter pretrained language model and VLM, trained on over 5000B tokens tokens and 11 languages\\n\\n\\t\\t\\t\\tBy\\xa0\\nQuent-01\\n\\n\\nMay 24, 2024\\n\\nguest\\nâ€¢\\n\\n\\n\\t\\t\\t\\t14\\n\\n\\n\\t\\tUpvote\\n\\n\\t\\t248\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n+236\\n\\nCompany\\nÂ© Hugging Face\\nTOS\\nPrivacy\\nAbout\\nJobs\\n\\nWebsite\\nModels\\nDatasets\\nSpaces\\nPricing\\nDocs\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://huggingface.co/blog/llama3', 'title': \"Welcome Llama 3 - Meta's new open LLM\", 'description': 'Weâ€™re on a journey to advance and democratize artificial intelligence through open source and open science.', 'language': 'No language found.'})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"Welcome Llama 3 - Meta's new open LLM\\n\\n\\n\\n\\n\\n\\n\\nHugging Face\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\tModels\\n\\n\\t\\t\\t\\t\\tDatasets\\n\\n\\t\\t\\t\\t\\tSpaces\\n\\n\\t\\t\\t\\t\\tPosts\\n\\n\\t\\t\\t\\t\\tDocs\\n\\n\\n\\n\\n\\t\\t\\tSolutions\\n\\t\\t\\n\\nPricing\\n\\t\\t\\t\\n\\n\\n\\n\\n\\n\\nLog In\\n\\t\\t\\t\\t\\nSign Up\\n\\t\\t\\t\\t\\t\\n\\n\\n\\n\\t\\t\\t\\t\\t\\tBack to Articles\\n\\n\\n\\n\\n\\n\\t\\tWelcome Llama 3 - Metaâ€™s new open LLM\\n\\t\\n\\n\\nPublished\\n\\t\\t\\t\\tApril 18, 2024\\nUpdate on GitHub\\n\\n\\n\\t\\tUpvote\\n\\n\\t\\t248\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n+242\\n\\n\\n\\nphilschmid\\nPhilipp Schmid\\n\\n\\n\\n\\n\\nosanseviero\\nOmar Sanseviero\\n\\n\\n\\n\\n\\npcuenq\\nPedro Cuenca\\n\\n\\n\\n\\n\\nybelkada\\nYounes Belkada\\n\\n\\n\\n\\n\\nlvwerra\\nLeandro von Werra\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIntroduction\\n\\nTable of contents\\n\\nWhatâ€™s new with Llama 3?\\n\\nLlama 3 evaluation\\n\\nHow to prompt Llama 3\\n\\nDemo\\n\\nUsing ðŸ¤—\\xa0Transformers\\n\\nInference Integrations\\nIntegration with Inference Endpoints\\n\\nIntegration with Google Cloud\\n\\nIntegration with Amazon SageMaker\\n\\n\\nFine-tuning with ðŸ¤—\\xa0TRL\\n\\nAdditional Resources\\n\\nAcknowledgments\\n\\n\\n\\n\\n\\n\\n\\t\\tIntroduction\", metadata={'source': 'https://huggingface.co/blog/llama3', 'title': \"Welcome Llama 3 - Meta's new open LLM\", 'description': 'Weâ€™re on a journey to advance and democratize artificial intelligence through open source and open science.', 'language': 'No language found.'})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Split the document into chunks with a specified chunk size\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=100)\n",
    "all_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "all_splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Store the document into a vector store with a specific embedding model\n",
    "\n",
    "embedding = OllamaEmbeddings(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = Chroma.from_documents(all_splits, embedding=embedding, persist_directory=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi! It's nice to meet you. Is there something I can help you with or would you like to chat?\", response_metadata={'model': 'llama3', 'created_at': '2024-06-11T20:04:00.365519Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 638843541, 'load_duration': 2102625, 'prompt_eval_count': 10, 'prompt_eval_duration': 227186000, 'eval_count': 25, 'eval_duration': 407435000}, id='run-07ac2378-d4cc-4073-9a6d-c65d35559867-0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3\")\n",
    "llm.invoke(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, retriever=vector_db.as_retriever(), return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is llama3?',\n",
       " 'result': 'Llama 3 refers to a set of language models developed by Meta AI. It comes in two sizes: 8B and 70B, which are designed for different use cases.\\n\\nThe Llama 3 models were trained on a large dataset of over 15 trillion tokens, using a mix of publicly available online data, and were optimized for various applications such as dialogue and natural language processing tasks. The models were fine-tuned using different techniques like supervised fine-tuning, rejection sampling, proximal policy optimization, and direct policy optimization.\\n\\nOne of the notable features of Llama 3 is its ability to generate human-like text and perform well on a variety of tasks, including conversational dialogue and text classification.',\n",
       " 'source_documents': [Document(page_content='Llama 3 comes in two sizes: 8B for efficient deployment and development on consumer-size GPU, and 70B for large-scale AI native applications. Both come in base and instruction-tuned variants. In addition to the 4 models, a new version of Llama Guard was fine-tuned on Llama 3 8B and is released as Llama Guard 2 (safety fine-tune).', metadata={'description': 'Weâ€™re on a journey to advance and democratize artificial intelligence through open source and open science.', 'language': 'No language found.', 'source': 'https://huggingface.co/blog/llama3', 'title': \"Welcome Llama 3 - Meta's new open LLM\"}),\n",
       "  Document(page_content='Assistant responses may end with the special token <|eot_id|>, but we must also stop generation if the regular EOS token is found. We can stop generation early by providing a list of terminators in the eos_token_id parameter.\\nWe used the default sampling parameters (temperature and top_p) taken from the original meta codebase. We havenâ€™t had time to conduct extensive tests yet, feel free to explore!', metadata={'description': 'Weâ€™re on a journey to advance and democratize artificial intelligence through open source and open science.', 'language': 'No language found.', 'source': 'https://huggingface.co/blog/llama3', 'title': \"Welcome Llama 3 - Meta's new open LLM\"}),\n",
       "  Document(page_content='The Llama 3 models were trained ~8x more data on over 15 trillion tokens on a new mix of publicly available online data on two clusters with 24,000 GPUs. We donâ€™t know the exact details of the training mix, and we can only guess that bigger and more careful data curation was a big factor in the improved performance. Llama 3 Instruct has been optimized for dialogue applications and was trained on over 10 Million human-annotated data samples with combination of supervised fine-tuning (SFT), rejection sampling, proximal policy optimization (PPO), and direct policy optimization (DPO).', metadata={'description': 'Weâ€™re on a journey to advance and democratize artificial intelligence through open source and open science.', 'language': 'No language found.', 'source': 'https://huggingface.co/blog/llama3', 'title': \"Welcome Llama 3 - Meta's new open LLM\"}),\n",
       "  Document(page_content='In addition to these 4 base models, Llama Guard 2 was also released. Fine-tuned on Llama 3 8B, itâ€™s the latest iteration in the Llama Guard family. Llama Guard 2, built for production use cases, is designed to classify LLM inputs (prompts) as well as LLM responses in order to detect content that would be considered unsafe in a risk taxonomy.', metadata={'description': 'Weâ€™re on a journey to advance and democratize artificial intelligence through open source and open science.', 'language': 'No language found.', 'source': 'https://huggingface.co/blog/llama3', 'title': \"Welcome Llama 3 - Meta's new open LLM\"})]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = qa.invoke(\"What is llama3?\", return_sources=True)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-llama3",
   "language": "python",
   "name": "oreilly-llama3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
