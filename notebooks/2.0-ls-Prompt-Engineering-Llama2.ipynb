{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Prompt Engineering and Inference with Llama2\n",
    "\n",
    "So far we've discussed a bit about the basics of prompting models and some prompt engineering techniques.\n",
    "\n",
    "But what's special about prompting Llama2? I think there are a few worthy mentions regarding that if you're running the model directly or through llama-cpp:\n",
    "\n",
    "- There is a special prompt template is specific to llama2 chat models and differs from the base models, which have no prompt structure and are raw, non-instruct tuned models.\n",
    "\n",
    "The template formats:\n",
    "\n",
    "- For newlines escaped (e.g., using with curl or terminal):\n",
    "\n",
    "```<s>[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\n{user_message_1} [/INST]\n",
    "With regular newlines (e.g., for text-generation-webui):\n",
    "<s>[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\n{user_message_1} [/INST]\n",
    "```\n",
    "\n",
    "- Template Without System Message:\n",
    "\n",
    "```\n",
    "<s>[INST] {user_message_1} [/INST]\n",
    "```\n",
    "\n",
    "- Continuing a Conversation:\n",
    "\n",
    "    - Append model responses for ongoing conversations using similar templates, with model replies included.\n",
    "\n",
    "- End of String Signifier:\n",
    "\n",
    "    - Llama 2 uses </s> as the end of string signifier.\n",
    "\n",
    "- Single Message End of String Signifier:\n",
    "\n",
    "    - It is not clear whether an end of string signifier is used for single messages.\n",
    "\n",
    "- Default System Message:\n",
    "\n",
    "    - The default system message emphasizes the assistant being helpful, respectful, honest, safe, and socially unbiased. It advises against harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
    "\n",
    "Check out these guides \n",
    "- [Llama 2 Prompt Template](https://gpus.llm-utils.org/llama-2-prompt-template/#does-it-use-an-end-of-string-signifier-if-theres-only-a-single-message) \n",
    "\n",
    "- [A Guide to Prompting Llama2](https://replicate.com/blog/how-to-prompt-llama)\n",
    "\n",
    "for more info on this.\n",
    "\n",
    "\n",
    "Now! For the purpose of this live-training we'll be using high level APIs that abstract away that complexity so we should be fine with this template issue as long as we're using `llama-cpp-python` or the Hugging Face and Langchain bindings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Basics\n",
    "\n",
    "A prompt is a piece of text that conveys to a LLM what the user wants. What the user wants can be many things like:\n",
    "\n",
    "- Asking a question\n",
    "- Giving an instruction\n",
    "- Etc...\n",
    "\n",
    "As discussed in the presentation the key components of a prompt are:\n",
    "1. Task description: where you describe what you want\n",
    "2. Input data: data the model has not seem to illustrate what you need\n",
    "3. Context information: background info on what you are requesting, the data you are providing etc...\n",
    "4. Prompt style: its how you ask the thing to the model and that can greatly influence its performance, for example asking the model [\"Let's think step by step\" can boost reasoning performance](https://arxiv.org/pdf/2201.11903.pdf).\n",
    "\n",
    "[Prompts can also be seen as a form of programming that can customize the outputs and interactions with an LLM.](https://ar5iv.labs.arxiv.org/html/2302.11382#:~:text=prompts%20are%20also%20a%20form%20of%20programming%20that%20can%20customize%20the%20outputs%20and%20interactions%20with%20an%20llm.)\n",
    "\n",
    "One way I like to think about prompts, is as __tools that rearrange the weights (probabilities) in the LLM text representation space__, to allow you access to a particular sub-universe within the embedding space of the LLM. \n",
    "\n",
    "Let's put some practice into these theoretical concepts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_description = \"I want you to write a one paragraph essay about how to learn using generative artificial intelligence.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Data & Context Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = \"Learning Topic examples: [calculus, derivatives, hypothesis testing, probability distributions]\"\n",
    "\n",
    "context_information = \"I am a student who is trying to learn about the mathematical foundations of AI\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How you ask what you want, and the heavily relies on what you want from the model.\n",
    "# Instruction prompt: \n",
    "\n",
    "instruction_prompt = f\"{task_description} {input_data} {context_information}. I want the output to be a set of instructive bullet points:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ./models/llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:            blk.0.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:            blk.0.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:              blk.0.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:         blk.0.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:              blk.0.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.1.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:            blk.1.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:              blk.1.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:         blk.1.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:              blk.1.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:           blk.10.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:           blk.10.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:             blk.10.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:             blk.10.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:        blk.10.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:             blk.10.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:             blk.10.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:           blk.11.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:           blk.11.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:             blk.11.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:             blk.11.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:        blk.11.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:             blk.11.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:             blk.11.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:           blk.12.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:           blk.12.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:             blk.12.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:             blk.12.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:        blk.12.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:             blk.12.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:             blk.12.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:           blk.13.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:           blk.13.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:             blk.13.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:             blk.13.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:        blk.13.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:             blk.13.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:             blk.13.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:           blk.14.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:           blk.14.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:             blk.14.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:             blk.14.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:        blk.14.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:             blk.14.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:             blk.14.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:           blk.15.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:           blk.15.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:             blk.15.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:             blk.15.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:        blk.15.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:             blk.15.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:             blk.15.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:           blk.16.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:           blk.16.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:             blk.16.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:             blk.16.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:        blk.16.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:             blk.16.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:             blk.16.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:           blk.17.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:           blk.17.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:             blk.17.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:             blk.17.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:        blk.17.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:             blk.17.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:             blk.17.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:           blk.18.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:           blk.18.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.18.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:             blk.18.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:        blk.18.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:             blk.18.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.18.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.19.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:           blk.19.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.19.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:             blk.19.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:        blk.19.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:             blk.19.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.19.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:            blk.2.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:            blk.2.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:              blk.2.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:              blk.2.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:         blk.2.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:              blk.2.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:              blk.2.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.20.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:           blk.20.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.20.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:             blk.20.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:        blk.20.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:             blk.20.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.20.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.21.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:           blk.21.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.21.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:             blk.21.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:        blk.21.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:             blk.21.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.21.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.22.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:           blk.22.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.22.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:             blk.22.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:        blk.22.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:             blk.22.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.22.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.23.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:           blk.23.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.23.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:             blk.23.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:        blk.23.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:             blk.23.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.23.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:            blk.3.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:            blk.3.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:              blk.3.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:              blk.3.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:         blk.3.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:              blk.3.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:              blk.3.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:            blk.4.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:            blk.4.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:              blk.4.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:              blk.4.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:         blk.4.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:              blk.4.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:              blk.4.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:            blk.5.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:            blk.5.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:              blk.5.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:              blk.5.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:         blk.5.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:              blk.5.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:              blk.5.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:            blk.6.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:            blk.6.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:              blk.6.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:              blk.6.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:         blk.6.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:              blk.6.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:              blk.6.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:            blk.7.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:            blk.7.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:              blk.7.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:              blk.7.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:         blk.7.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:              blk.7.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:              blk.7.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:            blk.8.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:            blk.8.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:              blk.8.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:              blk.8.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:         blk.8.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:              blk.8.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:              blk.8.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:            blk.9.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:            blk.9.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:              blk.9.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:              blk.9.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:         blk.9.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:              blk.9.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:              blk.9.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:           blk.24.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:           blk.24.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:             blk.24.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:        blk.24.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:             blk.24.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:           blk.25.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:           blk.25.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:             blk.25.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:        blk.25.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:             blk.25.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:           blk.26.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:           blk.26.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:             blk.26.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:        blk.26.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:             blk.26.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:           blk.27.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:           blk.27.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:             blk.27.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:        blk.27.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:             blk.27.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:           blk.28.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:           blk.28.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:             blk.28.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:        blk.28.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:             blk.28.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:           blk.29.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:           blk.29.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:             blk.29.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:        blk.29.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:             blk.29.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:           blk.30.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:           blk.30.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:             blk.30.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:        blk.30.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:             blk.30.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:           blk.31.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:           blk.31.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:             blk.31.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:        blk.31.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:             blk.31.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str     \n",
      "llama_model_loader: - kv   1:                               general.name str     \n",
      "llama_model_loader: - kv   2:                       llama.context_length u32     \n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32     \n",
      "llama_model_loader: - kv   4:                          llama.block_count u32     \n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32     \n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32     \n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32     \n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32     \n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \n",
      "llama_model_loader: - kv  10:                          general.file_type u32     \n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str     \n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr     \n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr     \n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr     \n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32     \n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32     \n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32     \n",
      "llama_model_loader: - kv  18:               general.quantization_version u32     \n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_print_meta: format           = GGUF V2 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = mostly Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name   = LLaMA v2\n",
      "llm_load_print_meta: BOS token = 1 '<s>'\n",
      "llm_load_print_meta: EOS token = 2 '</s>'\n",
      "llm_load_print_meta: UNK token = 0 '<unk>'\n",
      "llm_load_print_meta: LF token  = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.09 MB\n",
      "llm_load_tensors: mem required  = 3891.34 MB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: kv self size  = 2048.00 MB\n",
      "llama_new_context_with_model: compute buffer total size = 293.88 MB\n",
      "llama_new_context_with_model: max tensor size =   102.54 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3891.95 MB, ( 3892.58 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  2050.00 MB, ( 5942.58 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =   288.02 MB, ( 6230.59 / 10922.67)\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# Model was obtained from here: https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF\n",
    "llm = Llama(model_path=\"./models/llama-2-7b-chat.Q4_K_M.gguf\", n_gpu_layers=50, n_ctx=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =   127.70 ms /   128 runs   (    1.00 ms per token,  1002.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1086.85 ms /    69 tokens (   15.75 ms per token,    63.49 tokens per second)\n",
      "llama_print_timings:        eval time =  3915.62 ms /   127 runs   (   30.83 ms per token,    32.43 tokens per second)\n",
      "llama_print_timings:       total time =  5372.12 ms\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       " * Identify key concepts and relationships. This will help you understand how different elements of calculus are connected and how they contribute to overall understanding of AI. * Break down complex topics into smaller, more manageable pieces. Focus on mastering one concept at a time, rather than trying to tackle everything all at once. * Use interactive tools such as online quizzes or coding challenges to test your knowledge and reinforce key concepts. These can help you identify areas where you need extra practice or review. * Review and practice regularly. Consistency is key when it comes to learning any new skill, including AI."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "output = llm(instruction_prompt)\n",
    "\n",
    "Markdown(output[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok great! Now let's wrap the call to Llama2 into a function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llama2_response(prompt):\n",
    "    \"\"\"\n",
    "    Get the response from the Llama2 model.\n",
    "    \"\"\"\n",
    "    return llm(prompt)[\"choices\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =    74.46 ms /    84 runs   (    0.89 ms per token,  1128.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3493.64 ms /    84 runs   (   41.59 ms per token,    24.04 tokens per second)\n",
      "llama_print_timings:       total time =  3704.84 ms\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "•\tDr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1964)\n",
       "•\tThe Big Lebowski (1998)\n",
       "•\tThe Hunt for Red October (1990)\n",
       "•\tFargo (1996)\n",
       "•\tThis Is Spinal Tap (1984)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = get_llama2_response(\"Write down 5 great dark comedy movies. Output only the names in bullet points.\")\n",
    "\n",
    "Markdown(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering Guide\n",
    "\n",
    "What is a prompt engineering framework? How to think about prompt engineering as a practice?\n",
    "\n",
    "Easy, let's start with our goal:\n",
    "\n",
    "- Finding the best prompt for a particular problem\n",
    "\n",
    "Given this goal we can define prompt engineering as a discipline concerned with stablishing the rules for obtaining the most deterministic outputs possible from a LLM by employing engineering techniques and protocols to enture reproducibility and consistency.\n",
    "\n",
    "***In a simplified way, prompt engineering is the means by which LLMs can be programmed through prompting.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic goal of prompt engineering is designing appropriate inputs for prompting methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering Techniques\n",
    "\n",
    "Now, let's walk through a simplified guide of prompt engineering techniques discussed during the presentation:\n",
    "\n",
    "- [Zero-shot Prompting](https://www.promptingguide.ai/techniques/zeroshot#:~:text=Large%20LLMs%20today,examples%20we%20used%3A)\n",
    "- [Few-shot Prompting](https://www.promptingguide.ai/techniques/fewshot#:~:text=few-shot%20prompting%20can%20be%20used%20as%20a%20technique%20to%20enable%20in-context%20learning%20where%20we%20provide%20demonstrations%20in%20the%20prompt%20to%20steer%20the%20model%20to%20better%20performance)\n",
    "- [Chain-of-Thought](https://www.promptingguide.ai/techniques/cot#:~:text=introduced%20in%20wei%20et%20al.%20(2022)%20(opens%20in%20a%20new%20tab)%2C%20chain-of-thought%20(cot)%20prompting%20enables%20complex%20reasoning%20capabilities%20through%20intermediate%20reasoning%20steps.%20you%20can%20combine%20it%20with%20few-shot%20prompting%20to%20get%20better%20results%20on%20more%20complex%20tasks%20that%20require%20reasoning%20before%20responding.)\n",
    "- [Self-consistency](https://www.promptingguide.ai/techniques/consistency#:~:text=Perhaps%20one%20of,and%20commonsense%20reasoning.)\n",
    "- [Generate Knowledge](https://www.promptingguide.ai/techniques/knowledge#:~:text=LLMs%20continue%20to,as%20commonsense%20reasoning%3F)\n",
    "- [Tree of thoughts (ToT)](https://www.promptingguide.ai/techniques/tot#:~:text=For%20complex%20tasks,with%20language%20models.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-shot Prompting\n",
    "\n",
    "[Zero-shot prompting](https://arxiv.org/pdf/2109.01652.pdf) is when you solve the task without showing any examples of what a solution might look like.\n",
    "\n",
    "For example consider a prompt like:\n",
    "\n",
    "```\n",
    "Classify the sentiment in this sentence as negative or positive:\n",
    "Text: I will go to a vacation\n",
    "Sentiment:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =     3.11 ms /     3 runs   (    1.04 ms per token,   963.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =   948.71 ms /    29 tokens (   32.71 ms per token,    30.57 tokens per second)\n",
      "llama_print_timings:        eval time =    61.40 ms /     2 runs   (   30.70 ms per token,    32.57 tokens per second)\n",
      "llama_print_timings:       total time =  1020.96 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Negative'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "prompt = \"\"\"Classify the sentiment in this sentence as negative or positive:\n",
    "Text: I don't like studying at all!.\n",
    "Sentiment:\"\"\"\n",
    "get_llama2_response(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do a few more like:\n",
    "\n",
    "```\n",
    "What is the capital of Canada?\n",
    "Answer:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =     4.73 ms /     4 runs   (    1.18 ms per token,   844.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =   285.94 ms /    13 tokens (   22.00 ms per token,    45.46 tokens per second)\n",
      "llama_print_timings:        eval time =    90.89 ms /     3 runs   (   30.30 ms per token,    33.01 tokens per second)\n",
      "llama_print_timings:       total time =   389.26 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Ottawa.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"What is the capital of Canada?\\nAnswer (one word):\"\n",
    "get_llama2_response(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and so on and so forth, one can use this as the first try at a model to see what kinds of tasks that LLM can already solve out of the box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-shot Prompting\n",
    "\n",
    "As the complexity of a task increases, you might need to provide information in the form of examples to the LLM.\n",
    "\n",
    "**Few-shot Prompting** is a prompting technique where you show a few examples of what a solution might look like.\n",
    "\n",
    "THe goal is to enable what is called 'in-context learning' where the model improves by learning contextual information about the task at hand.\n",
    "\n",
    "We do that by giving demonstrations that will serve as conditionning for subsequent examples where we would like the model to generate a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =    26.07 ms /    25 runs   (    1.04 ms per token,   958.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =   713.43 ms /    89 tokens (    8.02 ms per token,   124.75 tokens per second)\n",
      "llama_print_timings:        eval time =   735.41 ms /    24 runs   (   30.64 ms per token,    32.63 tokens per second)\n",
      "llama_print_timings:       total time =  1522.03 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The kids started farduddling in the backyard when they heard the ice cream truck arrive.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the example was taken from here: https://www.promptingguide.ai/techniques/fewshot\n",
    "\n",
    "\n",
    "prompt = \"\"\"\n",
    "A \"whatpu\" is a small, furry animal native to Tanzania. An example of a sentence that uses\n",
    "the word whatpu is:\n",
    "We were traveling in Africa and we saw these very cute whatpus.\n",
    "To do a \"farduddle\" means to jump up and down really fast. An example of a sentence that uses\n",
    "the word farduddle is:\n",
    "\"\"\"\n",
    "get_llama2_response(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain-of-Thought\n",
    "\n",
    "This is a prompting technique where we induce step-by-step reasoning and planning within the prompt to enhance performance of the model.\n",
    "\n",
    "According to [Wei et al. (2022)](https://arxiv.org/abs/2201.11903), chain-of-thought (CoT) prompting enables complex reasoning capabilities through intermediate reasoning steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =    59.27 ms /    67 runs   (    0.88 ms per token,  1130.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1003.46 ms /   166 tokens (    6.04 ms per token,   165.43 tokens per second)\n",
      "llama_print_timings:        eval time =  2023.10 ms /    66 runs   (   30.65 ms per token,    32.62 tokens per second)\n",
      "llama_print_timings:       total time =  3188.11 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'If I am 17 years old and Sally is 5 years younger, then Sally is 17-5=12 years old. If Jack is 2 years older than Sally, then Jack is 12+2=14 years old. The answer is 14 years old.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the example was taken from here: https://www.promptingguide.ai/techniques/fewshot\n",
    "\n",
    "prompt_CoT = \"\"\"\n",
    "Q: I have one sister and one brother. I am 20 years of age. My sister is 5 years older and my brother 2 years younger than my sister.\n",
    "How old is my brother?\n",
    "A: If I am 20 years of age and my sister is 5 years older, my sister is 20+5=25 years old. If my brother is 2 years younger than my sister, my brother is 25-2=23 years old. The answer is 23 years old.\n",
    "\n",
    "Q: I have 2 friends, Jack and Sally. Jack is 2 years older than Sally. Sally is 5 years younger than me. I am 17 years old. How old is Jack?\n",
    "A:\n",
    "\"\"\"\n",
    "get_llama2_response(prompt_CoT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can combine few-shot prompting with chain-of-thought to get better results on highly complex tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IN the below examples 7B models will likely fail due to their poor reasoning capabilities, this is where we need models like 13B-chat or\n",
    "much better 70B-chat models for this kind of task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =    22.52 ms /    23 runs   (    0.98 ms per token,  1021.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1653.94 ms /   318 tokens (    5.20 ms per token,   192.27 tokens per second)\n",
      "llama_print_timings:        eval time =   695.01 ms /    22 runs   (   31.59 ms per token,    31.65 tokens per second)\n",
      "llama_print_timings:       total time =  2411.62 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Adding all the odd numbers (5, 13) gives 18. The answer is True.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source: https://www.promptingguide.ai/techniques/cot \n",
    "prompt_few_CoT = \"\"\"\n",
    "Q: The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n",
    "\n",
    "Q:The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
    "A: Adding all the odd numbers (17, 19) gives 36. The answer is True.\n",
    "\n",
    "Q:The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
    "A: Adding all the odd numbers (11, 13) gives 24. The answer is True.\n",
    "\n",
    "Q:The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
    "A: Adding all the odd numbers (17, 9, 13) gives 39. The answer is False.\n",
    "Q:The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "A:\"\"\"\n",
    "get_llama2_response(prompt_few_CoT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-consistency\n",
    "\n",
    "You use few shot prompting and chain of thoughts to sample a bunch of reasoning paths and then use generations to select the most consistent answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   959.71 ms\n",
      "llama_print_timings:      sample time =    26.62 ms /    39 runs   (    0.68 ms per token,  1465.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4087.26 ms /   788 tokens (    5.19 ms per token,   192.79 tokens per second)\n",
      "llama_print_timings:        eval time =  1267.69 ms /    38 runs   (   33.36 ms per token,    29.98 tokens per second)\n",
      "llama_print_timings:       total time =  5428.20 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Olivia has $23 and spent $15 on the bagels, so now she has $23 - $15 = $8. The answer is $8.\n",
      "*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   959.71 ms\n",
      "llama_print_timings:      sample time =    46.22 ms /    60 runs   (    0.77 ms per token,  1298.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2005.11 ms /    60 runs   (   33.42 ms per token,    29.92 tokens per second)\n",
      "llama_print_timings:       total time =  2126.08 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Olivia had $23 and she spent $3 on each of the five bagels, so she spent a total of $3 * 5 = 15 dollars. Now she has 23 - 15 = 8 dollars left. The answer is 8.\n",
      "*\n",
      "Olivia had $23 and spent $3 each on 5 bagels, so in total she spent $3 * 5 = $15. Now she has $23 - $15 = $8 left. The answer is $8.\n",
      "Q: Alexa has 39 pencils in her backpack. She gives 6 to her friend. How many pencils does she have left?\n",
      "A:  Alexa initially had 39 pencils and gave 6 to her friend, so now she has 39 - 6 = 33 pencils\n",
      "*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   959.71 ms\n",
      "llama_print_timings:      sample time =    86.40 ms /   128 runs   (    0.67 ms per token,  1481.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4304.06 ms /   128 runs   (   33.63 ms per token,    29.74 tokens per second)\n",
      "llama_print_timings:       total time =  4532.12 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   959.71 ms\n",
      "llama_print_timings:      sample time =    35.75 ms /    47 runs   (    0.76 ms per token,  1314.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1593.17 ms /    47 runs   (   33.90 ms per token,    29.50 tokens per second)\n",
      "llama_print_timings:       total time =  1695.16 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Olivia had $23. She spent $3 * 5 = $15 on the bagels. So now she has $23 - $15 = $8 left. The answer is $8.\n",
      "*\n",
      "Olivia had $23 to start with. For five bagels, she spent $5 * 3 = $15. Now she has $23 - $15 = $8 left. The answer is $8.\n",
      "*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   959.71 ms\n",
      "llama_print_timings:      sample time =    36.41 ms /    50 runs   (    0.73 ms per token,  1373.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1679.11 ms /    50 runs   (   33.58 ms per token,    29.78 tokens per second)\n",
      "llama_print_timings:       total time =  1774.66 ms\n"
     ]
    }
   ],
   "source": [
    "# source: https://arxiv.org/pdf/2203.11171.pdf\n",
    "few_shot_CoT_prompt = \"\"\"\n",
    "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done,\n",
    "there will be 21 trees. How many trees did the grove workers plant today?\n",
    "A: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted.\n",
    "So, they must have planted 21 - 15 = 6 trees. The answer is 6.\n",
    "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "A: There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.\n",
    "Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
    "A: Leah had 32 chocolates and Leah’s sister had 42. That means there were originally 32 + 42 = 74\n",
    "chocolates. 35 have been eaten. So in total they still have 74 - 35 = 39 chocolates. The answer is 39.\n",
    "Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops\n",
    "did Jason give to Denny?\n",
    "A: Jason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny. The number of\n",
    "lollipops he has given to Denny must have been 20 - 12 = 8 lollipops. The answer is 8.\n",
    "Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does\n",
    "he have now?\n",
    "A: He has 5 toys. He got 2 from mom, so after that he has 5 + 2 = 7 toys. Then he got 2 more from dad, so\n",
    "in total he has 7 + 2 = 9 toys. The answer is 9.\n",
    "Q: There were nine computers in the server room. Five more computers were installed each day, from\n",
    "monday to thursday. How many computers are now in the server room?\n",
    "A: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 =\n",
    "20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers.\n",
    "The answer is 29.\n",
    "Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many\n",
    "golf balls did he have at the end of wednesday?\n",
    "A: Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has 58 - 23 = 35 balls. On\n",
    "Wednesday he lost 2 more so now he has 35 - 2 = 33 balls. The answer is 33.\n",
    "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
    "A: \n",
    "\"\"\"\n",
    "\n",
    "n_reasoning_paths = 5\n",
    "answers = []\n",
    "for i in range(n_reasoning_paths):\n",
    "    response = get_llama2_response(few_shot_CoT_prompt)\n",
    "    answers.append(response)\n",
    "    print(response)\n",
    "    print(\"*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Knowledge\n",
    "\n",
    "This technique is about inserting knowledge into the prompt in order to yield better performance, you use the model to generate knowledge about a field, and then use that generated knowledge to improve its performance on a downstream task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   959.71 ms\n",
      "llama_print_timings:      sample time =    49.58 ms /    54 runs   (    0.92 ms per token,  1089.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3131.76 ms /   450 tokens (    6.96 ms per token,   143.69 tokens per second)\n",
      "llama_print_timings:        eval time =  1689.57 ms /    53 runs   (   31.88 ms per token,    31.37 tokens per second)\n",
      "llama_print_timings:       total time =  4969.81 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   959.71 ms\n",
      "llama_print_timings:      sample time =    91.24 ms /   128 runs   (    0.71 ms per token,  1402.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4095.27 ms /   128 runs   (   31.99 ms per token,    31.26 tokens per second)\n",
      "llama_print_timings:       total time =  4344.44 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' The objective of golf is to hit the ball into each hole in as few strokes as possible, with the lowest score winning. The player with the highest score, or who takes more than the allotted number of strokes to complete the course, loses.', \" The objective in golf is to hit the ball into each hole in as few strokes as possible, with the lowest score winning. Golfers use hand-eye coordination and physical skill to control their shots, maneuver around obstacles, and plan their strategy to achieve a lower score than their opponents.\\nInput: A cat can jump over a house.\\nKnowledge: Cats are capable of jumping long distances, but the height and distance of their jumps depend on various factors such as the cat's size, age, and physical condition, as well as the height and size of any\", ' In professional golf, the winner of each tournament is determined by having the lowest score in that competition. The winner is also awarded prize money based on their finishing position and the size of the purse. The object of the game is to hit the ball into each hole in as few strokes as possible, with the goal of achieving a lower score than your opponents.\\nInput: A bird can fly faster than a car.\\nKnowledge: Birds are capable of flying at speeds of up to 100 mph (161 kph), while the fastest production car ever made, the Bug']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   959.71 ms\n",
      "llama_print_timings:      sample time =    86.65 ms /   128 runs   (    0.68 ms per token,  1477.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4085.51 ms /   128 runs   (   31.92 ms per token,    31.33 tokens per second)\n",
      "llama_print_timings:       total time =  4312.72 ms\n"
     ]
    }
   ],
   "source": [
    "# source: https://www.promptingguide.ai/techniques/knowledge\n",
    "prompt = \"\"\"Input: Greece is larger than mexico.\n",
    "Knowledge: Greece is approximately 131,957 sq km, while Mexico is approximately 1,964,375 sq km, making Mexico 1,389% larger than Greece.\n",
    "Input: Glasses always fog up.\n",
    "Knowledge: Condensation occurs on eyeglass lenses when water vapor from your sweat, breath, and ambient humidity lands on a cold surface, cools, and then changes into tiny drops of liquid, forming a film that you see as fog. Your lenses will be relatively cool compared to your breath, especially when the outside air is cold.\n",
    "Input: A fish is capable of thinking.\n",
    "Knowledge: Fish are more intelligent than they appear. In many areas, such as memory, their cognitive powers match or exceed those of ’higher’ vertebrates including non-human primates. Fish’s long-term memories help them keep track of complex social relationships.\n",
    "Input: A common effect of smoking lots of cigarettes in one’s lifetime is a higher than normal chance of getting lung cancer.\n",
    "Knowledge: Those who consistently averaged less than one cigarette per day over their lifetime had nine times the risk of dying from lung cancer than never smokers. Among people who smoked between one and 10 cigarettes per day, the risk of dying from lung cancer was nearly 12 times higher than that of never smokers.\n",
    "Input: A rock is the same size as a pebble.\n",
    "Knowledge: A pebble is a clast of rock with a particle size of 4 to 64 millimetres based on the Udden-Wentworth scale of sedimentology. Pebbles are generally considered larger than granules (2 to 4 millimetres diameter) and smaller than cobbles (64 to 256 millimetres diameter).\n",
    "Input: Part of golf is trying to get a higher point total than others.\n",
    "Knowledge:\"\"\"\n",
    "knowledges = []\n",
    "num_knowledges = 3\n",
    "for i in range(num_knowledges):\n",
    "    knowledges.append(get_llama2_response(prompt))\n",
    "\n",
    "print(knowledges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We integrate the knowledge to get a prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Yes. The objective of golf is to play a set of holes in the least number of strokes, which means trying to get a lower score than your opponents or the course par. The player with the lowest score at the end of the round wins the game. So, part of golf is indeed trying to get a higher point total than others by scoring as few strokes as possible on each hole and overall in the round.'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_print_timings:        load time =   959.71 ms\n",
      "llama_print_timings:      sample time =    64.76 ms /    88 runs   (    0.74 ms per token,  1358.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  3676.39 ms /    88 runs   (   41.78 ms per token,    23.94 tokens per second)\n",
      "llama_print_timings:       total time =  3859.65 ms\n"
     ]
    }
   ],
   "source": [
    "# In this example the model says yes but in its reasoning it actually achieves the correct answer.\n",
    "\n",
    "# source: https://www.promptingguide.ai/techniques/knowledge\n",
    "prompt = \"\"\"Question: Part of golf is trying to get a higher point total than others. Yes or No?\n",
    "Knowledge: The objective of golf is to play a set of holes in the least number of strokes. A round of golf typically consists of 18 holes. Each hole is played once in the round on a standard golf course. Each stroke is counted as one point, and the total number of strokes is used to determine the winner of the game.\n",
    "Explain and Answer: \"\"\"\n",
    "\n",
    "get_llama2_response(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree of thoughts (ToT)\n",
    "\n",
    "\n",
    "ToT [Long (2023)](https://arxiv.org/pdf/2305.08291.pdf) is a framework that generalizes over chain-of-thought prompting and encourages exploration over thoughts that ser as intermediate steps for general problem solving with LMs.\n",
    "\n",
    "This technique involves a framework where a tree of thoughts is maintained, where a thought here means a coherent sequence of steps that represent moving forward in the solution. The LMs are given the ability to self-evaluate on how intermediate thoughts contribute towards progress solving the problem through a deliberate reasoning process which involves combining this evaluation ability with search algorithms to allow for backtracking and lookahead over the space of possible thoughts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/ToT_framework.png)\n",
    "Image Source: [Yao et al. (2023)](https://arxiv.org/pdf/2305.08291.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Many More but That's Enough\n",
    "\n",
    "There are many more prompt engineering techniques that grow in complexity like:\n",
    "- [Retrieval Augmented Generation (RAG)](https://www.promptingguide.ai/techniques/rag)\n",
    "- [Automatic Prompt Engineer](https://www.promptingguide.ai/techniques/ape)\n",
    "- [Active Prompt](https://www.promptingguide.ai/techniques/activeprompt)\n",
    "- [Directional Stimulus Prompting](https://www.promptingguide.ai/techniques/dsp)\n",
    "- [React Prompting](https://www.promptingguide.ai/techniques/react)\n",
    "- [Mulitmodal CoT](https://www.promptingguide.ai/techniques/multimodalcot)\n",
    "- [Graph Prompting](https://www.promptingguide.ai/techniques/graph)\n",
    "- [Least-to-Most prompting](https://arxiv.org/abs/2205.10625)\n",
    "- [Step-back-Prompting](https://cobusgreyling.medium.com/a-new-prompt-engineering-technique-has-been-introduced-called-step-back-prompting-b00e8954cacb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering Practical Case Study\n",
    "\n",
    "Now, let's take the concepts and ideas discussed in this lesson, and apply them to an actual problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a simple example, imagine you want to extract dates from text. You might set up a LLM to do that by first creating a set of examples of phrases with dates, something we can start with ChatGPT itself to help us get some nice data to run experiments with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ah, the universe flows like a river, carrying with it the wisdom of millennia. On this beautiful day, the 25th of September, 2023, we are reminded of the cyclical nature of time and the constant evolution of our souls. Let us embrace the energy of this moment, the 14th day of July in the year 2024, and open our hearts to the infinite possibilities that lie ahead.',\n",
       " 'The passage speaks of the timeless wisdom that was revealed on the 12th day of October in 1998, when the universe seemed to align and offer a glimpse into the eternal truths that govern our existence. It was a day marked by profound insights and serendipitous connections, reminding us that the universe is always speaking to us in its own mysterious language, waiting for us to decipher its hidden messages and unlock the secrets of life.\\n',\n",
       " 'The 25th of July, 2025, marks a significant turning point in the cosmic dance of existence, as the universe aligns to bestow wisdom upon those seeking enlightenment. On 2025-07-25, the energy of the cosmos flows with purpose and clarity, illuminating the path to universal truths. As we contemplate the mysteries of life, let us remember the profound significance of July 25, 2025, and the boundless potential it holds for those attuned to',\n",
       " 'As the sun sets on the 21st day of September in the year 2030, let us reflect on the infinite possibilities that exist within each passing moment. Embrace the beauty of the present, for it holds the key to unlocking the mysteries of the future and the wisdom of the past.',\n",
       " 'In the grand scheme of existence, the 12th of November, 2023, is but a fleeting moment, a mere blip in the endless flow of time. However, for those in the midst of their journey, the 12th day of November in the year 2023 holds the potential for profound experiences and transformative revelations, illuminating the path towards greater understanding and wisdom.',\n",
       " 'In the grand tapestry of existence, the 11th of July, 2023, stands as a pivotal moment in the unfolding of the cosmos, where the forces of destiny converge and the hand of fate guides us towards the infinite possibilities of tomorrow.',\n",
       " 'In the grand tapestry of existence, the 18th of September, 2023, marks a pivotal moment in the cosmic dance of time and space. As the year 2023 unfolds, the 13th of December whispers ancient secrets and infinite possibilities, weaving together the fabric of destiny and evolution.',\n",
       " 'Certainly! Here\\'s a phrase with a complete date in different formats: \"On the 12th of December 2022, as we journey through the year 2022, let us embrace the opportunities that lie ahead and reflect on the lessons learned from the year 2021.\"',\n",
       " 'In the grand tapestry of existence, every moment from the 15th of August, 2022, to the present day is woven together in a beautiful display of interconnectedness. Each experience, whether joyful or challenging, contributes to the intricate design of our lives, shaping who we are and guiding us toward our destiny. Embrace the 22nd day of September, 2023, as an opportunity to align with the universal flow and manifest your deepest desires into reality.',\n",
       " 'Ah, the mysteries of existence unfold before us like the unfolding of a delicate origami flower, each petal revealing the secrets of life and the universe. On this 24th day of October, in the year 2022, let us embrace the knowledge that every moment holds the potential for transformation and growth, reminding us that we are all interconnected with the energy of the cosmos.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "def get_response(prompt_question):\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(model=\"gpt-3.5-turbo-1106\", \n",
    "                             messages=\n",
    "                             [\n",
    "                                 {\"role\": \"system\", \"content\": \"You are a savy guru with knowledge about existence and the secrets of life.\"},\n",
    "                                 {\"role\": \"user\", \"content\": prompt}   \n",
    "                             ],\n",
    "                             max_tokens=100,\n",
    "                             temperature=0.9,\n",
    "                             n = 1)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "num_samples = 10\n",
    "phrases_with_dates = []\n",
    "prompt = \"Create a 1 paragraph phrase containing a complete date (day month  and year) anywhere in the text formatted in different ways.\"\n",
    "for i in range(num_samples):\n",
    "    phrases_with_dates.append(get_response(prompt))\n",
    "phrases_with_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok perfect! Now that we have this evaluation set, we can set up a simple experiment by first creating a demonstration set with our prompt candidates.\n",
    "\n",
    "We'll begin with a baseline using only zero-shot prompt examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_prompts = [\"Extract the date from this text as DD-MM-YYYY\", \n",
    "                     \"Fetch the date from this text as DD-MM-YYYY\",\n",
    "                     \"Get the date from this phrase as DD-MM-YYYY\",\n",
    "                     ]\n",
    "\n",
    "few_shot_prompts = [\n",
    "    \"\"\"\n",
    "    Example 1:\n",
    "    Input: \"I have an appointment on 10th of July, 2021.\"\n",
    "    Output: \"10-07-2021\"\n",
    "    \n",
    "    Example 2:\n",
    "    Input: \"Her birthday is on 23rd February 1999.\"\n",
    "    Output: \"23-02-1999\"\n",
    "    \n",
    "    Example 3:\n",
    "    Input: \"We met on a sunny day, 05 May 2018.\"\n",
    "    Output: \"05-05-2018\"\n",
    "    Final Query:\n",
    "\n",
    "    Input: \"Fetch the date from this text as DD-MM-YYYY.\"\n",
    "    Output:\n",
    "    \"\"\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we have our candidates, so let's now test them creating a table with the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =    27.76 ms /    31 runs   (    0.90 ms per token,  1116.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1969.45 ms /   114 tokens (   17.28 ms per token,    57.88 tokens per second)\n",
      "llama_print_timings:        eval time =   913.87 ms /    30 runs   (   30.46 ms per token,    32.83 tokens per second)\n",
      "llama_print_timings:       total time =  2965.47 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =   115.87 ms /   128 runs   (    0.91 ms per token,  1104.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =   514.31 ms /   114 tokens (    4.51 ms per token,   221.66 tokens per second)\n",
      "llama_print_timings:        eval time =  3904.87 ms /   127 runs   (   30.75 ms per token,    32.52 tokens per second)\n",
      "llama_print_timings:       total time =  4741.81 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =    32.26 ms /    36 runs   (    0.90 ms per token,  1115.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =   512.55 ms /   113 tokens (    4.54 ms per token,   220.47 tokens per second)\n",
      "llama_print_timings:        eval time =  1072.35 ms /    35 runs   (   30.64 ms per token,    32.64 tokens per second)\n",
      "llama_print_timings:       total time =  1672.65 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1430.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1155.82 ms /   269 tokens (    4.30 ms per token,   232.73 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  1157.45 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "Llama.generate: prefix-match hit\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =    98.80 ms /   128 runs   (    0.77 ms per token,  1295.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =   521.16 ms /   122 tokens (    4.27 ms per token,   234.09 tokens per second)\n",
      "llama_print_timings:        eval time =  3909.88 ms /   127 runs   (   30.79 ms per token,    32.48 tokens per second)\n",
      "llama_print_timings:       total time =  4703.91 ms\n",
      "\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =     8.70 ms /    13 runs   (    0.67 ms per token,  1494.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =   521.24 ms /   122 tokens (    4.27 ms per token,   234.06 tokens per second)\n",
      "llama_print_timings:        eval time =   367.20 ms /    12 runs   (   30.60 ms per token,    32.68 tokens per second)\n",
      "llama_print_timings:       total time =   911.51 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =     8.40 ms /    12 runs   (    0.70 ms per token,  1429.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =   520.39 ms /   121 tokens (    4.30 ms per token,   232.52 tokens per second)\n",
      "llama_print_timings:        eval time =   336.38 ms /    11 runs   (   30.58 ms per token,    32.70 tokens per second)\n",
      "llama_print_timings:       total time =   879.43 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =    34.90 ms /    51 runs   (    0.68 ms per token,  1461.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1170.71 ms /   277 tokens (    4.23 ms per token,   236.61 tokens per second)\n",
      "llama_print_timings:        eval time =  1562.39 ms /    50 runs   (   31.25 ms per token,    32.00 tokens per second)\n",
      "llama_print_timings:       total time =  2826.10 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =    24.95 ms /    36 runs   (    0.69 ms per token,  1442.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =   642.58 ms /   138 tokens (    4.66 ms per token,   214.76 tokens per second)\n",
      "llama_print_timings:        eval time =  1071.11 ms /    35 runs   (   30.60 ms per token,    32.68 tokens per second)\n",
      "llama_print_timings:       total time =  1781.16 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =    19.89 ms /    29 runs   (    0.69 ms per token,  1457.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =   642.68 ms /   138 tokens (    4.66 ms per token,   214.73 tokens per second)\n",
      "llama_print_timings:        eval time =   858.77 ms /    28 runs   (   30.67 ms per token,    32.60 tokens per second)\n",
      "llama_print_timings:       total time =  1552.75 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =    43.31 ms /    63 runs   (    0.69 ms per token,  1454.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =   640.44 ms /   137 tokens (    4.67 ms per token,   213.92 tokens per second)\n",
      "llama_print_timings:        eval time =  1900.48 ms /    62 runs   (   30.65 ms per token,    32.62 tokens per second)\n",
      "llama_print_timings:       total time =  2654.41 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =    18.77 ms /    28 runs   (    0.67 ms per token,  1491.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1292.75 ms /   293 tokens (    4.41 ms per token,   226.65 tokens per second)\n",
      "llama_print_timings:        eval time =   848.26 ms /    27 runs   (   31.42 ms per token,    31.83 tokens per second)\n",
      "llama_print_timings:       total time =  2191.05 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =    49.33 ms /    45 runs   (    1.10 ms per token,   912.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =   399.63 ms /    81 tokens (    4.93 ms per token,   202.69 tokens per second)\n",
      "llama_print_timings:        eval time =  1346.71 ms /    44 runs   (   30.61 ms per token,    32.67 tokens per second)\n",
      "llama_print_timings:       total time =  1880.82 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =    22.19 ms /    22 runs   (    1.01 ms per token,   991.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =   396.39 ms /    81 tokens (    4.89 ms per token,   204.35 tokens per second)\n",
      "llama_print_timings:        eval time =   642.69 ms /    21 runs   (   30.60 ms per token,    32.68 tokens per second)\n",
      "llama_print_timings:       total time =  1099.95 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =    95.48 ms /   128 runs   (    0.75 ms per token,  1340.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =   398.56 ms /    80 tokens (    4.98 ms per token,   200.72 tokens per second)\n",
      "llama_print_timings:        eval time =  3903.03 ms /   127 runs   (   30.73 ms per token,    32.54 tokens per second)\n",
      "llama_print_timings:       total time =  4571.00 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =    62.01 ms /    75 runs   (    0.83 ms per token,  1209.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1027.21 ms /   236 tokens (    4.35 ms per token,   229.75 tokens per second)\n",
      "llama_print_timings:        eval time =  2316.16 ms /    74 runs   (   31.30 ms per token,    31.95 tokens per second)\n",
      "llama_print_timings:       total time =  3515.78 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "Llama.generate: prefix-match hit\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =    20.22 ms /    29 runs   (    0.70 ms per token,  1434.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =   510.82 ms /   106 tokens (    4.82 ms per token,   207.51 tokens per second)\n",
      "llama_print_timings:        eval time =   857.89 ms /    28 runs   (   30.64 ms per token,    32.64 tokens per second)\n",
      "llama_print_timings:       total time =  1423.99 ms\n",
      "\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =    72.92 ms /   107 runs   (    0.68 ms per token,  1467.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =   510.66 ms /   106 tokens (    4.82 ms per token,   207.58 tokens per second)\n",
      "llama_print_timings:        eval time =  3250.90 ms /   106 runs   (   30.67 ms per token,    32.61 tokens per second)\n",
      "llama_print_timings:       total time =  3958.51 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =     7.40 ms /    10 runs   (    0.74 ms per token,  1351.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =   510.49 ms /   105 tokens (    4.86 ms per token,   205.68 tokens per second)\n",
      "llama_print_timings:        eval time =   277.59 ms /     9 runs   (   30.84 ms per token,    32.42 tokens per second)\n",
      "llama_print_timings:       total time =   807.30 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "Llama.generate: prefix-match hit\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =    56.74 ms /    49 runs   (    1.16 ms per token,   863.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1160.62 ms /   261 tokens (    4.45 ms per token,   224.88 tokens per second)\n",
      "llama_print_timings:        eval time =  1509.65 ms /    48 runs   (   31.45 ms per token,    31.80 tokens per second)\n",
      "llama_print_timings:       total time =  2823.73 ms\n",
      "\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =     9.81 ms /    12 runs   (    0.82 ms per token,  1222.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =   395.23 ms /    78 tokens (    5.07 ms per token,   197.36 tokens per second)\n",
      "llama_print_timings:        eval time =   337.37 ms /    11 runs   (   30.67 ms per token,    32.61 tokens per second)\n",
      "llama_print_timings:       total time =   762.01 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "Llama.generate: prefix-match hit\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =   120.18 ms /   128 runs   (    0.94 ms per token,  1065.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =   394.66 ms /    78 tokens (    5.06 ms per token,   197.64 tokens per second)\n",
      "llama_print_timings:        eval time =  3915.19 ms /   127 runs   (   30.83 ms per token,    32.44 tokens per second)\n",
      "llama_print_timings:       total time =  4649.28 ms\n",
      "\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =    12.45 ms /    18 runs   (    0.69 ms per token,  1446.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =   394.91 ms /    77 tokens (    5.13 ms per token,   194.98 tokens per second)\n",
      "llama_print_timings:        eval time =   517.77 ms /    17 runs   (   30.46 ms per token,    32.83 tokens per second)\n",
      "llama_print_timings:       total time =   945.96 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =    26.83 ms /    17 runs   (    1.58 ms per token,   633.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1029.03 ms /   233 tokens (    4.42 ms per token,   226.43 tokens per second)\n",
      "llama_print_timings:        eval time =   500.97 ms /    16 runs   (   31.31 ms per token,    31.94 tokens per second)\n",
      "llama_print_timings:       total time =  1611.83 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =    34.46 ms /    47 runs   (    0.73 ms per token,  1364.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =   406.16 ms /    92 tokens (    4.41 ms per token,   226.51 tokens per second)\n",
      "llama_print_timings:        eval time =  1405.02 ms /    46 runs   (   30.54 ms per token,    32.74 tokens per second)\n",
      "llama_print_timings:       total time =  1904.95 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =    88.20 ms /   128 runs   (    0.69 ms per token,  1451.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =   406.19 ms /    92 tokens (    4.42 ms per token,   226.49 tokens per second)\n",
      "llama_print_timings:        eval time =  3895.94 ms /   127 runs   (   30.68 ms per token,    32.60 tokens per second)\n",
      "llama_print_timings:       total time =  4543.58 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =     9.05 ms /    13 runs   (    0.70 ms per token,  1436.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =   402.39 ms /    91 tokens (    4.42 ms per token,   226.15 tokens per second)\n",
      "llama_print_timings:        eval time =   362.79 ms /    12 runs   (   30.23 ms per token,    33.08 tokens per second)\n",
      "llama_print_timings:       total time =   789.76 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =    57.44 ms /    84 runs   (    0.68 ms per token,  1462.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1027.43 ms /   247 tokens (    4.16 ms per token,   240.40 tokens per second)\n",
      "llama_print_timings:        eval time =  2601.98 ms /    83 runs   (   31.35 ms per token,    31.90 tokens per second)\n",
      "llama_print_timings:       total time =  3786.82 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =    39.19 ms /    47 runs   (    0.83 ms per token,  1199.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =   401.60 ms /    85 tokens (    4.72 ms per token,   211.65 tokens per second)\n",
      "llama_print_timings:        eval time =  1401.21 ms /    46 runs   (   30.46 ms per token,    32.83 tokens per second)\n",
      "llama_print_timings:       total time =  1909.09 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "Llama.generate: prefix-match hit\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =    82.01 ms /    93 runs   (    0.88 ms per token,  1133.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =   403.33 ms /    85 tokens (    4.75 ms per token,   210.75 tokens per second)\n",
      "llama_print_timings:        eval time =  2870.16 ms /    92 runs   (   31.20 ms per token,    32.05 tokens per second)\n",
      "llama_print_timings:       total time =  3511.11 ms\n",
      "\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =    25.76 ms /    36 runs   (    0.72 ms per token,  1397.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =   399.15 ms /    84 tokens (    4.75 ms per token,   210.45 tokens per second)\n",
      "llama_print_timings:        eval time =  1068.86 ms /    35 runs   (   30.54 ms per token,    32.75 tokens per second)\n",
      "llama_print_timings:       total time =  1540.86 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =    93.62 ms /   128 runs   (    0.73 ms per token,  1367.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1042.76 ms /   240 tokens (    4.34 ms per token,   230.16 tokens per second)\n",
      "llama_print_timings:        eval time =  4107.37 ms /   127 runs   (   32.34 ms per token,    30.92 tokens per second)\n",
      "llama_print_timings:       total time =  5423.93 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =   137.83 ms /   128 runs   (    1.08 ms per token,   928.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =   632.72 ms /   129 tokens (    4.90 ms per token,   203.88 tokens per second)\n",
      "llama_print_timings:        eval time =  4049.02 ms /   127 runs   (   31.88 ms per token,    31.37 tokens per second)\n",
      "llama_print_timings:       total time =  5122.57 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "Llama.generate: prefix-match hit\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =   105.79 ms /   128 runs   (    0.83 ms per token,  1209.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =   632.07 ms /   129 tokens (    4.90 ms per token,   204.09 tokens per second)\n",
      "llama_print_timings:        eval time =  4007.94 ms /   127 runs   (   31.56 ms per token,    31.69 tokens per second)\n",
      "llama_print_timings:       total time =  4970.38 ms\n",
      "\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =    90.46 ms /   128 runs   (    0.71 ms per token,  1414.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =   515.43 ms /   128 tokens (    4.03 ms per token,   248.34 tokens per second)\n",
      "llama_print_timings:        eval time =  3945.43 ms /   127 runs   (   31.07 ms per token,    32.19 tokens per second)\n",
      "llama_print_timings:       total time =  4720.86 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =    72.66 ms /    39 runs   (    1.86 ms per token,   536.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1173.99 ms /   284 tokens (    4.13 ms per token,   241.91 tokens per second)\n",
      "llama_print_timings:        eval time =  1260.70 ms /    38 runs   (   33.18 ms per token,    30.14 tokens per second)\n",
      "llama_print_timings:       total time =  2680.66 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =   117.76 ms /   128 runs   (    0.92 ms per token,  1086.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =   506.29 ms /   104 tokens (    4.87 ms per token,   205.42 tokens per second)\n",
      "llama_print_timings:        eval time =  3966.42 ms /   127 runs   (   31.23 ms per token,    32.02 tokens per second)\n",
      "llama_print_timings:       total time =  4843.86 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =    98.28 ms /    56 runs   (    1.75 ms per token,   569.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =   507.27 ms /   104 tokens (    4.88 ms per token,   205.02 tokens per second)\n",
      "llama_print_timings:        eval time =  1792.65 ms /    55 runs   (   32.59 ms per token,    30.68 tokens per second)\n",
      "llama_print_timings:       total time =  2643.64 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =     3.88 ms /     1 runs   (    3.88 ms per token,   257.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =   510.34 ms /   103 tokens (    4.95 ms per token,   201.83 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   522.83 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5521.59 ms\n",
      "llama_print_timings:      sample time =    38.56 ms /    45 runs   (    0.86 ms per token,  1166.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1164.16 ms /   259 tokens (    4.49 ms per token,   222.48 tokens per second)\n",
      "llama_print_timings:        eval time =  1382.83 ms /    44 runs   (   31.43 ms per token,    31.82 tokens per second)\n",
      "llama_print_timings:       total time =  2665.06 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ah, the universe flows like a river, carrying ...</td>\n",
       "      <td>Extract the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\n\\nThe answer is: 25-09-2023 and 14-07-2024.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ah, the universe flows like a river, carrying ...</td>\n",
       "      <td>Fetch the date from this text as DD-MM-YYYY</td>\n",
       "      <td>In this exquisite month of May, the 13th day ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ah, the universe flows like a river, carrying ...</td>\n",
       "      <td>Get the date from this phrase as DD-MM-YYYY</td>\n",
       "      <td>\\n\\nIn the provided phrase, what is the date?\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ah, the universe flows like a river, carrying ...</td>\n",
       "      <td>\\n    Example 1:\\n    Input: \"I have an appoin...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The passage speaks of the timeless wisdom that...</td>\n",
       "      <td>Extract the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The passage speaks of the timeless wisdom that...</td>\n",
       "      <td>Fetch the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The passage speaks of the timeless wisdom that...</td>\n",
       "      <td>Get the date from this phrase as DD-MM-YYYY</td>\n",
       "      <td>Answer: 12-10-98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The passage speaks of the timeless wisdom that...</td>\n",
       "      <td>\\n    Example 1:\\n    Input: \"I have an appoin...</td>\n",
       "      <td>\\nPlease write a JavaScript function that take...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The 25th of July, 2025, marks a significant tu...</td>\n",
       "      <td>Extract the date from this text as DD-MM-YYYY</td>\n",
       "      <td>its cosmic rhythms.\\n\\n\\nI extracted the date...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The 25th of July, 2025, marks a significant tu...</td>\n",
       "      <td>Fetch the date from this text as DD-MM-YYYY</td>\n",
       "      <td>its celestial rhythms.\\n\\nThe text contains t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The 25th of July, 2025, marks a significant tu...</td>\n",
       "      <td>Get the date from this phrase as DD-MM-YYYY</td>\n",
       "      <td>its celestial vibrations.\\nA) July 25, 2025\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The 25th of July, 2025, marks a significant tu...</td>\n",
       "      <td>\\n    Example 1:\\n    Input: \"I have an appoin...</td>\n",
       "      <td>its celestial vibrations.\\n    The output dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>As the sun sets on the 21st day of September i...</td>\n",
       "      <td>Extract the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\n\\n\\n\\nThe output should be: 21-09-2030\\n\\nNo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>As the sun sets on the 21st day of September i...</td>\n",
       "      <td>Fetch the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\n\\nThe text given contains the following date...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>As the sun sets on the 21st day of September i...</td>\n",
       "      <td>Get the date from this phrase as DD-MM-YYYY</td>\n",
       "      <td>\\n\\nTo get the date from this phrase you can u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>As the sun sets on the 21st day of September i...</td>\n",
       "      <td>\\n    Example 1:\\n    Input: \"I have an appoin...</td>\n",
       "      <td>As we stand at this crossroads, may we find s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>In the grand scheme of existence, the 12th of ...</td>\n",
       "      <td>Extract the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\n\\nPlease help me with this problem. How do I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>In the grand scheme of existence, the 12th of ...</td>\n",
       "      <td>Fetch the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\nThe text contains the following date: 12-11-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>In the grand scheme of existence, the 12th of ...</td>\n",
       "      <td>Get the date from this phrase as DD-MM-YYYY</td>\n",
       "      <td>\\nWhat is the date in this phrase?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>In the grand scheme of existence, the 12th of ...</td>\n",
       "      <td>\\n    Example 1:\\n    Input: \"I have an appoin...</td>\n",
       "      <td>\\n    Note: The input text may contain any num...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>In the grand tapestry of existence, the 11th o...</td>\n",
       "      <td>Extract the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\n\\nCan you please help me with this task?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>In the grand tapestry of existence, the 11th o...</td>\n",
       "      <td>Fetch the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\nHow can I get the date from this text?\\nI wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>In the grand tapestry of existence, the 11th o...</td>\n",
       "      <td>Get the date from this phrase as DD-MM-YYYY</td>\n",
       "      <td>\\n\\nThe answer is: 11-07-2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>In the grand tapestry of existence, the 11th o...</td>\n",
       "      <td>\\n    Example 1:\\n    Input: \"I have an appoin...</td>\n",
       "      <td>\\n    Output: \"11-07-2023\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>In the grand tapestry of existence, the 18th o...</td>\n",
       "      <td>Extract the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\n\\nIn this task, you need to extract the date...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>In the grand tapestry of existence, the 18th o...</td>\n",
       "      <td>Fetch the date from this text as DD-MM-YYYY</td>\n",
       "      <td>These are the moments that define our lives, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>In the grand tapestry of existence, the 18th o...</td>\n",
       "      <td>Get the date from this phrase as DD-MM-YYYY</td>\n",
       "      <td>\\nIn this phrase, what is the date written as?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>In the grand tapestry of existence, the 18th o...</td>\n",
       "      <td>\\n    Example 1:\\n    Input: \"I have an appoin...</td>\n",
       "      <td>The 8th of March, 2024, is a tapestry woven f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Certainly! Here's a phrase with a complete dat...</td>\n",
       "      <td>Extract the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\n \\nNow, let's extract the date in DD-MM-YYYY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Certainly! Here's a phrase with a complete dat...</td>\n",
       "      <td>Fetch the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\nЉ�! Here are three dates in different format...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Certainly! Here's a phrase with a complete dat...</td>\n",
       "      <td>Get the date from this phrase as DD-MM-YYYY</td>\n",
       "      <td>\\n Rearrange these letters to form a correct d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Certainly! Here's a phrase with a complete dat...</td>\n",
       "      <td>\\n    Example 1:\\n    Input: \"I have an appoin...</td>\n",
       "      <td>\\n    * Date in DD-MM-YYYY format: 12-12-2022\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>In the grand tapestry of existence, every mome...</td>\n",
       "      <td>Extract the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\nThis text contains the date 15-08-2022 and 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>In the grand tapestry of existence, every mome...</td>\n",
       "      <td>Fetch the date from this text as DD-MM-YYYY</td>\n",
       "      <td>The time is now to let go of old patterns and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>In the grand tapestry of existence, every mome...</td>\n",
       "      <td>Get the date from this phrase as DD-MM-YYYY</td>\n",
       "      <td>Take a moment to reflect on the journey that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>In the grand tapestry of existence, every mome...</td>\n",
       "      <td>\\n    Example 1:\\n    Input: \"I have an appoin...</td>\n",
       "      <td>For within the cosmic web of time, every seco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Ah, the mysteries of existence unfold before u...</td>\n",
       "      <td>Extract the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Ah, the mysteries of existence unfold before u...</td>\n",
       "      <td>Fetch the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\nThe date you want is: 24-10-2022\\nThe text m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Ah, the mysteries of existence unfold before u...</td>\n",
       "      <td>Get the date from this phrase as DD-MM-YYYY</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Ah, the mysteries of existence unfold before u...</td>\n",
       "      <td>\\n    Example 1:\\n    Input: \"I have an appoin...</td>\n",
       "      <td>May our actions be guided by wisdom and compa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               phrase  \\\n",
       "0   Ah, the universe flows like a river, carrying ...   \n",
       "1   Ah, the universe flows like a river, carrying ...   \n",
       "2   Ah, the universe flows like a river, carrying ...   \n",
       "3   Ah, the universe flows like a river, carrying ...   \n",
       "4   The passage speaks of the timeless wisdom that...   \n",
       "5   The passage speaks of the timeless wisdom that...   \n",
       "6   The passage speaks of the timeless wisdom that...   \n",
       "7   The passage speaks of the timeless wisdom that...   \n",
       "8   The 25th of July, 2025, marks a significant tu...   \n",
       "9   The 25th of July, 2025, marks a significant tu...   \n",
       "10  The 25th of July, 2025, marks a significant tu...   \n",
       "11  The 25th of July, 2025, marks a significant tu...   \n",
       "12  As the sun sets on the 21st day of September i...   \n",
       "13  As the sun sets on the 21st day of September i...   \n",
       "14  As the sun sets on the 21st day of September i...   \n",
       "15  As the sun sets on the 21st day of September i...   \n",
       "16  In the grand scheme of existence, the 12th of ...   \n",
       "17  In the grand scheme of existence, the 12th of ...   \n",
       "18  In the grand scheme of existence, the 12th of ...   \n",
       "19  In the grand scheme of existence, the 12th of ...   \n",
       "20  In the grand tapestry of existence, the 11th o...   \n",
       "21  In the grand tapestry of existence, the 11th o...   \n",
       "22  In the grand tapestry of existence, the 11th o...   \n",
       "23  In the grand tapestry of existence, the 11th o...   \n",
       "24  In the grand tapestry of existence, the 18th o...   \n",
       "25  In the grand tapestry of existence, the 18th o...   \n",
       "26  In the grand tapestry of existence, the 18th o...   \n",
       "27  In the grand tapestry of existence, the 18th o...   \n",
       "28  Certainly! Here's a phrase with a complete dat...   \n",
       "29  Certainly! Here's a phrase with a complete dat...   \n",
       "30  Certainly! Here's a phrase with a complete dat...   \n",
       "31  Certainly! Here's a phrase with a complete dat...   \n",
       "32  In the grand tapestry of existence, every mome...   \n",
       "33  In the grand tapestry of existence, every mome...   \n",
       "34  In the grand tapestry of existence, every mome...   \n",
       "35  In the grand tapestry of existence, every mome...   \n",
       "36  Ah, the mysteries of existence unfold before u...   \n",
       "37  Ah, the mysteries of existence unfold before u...   \n",
       "38  Ah, the mysteries of existence unfold before u...   \n",
       "39  Ah, the mysteries of existence unfold before u...   \n",
       "\n",
       "                                               prompt  \\\n",
       "0       Extract the date from this text as DD-MM-YYYY   \n",
       "1         Fetch the date from this text as DD-MM-YYYY   \n",
       "2         Get the date from this phrase as DD-MM-YYYY   \n",
       "3   \\n    Example 1:\\n    Input: \"I have an appoin...   \n",
       "4       Extract the date from this text as DD-MM-YYYY   \n",
       "5         Fetch the date from this text as DD-MM-YYYY   \n",
       "6         Get the date from this phrase as DD-MM-YYYY   \n",
       "7   \\n    Example 1:\\n    Input: \"I have an appoin...   \n",
       "8       Extract the date from this text as DD-MM-YYYY   \n",
       "9         Fetch the date from this text as DD-MM-YYYY   \n",
       "10        Get the date from this phrase as DD-MM-YYYY   \n",
       "11  \\n    Example 1:\\n    Input: \"I have an appoin...   \n",
       "12      Extract the date from this text as DD-MM-YYYY   \n",
       "13        Fetch the date from this text as DD-MM-YYYY   \n",
       "14        Get the date from this phrase as DD-MM-YYYY   \n",
       "15  \\n    Example 1:\\n    Input: \"I have an appoin...   \n",
       "16      Extract the date from this text as DD-MM-YYYY   \n",
       "17        Fetch the date from this text as DD-MM-YYYY   \n",
       "18        Get the date from this phrase as DD-MM-YYYY   \n",
       "19  \\n    Example 1:\\n    Input: \"I have an appoin...   \n",
       "20      Extract the date from this text as DD-MM-YYYY   \n",
       "21        Fetch the date from this text as DD-MM-YYYY   \n",
       "22        Get the date from this phrase as DD-MM-YYYY   \n",
       "23  \\n    Example 1:\\n    Input: \"I have an appoin...   \n",
       "24      Extract the date from this text as DD-MM-YYYY   \n",
       "25        Fetch the date from this text as DD-MM-YYYY   \n",
       "26        Get the date from this phrase as DD-MM-YYYY   \n",
       "27  \\n    Example 1:\\n    Input: \"I have an appoin...   \n",
       "28      Extract the date from this text as DD-MM-YYYY   \n",
       "29        Fetch the date from this text as DD-MM-YYYY   \n",
       "30        Get the date from this phrase as DD-MM-YYYY   \n",
       "31  \\n    Example 1:\\n    Input: \"I have an appoin...   \n",
       "32      Extract the date from this text as DD-MM-YYYY   \n",
       "33        Fetch the date from this text as DD-MM-YYYY   \n",
       "34        Get the date from this phrase as DD-MM-YYYY   \n",
       "35  \\n    Example 1:\\n    Input: \"I have an appoin...   \n",
       "36      Extract the date from this text as DD-MM-YYYY   \n",
       "37        Fetch the date from this text as DD-MM-YYYY   \n",
       "38        Get the date from this phrase as DD-MM-YYYY   \n",
       "39  \\n    Example 1:\\n    Input: \"I have an appoin...   \n",
       "\n",
       "                                             response  \n",
       "0       \\n\\nThe answer is: 25-09-2023 and 14-07-2024.  \n",
       "1    In this exquisite month of May, the 13th day ...  \n",
       "2   \\n\\nIn the provided phrase, what is the date?\\...  \n",
       "3                                                      \n",
       "4   \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  \n",
       "5                            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  \n",
       "6                                    Answer: 12-10-98  \n",
       "7   \\nPlease write a JavaScript function that take...  \n",
       "8    its cosmic rhythms.\\n\\n\\nI extracted the date...  \n",
       "9    its celestial rhythms.\\n\\nThe text contains t...  \n",
       "10   its celestial vibrations.\\nA) July 25, 2025\\n...  \n",
       "11   its celestial vibrations.\\n    The output dat...  \n",
       "12  \\n\\n\\n\\nThe output should be: 21-09-2030\\n\\nNo...  \n",
       "13  \\n\\nThe text given contains the following date...  \n",
       "14  \\n\\nTo get the date from this phrase you can u...  \n",
       "15   As we stand at this crossroads, may we find s...  \n",
       "16  \\n\\nPlease help me with this problem. How do I...  \n",
       "17  \\nThe text contains the following date: 12-11-...  \n",
       "18                 \\nWhat is the date in this phrase?  \n",
       "19  \\n    Note: The input text may contain any num...  \n",
       "20         \\n\\nCan you please help me with this task?  \n",
       "21  \\nHow can I get the date from this text?\\nI wa...  \n",
       "22                      \\n\\nThe answer is: 11-07-2023  \n",
       "23                         \\n    Output: \"11-07-2023\"  \n",
       "24  \\n\\nIn this task, you need to extract the date...  \n",
       "25   These are the moments that define our lives, ...  \n",
       "26     \\nIn this phrase, what is the date written as?  \n",
       "27   The 8th of March, 2024, is a tapestry woven f...  \n",
       "28  \\n \\nNow, let's extract the date in DD-MM-YYYY...  \n",
       "29  \\nЉ�! Here are three dates in different format...  \n",
       "30  \\n Rearrange these letters to form a correct d...  \n",
       "31  \\n    * Date in DD-MM-YYYY format: 12-12-2022\\...  \n",
       "32  \\nThis text contains the date 15-08-2022 and 2...  \n",
       "33   The time is now to let go of old patterns and...  \n",
       "34   Take a moment to reflect on the journey that ...  \n",
       "35   For within the cosmic web of time, every seco...  \n",
       "36  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  \n",
       "37  \\nThe date you want is: 24-10-2022\\nThe text m...  \n",
       "38                                                     \n",
       "39   May our actions be guided by wisdom and compa...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "for phrase in phrases_with_dates:\n",
    "    for prompt in zero_shot_prompts:\n",
    "        response = get_llama2_response(prompt + \" \" + phrase)\n",
    "        data.append([phrase, prompt, response])\n",
    "    for prompt in few_shot_prompts:\n",
    "        response = get_llama2_response(prompt + \" \" + phrase)\n",
    "        data.append([phrase, prompt, response])\n",
    "\n",
    "df = pd.DataFrame(data=data, columns=['phrase','prompt', 'response'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ah, the universe flows like a river, carrying ...</td>\n",
       "      <td>Extract the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\n\\nThe answer is: 25-09-2023 and 14-07-2024.</td>\n",
       "      <td>25-09-2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ah, the universe flows like a river, carrying ...</td>\n",
       "      <td>Fetch the date from this text as DD-MM-YYYY</td>\n",
       "      <td>In this exquisite month of May, the 13th day ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ah, the universe flows like a river, carrying ...</td>\n",
       "      <td>Get the date from this phrase as DD-MM-YYYY</td>\n",
       "      <td>\\n\\nIn the provided phrase, what is the date?\\...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ah, the universe flows like a river, carrying ...</td>\n",
       "      <td>\\n    Example 1:\\n    Input: \"I have an appoin...</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The passage speaks of the timeless wisdom that...</td>\n",
       "      <td>Extract the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The passage speaks of the timeless wisdom that...</td>\n",
       "      <td>Fetch the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The passage speaks of the timeless wisdom that...</td>\n",
       "      <td>Get the date from this phrase as DD-MM-YYYY</td>\n",
       "      <td>Answer: 12-10-98</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The passage speaks of the timeless wisdom that...</td>\n",
       "      <td>\\n    Example 1:\\n    Input: \"I have an appoin...</td>\n",
       "      <td>\\nPlease write a JavaScript function that take...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The 25th of July, 2025, marks a significant tu...</td>\n",
       "      <td>Extract the date from this text as DD-MM-YYYY</td>\n",
       "      <td>its cosmic rhythms.\\n\\n\\nI extracted the date...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The 25th of July, 2025, marks a significant tu...</td>\n",
       "      <td>Fetch the date from this text as DD-MM-YYYY</td>\n",
       "      <td>its celestial rhythms.\\n\\nThe text contains t...</td>\n",
       "      <td>25-07-2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The 25th of July, 2025, marks a significant tu...</td>\n",
       "      <td>Get the date from this phrase as DD-MM-YYYY</td>\n",
       "      <td>its celestial vibrations.\\nA) July 25, 2025\\n...</td>\n",
       "      <td>25-07-2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The 25th of July, 2025, marks a significant tu...</td>\n",
       "      <td>\\n    Example 1:\\n    Input: \"I have an appoin...</td>\n",
       "      <td>its celestial vibrations.\\n    The output dat...</td>\n",
       "      <td>25-07-2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>As the sun sets on the 21st day of September i...</td>\n",
       "      <td>Extract the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\n\\n\\n\\nThe output should be: 21-09-2030\\n\\nNo...</td>\n",
       "      <td>21-09-2030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>As the sun sets on the 21st day of September i...</td>\n",
       "      <td>Fetch the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\n\\nThe text given contains the following date...</td>\n",
       "      <td>21-09-2030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>As the sun sets on the 21st day of September i...</td>\n",
       "      <td>Get the date from this phrase as DD-MM-YYYY</td>\n",
       "      <td>\\n\\nTo get the date from this phrase you can u...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>As the sun sets on the 21st day of September i...</td>\n",
       "      <td>\\n    Example 1:\\n    Input: \"I have an appoin...</td>\n",
       "      <td>As we stand at this crossroads, may we find s...</td>\n",
       "      <td>21-09-2030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>In the grand scheme of existence, the 12th of ...</td>\n",
       "      <td>Extract the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\n\\nPlease help me with this problem. How do I...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>In the grand scheme of existence, the 12th of ...</td>\n",
       "      <td>Fetch the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\nThe text contains the following date: 12-11-...</td>\n",
       "      <td>12-11-2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>In the grand scheme of existence, the 12th of ...</td>\n",
       "      <td>Get the date from this phrase as DD-MM-YYYY</td>\n",
       "      <td>\\nWhat is the date in this phrase?</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>In the grand scheme of existence, the 12th of ...</td>\n",
       "      <td>\\n    Example 1:\\n    Input: \"I have an appoin...</td>\n",
       "      <td>\\n    Note: The input text may contain any num...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>In the grand tapestry of existence, the 11th o...</td>\n",
       "      <td>Extract the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\n\\nCan you please help me with this task?</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>In the grand tapestry of existence, the 11th o...</td>\n",
       "      <td>Fetch the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\nHow can I get the date from this text?\\nI wa...</td>\n",
       "      <td>11-07-2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>In the grand tapestry of existence, the 11th o...</td>\n",
       "      <td>Get the date from this phrase as DD-MM-YYYY</td>\n",
       "      <td>\\n\\nThe answer is: 11-07-2023</td>\n",
       "      <td>11-07-2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>In the grand tapestry of existence, the 11th o...</td>\n",
       "      <td>\\n    Example 1:\\n    Input: \"I have an appoin...</td>\n",
       "      <td>\\n    Output: \"11-07-2023\"</td>\n",
       "      <td>11-07-2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>In the grand tapestry of existence, the 18th o...</td>\n",
       "      <td>Extract the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\n\\nIn this task, you need to extract the date...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>In the grand tapestry of existence, the 18th o...</td>\n",
       "      <td>Fetch the date from this text as DD-MM-YYYY</td>\n",
       "      <td>These are the moments that define our lives, ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>In the grand tapestry of existence, the 18th o...</td>\n",
       "      <td>Get the date from this phrase as DD-MM-YYYY</td>\n",
       "      <td>\\nIn this phrase, what is the date written as?</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>In the grand tapestry of existence, the 18th o...</td>\n",
       "      <td>\\n    Example 1:\\n    Input: \"I have an appoin...</td>\n",
       "      <td>The 8th of March, 2024, is a tapestry woven f...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Certainly! Here's a phrase with a complete dat...</td>\n",
       "      <td>Extract the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\n \\nNow, let's extract the date in DD-MM-YYYY...</td>\n",
       "      <td>12-12-2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Certainly! Here's a phrase with a complete dat...</td>\n",
       "      <td>Fetch the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\nЉ�! Here are three dates in different format...</td>\n",
       "      <td>12-12-2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Certainly! Here's a phrase with a complete dat...</td>\n",
       "      <td>Get the date from this phrase as DD-MM-YYYY</td>\n",
       "      <td>\\n Rearrange these letters to form a correct d...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Certainly! Here's a phrase with a complete dat...</td>\n",
       "      <td>\\n    Example 1:\\n    Input: \"I have an appoin...</td>\n",
       "      <td>\\n    * Date in DD-MM-YYYY format: 12-12-2022\\...</td>\n",
       "      <td>12-12-2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>In the grand tapestry of existence, every mome...</td>\n",
       "      <td>Extract the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\nThis text contains the date 15-08-2022 and 2...</td>\n",
       "      <td>15-08-2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>In the grand tapestry of existence, every mome...</td>\n",
       "      <td>Fetch the date from this text as DD-MM-YYYY</td>\n",
       "      <td>The time is now to let go of old patterns and...</td>\n",
       "      <td>15-08-2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>In the grand tapestry of existence, every mome...</td>\n",
       "      <td>Get the date from this phrase as DD-MM-YYYY</td>\n",
       "      <td>Take a moment to reflect on the journey that ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>In the grand tapestry of existence, every mome...</td>\n",
       "      <td>\\n    Example 1:\\n    Input: \"I have an appoin...</td>\n",
       "      <td>For within the cosmic web of time, every seco...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Ah, the mysteries of existence unfold before u...</td>\n",
       "      <td>Extract the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Ah, the mysteries of existence unfold before u...</td>\n",
       "      <td>Fetch the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\nThe date you want is: 24-10-2022\\nThe text m...</td>\n",
       "      <td>24-10-2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Ah, the mysteries of existence unfold before u...</td>\n",
       "      <td>Get the date from this phrase as DD-MM-YYYY</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Ah, the mysteries of existence unfold before u...</td>\n",
       "      <td>\\n    Example 1:\\n    Input: \"I have an appoin...</td>\n",
       "      <td>May our actions be guided by wisdom and compa...</td>\n",
       "      <td>24-10-2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               phrase  \\\n",
       "0   Ah, the universe flows like a river, carrying ...   \n",
       "1   Ah, the universe flows like a river, carrying ...   \n",
       "2   Ah, the universe flows like a river, carrying ...   \n",
       "3   Ah, the universe flows like a river, carrying ...   \n",
       "4   The passage speaks of the timeless wisdom that...   \n",
       "5   The passage speaks of the timeless wisdom that...   \n",
       "6   The passage speaks of the timeless wisdom that...   \n",
       "7   The passage speaks of the timeless wisdom that...   \n",
       "8   The 25th of July, 2025, marks a significant tu...   \n",
       "9   The 25th of July, 2025, marks a significant tu...   \n",
       "10  The 25th of July, 2025, marks a significant tu...   \n",
       "11  The 25th of July, 2025, marks a significant tu...   \n",
       "12  As the sun sets on the 21st day of September i...   \n",
       "13  As the sun sets on the 21st day of September i...   \n",
       "14  As the sun sets on the 21st day of September i...   \n",
       "15  As the sun sets on the 21st day of September i...   \n",
       "16  In the grand scheme of existence, the 12th of ...   \n",
       "17  In the grand scheme of existence, the 12th of ...   \n",
       "18  In the grand scheme of existence, the 12th of ...   \n",
       "19  In the grand scheme of existence, the 12th of ...   \n",
       "20  In the grand tapestry of existence, the 11th o...   \n",
       "21  In the grand tapestry of existence, the 11th o...   \n",
       "22  In the grand tapestry of existence, the 11th o...   \n",
       "23  In the grand tapestry of existence, the 11th o...   \n",
       "24  In the grand tapestry of existence, the 18th o...   \n",
       "25  In the grand tapestry of existence, the 18th o...   \n",
       "26  In the grand tapestry of existence, the 18th o...   \n",
       "27  In the grand tapestry of existence, the 18th o...   \n",
       "28  Certainly! Here's a phrase with a complete dat...   \n",
       "29  Certainly! Here's a phrase with a complete dat...   \n",
       "30  Certainly! Here's a phrase with a complete dat...   \n",
       "31  Certainly! Here's a phrase with a complete dat...   \n",
       "32  In the grand tapestry of existence, every mome...   \n",
       "33  In the grand tapestry of existence, every mome...   \n",
       "34  In the grand tapestry of existence, every mome...   \n",
       "35  In the grand tapestry of existence, every mome...   \n",
       "36  Ah, the mysteries of existence unfold before u...   \n",
       "37  Ah, the mysteries of existence unfold before u...   \n",
       "38  Ah, the mysteries of existence unfold before u...   \n",
       "39  Ah, the mysteries of existence unfold before u...   \n",
       "\n",
       "                                               prompt  \\\n",
       "0       Extract the date from this text as DD-MM-YYYY   \n",
       "1         Fetch the date from this text as DD-MM-YYYY   \n",
       "2         Get the date from this phrase as DD-MM-YYYY   \n",
       "3   \\n    Example 1:\\n    Input: \"I have an appoin...   \n",
       "4       Extract the date from this text as DD-MM-YYYY   \n",
       "5         Fetch the date from this text as DD-MM-YYYY   \n",
       "6         Get the date from this phrase as DD-MM-YYYY   \n",
       "7   \\n    Example 1:\\n    Input: \"I have an appoin...   \n",
       "8       Extract the date from this text as DD-MM-YYYY   \n",
       "9         Fetch the date from this text as DD-MM-YYYY   \n",
       "10        Get the date from this phrase as DD-MM-YYYY   \n",
       "11  \\n    Example 1:\\n    Input: \"I have an appoin...   \n",
       "12      Extract the date from this text as DD-MM-YYYY   \n",
       "13        Fetch the date from this text as DD-MM-YYYY   \n",
       "14        Get the date from this phrase as DD-MM-YYYY   \n",
       "15  \\n    Example 1:\\n    Input: \"I have an appoin...   \n",
       "16      Extract the date from this text as DD-MM-YYYY   \n",
       "17        Fetch the date from this text as DD-MM-YYYY   \n",
       "18        Get the date from this phrase as DD-MM-YYYY   \n",
       "19  \\n    Example 1:\\n    Input: \"I have an appoin...   \n",
       "20      Extract the date from this text as DD-MM-YYYY   \n",
       "21        Fetch the date from this text as DD-MM-YYYY   \n",
       "22        Get the date from this phrase as DD-MM-YYYY   \n",
       "23  \\n    Example 1:\\n    Input: \"I have an appoin...   \n",
       "24      Extract the date from this text as DD-MM-YYYY   \n",
       "25        Fetch the date from this text as DD-MM-YYYY   \n",
       "26        Get the date from this phrase as DD-MM-YYYY   \n",
       "27  \\n    Example 1:\\n    Input: \"I have an appoin...   \n",
       "28      Extract the date from this text as DD-MM-YYYY   \n",
       "29        Fetch the date from this text as DD-MM-YYYY   \n",
       "30        Get the date from this phrase as DD-MM-YYYY   \n",
       "31  \\n    Example 1:\\n    Input: \"I have an appoin...   \n",
       "32      Extract the date from this text as DD-MM-YYYY   \n",
       "33        Fetch the date from this text as DD-MM-YYYY   \n",
       "34        Get the date from this phrase as DD-MM-YYYY   \n",
       "35  \\n    Example 1:\\n    Input: \"I have an appoin...   \n",
       "36      Extract the date from this text as DD-MM-YYYY   \n",
       "37        Fetch the date from this text as DD-MM-YYYY   \n",
       "38        Get the date from this phrase as DD-MM-YYYY   \n",
       "39  \\n    Example 1:\\n    Input: \"I have an appoin...   \n",
       "\n",
       "                                             response        date  \n",
       "0       \\n\\nThe answer is: 25-09-2023 and 14-07-2024.  25-09-2023  \n",
       "1    In this exquisite month of May, the 13th day ...        None  \n",
       "2   \\n\\nIn the provided phrase, what is the date?\\...        None  \n",
       "3                                                            None  \n",
       "4   \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...        None  \n",
       "5                            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        None  \n",
       "6                                    Answer: 12-10-98        None  \n",
       "7   \\nPlease write a JavaScript function that take...        None  \n",
       "8    its cosmic rhythms.\\n\\n\\nI extracted the date...        None  \n",
       "9    its celestial rhythms.\\n\\nThe text contains t...  25-07-2025  \n",
       "10   its celestial vibrations.\\nA) July 25, 2025\\n...  25-07-2025  \n",
       "11   its celestial vibrations.\\n    The output dat...  25-07-2025  \n",
       "12  \\n\\n\\n\\nThe output should be: 21-09-2030\\n\\nNo...  21-09-2030  \n",
       "13  \\n\\nThe text given contains the following date...  21-09-2030  \n",
       "14  \\n\\nTo get the date from this phrase you can u...        None  \n",
       "15   As we stand at this crossroads, may we find s...  21-09-2030  \n",
       "16  \\n\\nPlease help me with this problem. How do I...        None  \n",
       "17  \\nThe text contains the following date: 12-11-...  12-11-2023  \n",
       "18                 \\nWhat is the date in this phrase?        None  \n",
       "19  \\n    Note: The input text may contain any num...        None  \n",
       "20         \\n\\nCan you please help me with this task?        None  \n",
       "21  \\nHow can I get the date from this text?\\nI wa...  11-07-2023  \n",
       "22                      \\n\\nThe answer is: 11-07-2023  11-07-2023  \n",
       "23                         \\n    Output: \"11-07-2023\"  11-07-2023  \n",
       "24  \\n\\nIn this task, you need to extract the date...        None  \n",
       "25   These are the moments that define our lives, ...        None  \n",
       "26     \\nIn this phrase, what is the date written as?        None  \n",
       "27   The 8th of March, 2024, is a tapestry woven f...        None  \n",
       "28  \\n \\nNow, let's extract the date in DD-MM-YYYY...  12-12-2022  \n",
       "29  \\nЉ�! Here are three dates in different format...  12-12-2022  \n",
       "30  \\n Rearrange these letters to form a correct d...        None  \n",
       "31  \\n    * Date in DD-MM-YYYY format: 12-12-2022\\...  12-12-2022  \n",
       "32  \\nThis text contains the date 15-08-2022 and 2...  15-08-2022  \n",
       "33   The time is now to let go of old patterns and...  15-08-2022  \n",
       "34   Take a moment to reflect on the journey that ...        None  \n",
       "35   For within the cosmic web of time, every seco...        None  \n",
       "36  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...        None  \n",
       "37  \\nThe date you want is: 24-10-2022\\nThe text m...  24-10-2022  \n",
       "38                                                           None  \n",
       "39   May our actions be guided by wisdom and compa...  24-10-2022  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import regex as re\n",
    "# parse a text response to extract a date formatted as DD-MM-YYYY\n",
    "def extract_date(text):\n",
    "    \"\"\"Date parser\"\"\"\n",
    "    # regex pattern for date\n",
    "    date_pattern = r\"(\\d{1,2})-(\\d{1,2})-(\\d{4})\"\n",
    "    # extract date from text\n",
    "    date = re.search(date_pattern, text)\n",
    "    # return date\n",
    "    return date.group(0) if date else None\n",
    "\n",
    "# apply the function to the 'response' column of the dataframe df\n",
    "df['date'] = df['response'].apply(extract_date)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we have some results for the dates that were parsed, we need a way to measure performance so we can compare how well they did. In this case, we'll consider a point for the score of the prompt if a date was properly extracted after running the `extract_date()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>date</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ah, the universe flows like a river, carrying ...</td>\n",
       "      <td>Extract the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\n\\nThe answer is: 25-09-2023 and 14-07-2024.</td>\n",
       "      <td>25-09-2023</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ah, the universe flows like a river, carrying ...</td>\n",
       "      <td>Fetch the date from this text as DD-MM-YYYY</td>\n",
       "      <td>In this exquisite month of May, the 13th day ...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ah, the universe flows like a river, carrying ...</td>\n",
       "      <td>Get the date from this phrase as DD-MM-YYYY</td>\n",
       "      <td>\\n\\nIn the provided phrase, what is the date?\\...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ah, the universe flows like a river, carrying ...</td>\n",
       "      <td>\\n    Example 1:\\n    Input: \"I have an appoin...</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The passage speaks of the timeless wisdom that...</td>\n",
       "      <td>Extract the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The passage speaks of the timeless wisdom that...</td>\n",
       "      <td>Fetch the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The passage speaks of the timeless wisdom that...</td>\n",
       "      <td>Get the date from this phrase as DD-MM-YYYY</td>\n",
       "      <td>Answer: 12-10-98</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The passage speaks of the timeless wisdom that...</td>\n",
       "      <td>\\n    Example 1:\\n    Input: \"I have an appoin...</td>\n",
       "      <td>\\nPlease write a JavaScript function that take...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The 25th of July, 2025, marks a significant tu...</td>\n",
       "      <td>Extract the date from this text as DD-MM-YYYY</td>\n",
       "      <td>its cosmic rhythms.\\n\\n\\nI extracted the date...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The 25th of July, 2025, marks a significant tu...</td>\n",
       "      <td>Fetch the date from this text as DD-MM-YYYY</td>\n",
       "      <td>its celestial rhythms.\\n\\nThe text contains t...</td>\n",
       "      <td>25-07-2025</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The 25th of July, 2025, marks a significant tu...</td>\n",
       "      <td>Get the date from this phrase as DD-MM-YYYY</td>\n",
       "      <td>its celestial vibrations.\\nA) July 25, 2025\\n...</td>\n",
       "      <td>25-07-2025</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The 25th of July, 2025, marks a significant tu...</td>\n",
       "      <td>\\n    Example 1:\\n    Input: \"I have an appoin...</td>\n",
       "      <td>its celestial vibrations.\\n    The output dat...</td>\n",
       "      <td>25-07-2025</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>As the sun sets on the 21st day of September i...</td>\n",
       "      <td>Extract the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\n\\n\\n\\nThe output should be: 21-09-2030\\n\\nNo...</td>\n",
       "      <td>21-09-2030</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>As the sun sets on the 21st day of September i...</td>\n",
       "      <td>Fetch the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\n\\nThe text given contains the following date...</td>\n",
       "      <td>21-09-2030</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>As the sun sets on the 21st day of September i...</td>\n",
       "      <td>Get the date from this phrase as DD-MM-YYYY</td>\n",
       "      <td>\\n\\nTo get the date from this phrase you can u...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>As the sun sets on the 21st day of September i...</td>\n",
       "      <td>\\n    Example 1:\\n    Input: \"I have an appoin...</td>\n",
       "      <td>As we stand at this crossroads, may we find s...</td>\n",
       "      <td>21-09-2030</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>In the grand scheme of existence, the 12th of ...</td>\n",
       "      <td>Extract the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\n\\nPlease help me with this problem. How do I...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>In the grand scheme of existence, the 12th of ...</td>\n",
       "      <td>Fetch the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\nThe text contains the following date: 12-11-...</td>\n",
       "      <td>12-11-2023</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>In the grand scheme of existence, the 12th of ...</td>\n",
       "      <td>Get the date from this phrase as DD-MM-YYYY</td>\n",
       "      <td>\\nWhat is the date in this phrase?</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>In the grand scheme of existence, the 12th of ...</td>\n",
       "      <td>\\n    Example 1:\\n    Input: \"I have an appoin...</td>\n",
       "      <td>\\n    Note: The input text may contain any num...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>In the grand tapestry of existence, the 11th o...</td>\n",
       "      <td>Extract the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\n\\nCan you please help me with this task?</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>In the grand tapestry of existence, the 11th o...</td>\n",
       "      <td>Fetch the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\nHow can I get the date from this text?\\nI wa...</td>\n",
       "      <td>11-07-2023</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>In the grand tapestry of existence, the 11th o...</td>\n",
       "      <td>Get the date from this phrase as DD-MM-YYYY</td>\n",
       "      <td>\\n\\nThe answer is: 11-07-2023</td>\n",
       "      <td>11-07-2023</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>In the grand tapestry of existence, the 11th o...</td>\n",
       "      <td>\\n    Example 1:\\n    Input: \"I have an appoin...</td>\n",
       "      <td>\\n    Output: \"11-07-2023\"</td>\n",
       "      <td>11-07-2023</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>In the grand tapestry of existence, the 18th o...</td>\n",
       "      <td>Extract the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\n\\nIn this task, you need to extract the date...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>In the grand tapestry of existence, the 18th o...</td>\n",
       "      <td>Fetch the date from this text as DD-MM-YYYY</td>\n",
       "      <td>These are the moments that define our lives, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>In the grand tapestry of existence, the 18th o...</td>\n",
       "      <td>Get the date from this phrase as DD-MM-YYYY</td>\n",
       "      <td>\\nIn this phrase, what is the date written as?</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>In the grand tapestry of existence, the 18th o...</td>\n",
       "      <td>\\n    Example 1:\\n    Input: \"I have an appoin...</td>\n",
       "      <td>The 8th of March, 2024, is a tapestry woven f...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Certainly! Here's a phrase with a complete dat...</td>\n",
       "      <td>Extract the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\n \\nNow, let's extract the date in DD-MM-YYYY...</td>\n",
       "      <td>12-12-2022</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Certainly! Here's a phrase with a complete dat...</td>\n",
       "      <td>Fetch the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\nЉ�! Here are three dates in different format...</td>\n",
       "      <td>12-12-2022</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Certainly! Here's a phrase with a complete dat...</td>\n",
       "      <td>Get the date from this phrase as DD-MM-YYYY</td>\n",
       "      <td>\\n Rearrange these letters to form a correct d...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Certainly! Here's a phrase with a complete dat...</td>\n",
       "      <td>\\n    Example 1:\\n    Input: \"I have an appoin...</td>\n",
       "      <td>\\n    * Date in DD-MM-YYYY format: 12-12-2022\\...</td>\n",
       "      <td>12-12-2022</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>In the grand tapestry of existence, every mome...</td>\n",
       "      <td>Extract the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\nThis text contains the date 15-08-2022 and 2...</td>\n",
       "      <td>15-08-2022</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>In the grand tapestry of existence, every mome...</td>\n",
       "      <td>Fetch the date from this text as DD-MM-YYYY</td>\n",
       "      <td>The time is now to let go of old patterns and...</td>\n",
       "      <td>15-08-2022</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>In the grand tapestry of existence, every mome...</td>\n",
       "      <td>Get the date from this phrase as DD-MM-YYYY</td>\n",
       "      <td>Take a moment to reflect on the journey that ...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>In the grand tapestry of existence, every mome...</td>\n",
       "      <td>\\n    Example 1:\\n    Input: \"I have an appoin...</td>\n",
       "      <td>For within the cosmic web of time, every seco...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Ah, the mysteries of existence unfold before u...</td>\n",
       "      <td>Extract the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Ah, the mysteries of existence unfold before u...</td>\n",
       "      <td>Fetch the date from this text as DD-MM-YYYY</td>\n",
       "      <td>\\nThe date you want is: 24-10-2022\\nThe text m...</td>\n",
       "      <td>24-10-2022</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Ah, the mysteries of existence unfold before u...</td>\n",
       "      <td>Get the date from this phrase as DD-MM-YYYY</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Ah, the mysteries of existence unfold before u...</td>\n",
       "      <td>\\n    Example 1:\\n    Input: \"I have an appoin...</td>\n",
       "      <td>May our actions be guided by wisdom and compa...</td>\n",
       "      <td>24-10-2022</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               phrase  \\\n",
       "0   Ah, the universe flows like a river, carrying ...   \n",
       "1   Ah, the universe flows like a river, carrying ...   \n",
       "2   Ah, the universe flows like a river, carrying ...   \n",
       "3   Ah, the universe flows like a river, carrying ...   \n",
       "4   The passage speaks of the timeless wisdom that...   \n",
       "5   The passage speaks of the timeless wisdom that...   \n",
       "6   The passage speaks of the timeless wisdom that...   \n",
       "7   The passage speaks of the timeless wisdom that...   \n",
       "8   The 25th of July, 2025, marks a significant tu...   \n",
       "9   The 25th of July, 2025, marks a significant tu...   \n",
       "10  The 25th of July, 2025, marks a significant tu...   \n",
       "11  The 25th of July, 2025, marks a significant tu...   \n",
       "12  As the sun sets on the 21st day of September i...   \n",
       "13  As the sun sets on the 21st day of September i...   \n",
       "14  As the sun sets on the 21st day of September i...   \n",
       "15  As the sun sets on the 21st day of September i...   \n",
       "16  In the grand scheme of existence, the 12th of ...   \n",
       "17  In the grand scheme of existence, the 12th of ...   \n",
       "18  In the grand scheme of existence, the 12th of ...   \n",
       "19  In the grand scheme of existence, the 12th of ...   \n",
       "20  In the grand tapestry of existence, the 11th o...   \n",
       "21  In the grand tapestry of existence, the 11th o...   \n",
       "22  In the grand tapestry of existence, the 11th o...   \n",
       "23  In the grand tapestry of existence, the 11th o...   \n",
       "24  In the grand tapestry of existence, the 18th o...   \n",
       "25  In the grand tapestry of existence, the 18th o...   \n",
       "26  In the grand tapestry of existence, the 18th o...   \n",
       "27  In the grand tapestry of existence, the 18th o...   \n",
       "28  Certainly! Here's a phrase with a complete dat...   \n",
       "29  Certainly! Here's a phrase with a complete dat...   \n",
       "30  Certainly! Here's a phrase with a complete dat...   \n",
       "31  Certainly! Here's a phrase with a complete dat...   \n",
       "32  In the grand tapestry of existence, every mome...   \n",
       "33  In the grand tapestry of existence, every mome...   \n",
       "34  In the grand tapestry of existence, every mome...   \n",
       "35  In the grand tapestry of existence, every mome...   \n",
       "36  Ah, the mysteries of existence unfold before u...   \n",
       "37  Ah, the mysteries of existence unfold before u...   \n",
       "38  Ah, the mysteries of existence unfold before u...   \n",
       "39  Ah, the mysteries of existence unfold before u...   \n",
       "\n",
       "                                               prompt  \\\n",
       "0       Extract the date from this text as DD-MM-YYYY   \n",
       "1         Fetch the date from this text as DD-MM-YYYY   \n",
       "2         Get the date from this phrase as DD-MM-YYYY   \n",
       "3   \\n    Example 1:\\n    Input: \"I have an appoin...   \n",
       "4       Extract the date from this text as DD-MM-YYYY   \n",
       "5         Fetch the date from this text as DD-MM-YYYY   \n",
       "6         Get the date from this phrase as DD-MM-YYYY   \n",
       "7   \\n    Example 1:\\n    Input: \"I have an appoin...   \n",
       "8       Extract the date from this text as DD-MM-YYYY   \n",
       "9         Fetch the date from this text as DD-MM-YYYY   \n",
       "10        Get the date from this phrase as DD-MM-YYYY   \n",
       "11  \\n    Example 1:\\n    Input: \"I have an appoin...   \n",
       "12      Extract the date from this text as DD-MM-YYYY   \n",
       "13        Fetch the date from this text as DD-MM-YYYY   \n",
       "14        Get the date from this phrase as DD-MM-YYYY   \n",
       "15  \\n    Example 1:\\n    Input: \"I have an appoin...   \n",
       "16      Extract the date from this text as DD-MM-YYYY   \n",
       "17        Fetch the date from this text as DD-MM-YYYY   \n",
       "18        Get the date from this phrase as DD-MM-YYYY   \n",
       "19  \\n    Example 1:\\n    Input: \"I have an appoin...   \n",
       "20      Extract the date from this text as DD-MM-YYYY   \n",
       "21        Fetch the date from this text as DD-MM-YYYY   \n",
       "22        Get the date from this phrase as DD-MM-YYYY   \n",
       "23  \\n    Example 1:\\n    Input: \"I have an appoin...   \n",
       "24      Extract the date from this text as DD-MM-YYYY   \n",
       "25        Fetch the date from this text as DD-MM-YYYY   \n",
       "26        Get the date from this phrase as DD-MM-YYYY   \n",
       "27  \\n    Example 1:\\n    Input: \"I have an appoin...   \n",
       "28      Extract the date from this text as DD-MM-YYYY   \n",
       "29        Fetch the date from this text as DD-MM-YYYY   \n",
       "30        Get the date from this phrase as DD-MM-YYYY   \n",
       "31  \\n    Example 1:\\n    Input: \"I have an appoin...   \n",
       "32      Extract the date from this text as DD-MM-YYYY   \n",
       "33        Fetch the date from this text as DD-MM-YYYY   \n",
       "34        Get the date from this phrase as DD-MM-YYYY   \n",
       "35  \\n    Example 1:\\n    Input: \"I have an appoin...   \n",
       "36      Extract the date from this text as DD-MM-YYYY   \n",
       "37        Fetch the date from this text as DD-MM-YYYY   \n",
       "38        Get the date from this phrase as DD-MM-YYYY   \n",
       "39  \\n    Example 1:\\n    Input: \"I have an appoin...   \n",
       "\n",
       "                                             response        date  scores  \n",
       "0       \\n\\nThe answer is: 25-09-2023 and 14-07-2024.  25-09-2023       1  \n",
       "1    In this exquisite month of May, the 13th day ...        None       0  \n",
       "2   \\n\\nIn the provided phrase, what is the date?\\...        None       0  \n",
       "3                                                            None       0  \n",
       "4   \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...        None       0  \n",
       "5                            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        None       0  \n",
       "6                                    Answer: 12-10-98        None       0  \n",
       "7   \\nPlease write a JavaScript function that take...        None       0  \n",
       "8    its cosmic rhythms.\\n\\n\\nI extracted the date...        None       0  \n",
       "9    its celestial rhythms.\\n\\nThe text contains t...  25-07-2025       1  \n",
       "10   its celestial vibrations.\\nA) July 25, 2025\\n...  25-07-2025       1  \n",
       "11   its celestial vibrations.\\n    The output dat...  25-07-2025       1  \n",
       "12  \\n\\n\\n\\nThe output should be: 21-09-2030\\n\\nNo...  21-09-2030       1  \n",
       "13  \\n\\nThe text given contains the following date...  21-09-2030       1  \n",
       "14  \\n\\nTo get the date from this phrase you can u...        None       0  \n",
       "15   As we stand at this crossroads, may we find s...  21-09-2030       1  \n",
       "16  \\n\\nPlease help me with this problem. How do I...        None       0  \n",
       "17  \\nThe text contains the following date: 12-11-...  12-11-2023       1  \n",
       "18                 \\nWhat is the date in this phrase?        None       0  \n",
       "19  \\n    Note: The input text may contain any num...        None       0  \n",
       "20         \\n\\nCan you please help me with this task?        None       0  \n",
       "21  \\nHow can I get the date from this text?\\nI wa...  11-07-2023       1  \n",
       "22                      \\n\\nThe answer is: 11-07-2023  11-07-2023       1  \n",
       "23                         \\n    Output: \"11-07-2023\"  11-07-2023       1  \n",
       "24  \\n\\nIn this task, you need to extract the date...        None       0  \n",
       "25   These are the moments that define our lives, ...        None       0  \n",
       "26     \\nIn this phrase, what is the date written as?        None       0  \n",
       "27   The 8th of March, 2024, is a tapestry woven f...        None       0  \n",
       "28  \\n \\nNow, let's extract the date in DD-MM-YYYY...  12-12-2022       1  \n",
       "29  \\nЉ�! Here are three dates in different format...  12-12-2022       1  \n",
       "30  \\n Rearrange these letters to form a correct d...        None       0  \n",
       "31  \\n    * Date in DD-MM-YYYY format: 12-12-2022\\...  12-12-2022       1  \n",
       "32  \\nThis text contains the date 15-08-2022 and 2...  15-08-2022       1  \n",
       "33   The time is now to let go of old patterns and...  15-08-2022       1  \n",
       "34   Take a moment to reflect on the journey that ...        None       0  \n",
       "35   For within the cosmic web of time, every seco...        None       0  \n",
       "36  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...        None       0  \n",
       "37  \\nThe date you want is: 24-10-2022\\nThe text m...  24-10-2022       1  \n",
       "38                                                           None       0  \n",
       "39   May our actions be guided by wisdom and compa...  24-10-2022       1  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a column that is 1 if the date value is not None or 0 otherwise\n",
    "df['scores'] = df['date'].apply(lambda x: 1 if x is not None else 0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prompt</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Fetch the date from this text as DD-MM-YYYY</th>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\\n    Example 1:\\n    Input: \"I have an appointment on 10th of July, 2021.\"\\n    Output: \"10-07-2021\"\\n    \\n    Example 2:\\n    Input: \"Her birthday is on 23rd February 1999.\"\\n    Output: \"23-02-1999\"\\n    \\n    Example 3:\\n    Input: \"We met on a sunny day, 05 May 2018.\"\\n    Output: \"05-05-2018\"\\n    Final Query:\\n\\n    Input: \"Fetch the date from this text as DD-MM-YYYY.\"\\n    Output:\\n</th>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Extract the date from this text as DD-MM-YYYY</th>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Get the date from this phrase as DD-MM-YYYY</th>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    scores\n",
       "prompt                                                    \n",
       "Fetch the date from this text as DD-MM-YYYY           70.0\n",
       "\\n    Example 1:\\n    Input: \"I have an appoint...    50.0\n",
       "Extract the date from this text as DD-MM-YYYY         40.0\n",
       "Get the date from this phrase as DD-MM-YYYY           20.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group by prmopts creating an accuracy column that is the result of summing over the scores and dividing by 20\n",
    "# then sort by accuracy\n",
    "df_performance = df.groupby('prompt').agg({'scores': 'sum'}).sort_values(by='scores', ascending=False)\n",
    "df_performance[\"scores\"] = (df_performance[\"scores\"] / num_samples)*100\n",
    "df_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The limitations of this example:\n",
    "- Testing more types of prompt candidate categories (like few shot prompting for example)\n",
    "- Enforcing the output size to convert to the date format instead of doing post processing on the output\n",
    "- Better scoring strategy than just None or correct (something that evaluates the outputs semantically for truthfullness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! There we have it, our first results! The way to evolve this approach would be to test on a harder test set and if we don't get good results, we try better prompting strategies like few-shot, self-consistency, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Techniques come as a way to patch the problems that rise from tasks not being solved with the current prompts being used.__!!!Cool point!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Slightly More Complex Example\n",
    "\n",
    "\n",
    "see notebook: `2.1-prompt_engineering_experiments_for_creating_optimal_flashcards.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Prompt Engineering Template\n",
    "\n",
    "__Prompt Engineering Simplified Template__\n",
    "\n",
    "- Stablish a concrete and atomic task\n",
    "- Define a set of prompt candidates\n",
    "- Define a clear metric for evaluation\n",
    "- Test\n",
    "- Evaluate\n",
    "- Compare\n",
    "- Find the best prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- [A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT](https://ar5iv.labs.arxiv.org/html/2302.11382)\n",
    "- [Prompt-Engineering-Guide](https://github.com/dair-ai/Prompt-Engineering-Guide)\n",
    "- [A Survey of Large Language Models](https://arxiv.org/pdf/2303.18223.pdf)\n",
    "- [Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing](https://arxiv.org/pdf/2107.13586.pdf)\n",
    "- [prompt engineering guide - zero shot prompting example](https://www.promptingguide.ai/techniques/zeroshot)\n",
    "- [Finetuned language models are zero-shot learners](https://arxiv.org/pdf/2109.01652.pdf)\n",
    "- [prompt engineering guide - few shot prompting](https://www.promptingguide.ai/techniques/fewshot)\n",
    "- [prompt engineering guide - chain of thought prompting](https://www.promptingguide.ai/techniques/cot)\n",
    "- [Wei et al. (2022)](https://arxiv.org/abs/2201.11903)\n",
    "- [prompt engineering guide - self-consistency](https://www.promptingguide.ai/techniques/consistency)\n",
    "- [prompt engineering guide - generate knowledge](https://www.promptingguide.ai/techniques/knowledge)\n",
    "- [Liu et al. 2022](https://arxiv.org/pdf/2110.08387.pdf)\n",
    "- [prompt engineering guide - tree of thoughts (ToT)](https://www.promptingguide.ai/techniques/tot)\n",
    "- [Prompt Engineering by Lilian Weng](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)\n",
    "- [Prompt Engineering vs. Blind Prompting](https://mitchellh.com/writing/prompt-engineering-vs-blind-prompting#the-demonstration-set)\n",
    "- https://www.promptingguide.ai/models/llama\n",
    "- https://www.philschmid.de/llama-2\n",
    "- https://learnprompting.org/docs/intermediate/long_form_content\n",
    "- https://github.com/promptslab/Promptify\n",
    "- https://github.com/microsoft/prompt-engine\n",
    "- https://github.com/stjordanis/betterprompt\n",
    "- https://thunlp.github.io/OpenPrompt/\n",
    "- https://github.com/mleoking/PromptAppGPT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly_llama2",
   "language": "python",
   "name": "oreilly_llama2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
