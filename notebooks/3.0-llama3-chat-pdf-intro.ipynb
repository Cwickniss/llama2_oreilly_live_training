{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local RAG with PDF and Llama 3.1\n",
    "\n",
    "## Steps\n",
    "\n",
    "1. Load the pdf\n",
    "2. Chunk that pdf (split that into pieces)\n",
    "3. Embed each piece\n",
    "4. Create the vector database, index\n",
    "5. Query (retrieving from that vector database using a llama3 model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk size?\n",
    "\n",
    "![](./assets-resources/optimal-chunk-size.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain langchain-community langchain-core langchain-ollama chromadb langchainhub pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "\n",
    "folder_path = \"./pdfs/llama3-80k.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': './pdfs/llama3-80k.pdf', 'page': 0}, page_content='Extending Llama-3’s Context Ten-Fold Overnight\\nPeitian Zhang1,2, Ninglu Shao1,2, Zheng Liu1∗, Shitao Xiao1, Hongjin Qian1,2,\\nQiwei Ye1, Zhicheng Dou2\\n1Beijing Academy of Artificial Intelligence\\n2Gaoling School of Artificial Intelligence, Renmin University of China\\nnamespace.pt@gmail.com zhengliu1026@gmail.com\\nAbstract\\nWe extend the context length of Llama-3-8B-Instruct from 8K to 80K via QLoRA\\nfine-tuning2. The entire training cycle is super efficient, which takes 8 hours on one\\n8xA800 (80G) GPU machine. The resulted model exhibits superior performances\\nacross a broad range of evaluation tasks, such as NIHS, topic retrieval, and long-\\ncontext language understanding; meanwhile, it also well preserves the original\\ncapability over short contexts. The dramatic context extension is mainly attributed\\nto merely 3.5K synthetic training samples generated by GPT-4 , which indicates\\nthe LLMs’ inherent (yet largely underestimated) potential to extend its original\\ncontext length. In fact, the context length could be extended far beyond 80K\\nwith more computation resources. Therefore, the team will publicly release the\\nentire resources (including data, model, data generation pipeline, training code) so\\nas to facilitate the future research from the community: https://github.com/\\nFlagOpen/FlagEmbedding .\\n1 Introduction\\nRecently, considerable attention has been directed towards long-context large language models,\\nwhere different approaches are adopted to establish long-context capabilities for large language\\nmodels [ 4,14,5,8,9,16,2]. However, most of them require significant compute and resources to\\naccomplish.\\nIn this technical report, we propose an efficient solution for entitling the long-context capabilities for\\nLLMs, with which we extend the context length of Llama-3-8B-Instruct3from 8K to 80K. Specifically,\\nwe use GPT-4 [13] to synthesize 3.5K long-context training data, covering three long-context tasks:\\n1.Single-Detail QA : the inquiry targets on one specific detail in a long context. To construct\\ndata for this task, we slice out a short segment (e.g., a chunk with less than 4096 tokens)\\nfrom a long context (e.g., a book or a long paper) and prompt GPT-4 to generate multiple\\nquestion-answer pairs based on this segment.\\n2.Multi-Detail QA : the inquiry requires information aggregation and reasoning over multiple\\ndetails in a long context. We define two types of long context. The homogeneous\\ncontext contains a coherent text, such as a book or a long paper. We prompt GPT-4 to\\ngenerate multiple question-answer pairs that require aggregating and analyzing information\\nfrom different locations in the context. The heterogeneous context consists of multiple\\nindependent texts. Notably, we perform clustering over a large corpus then extract texts from\\n∗Corresponding author.\\n2The model is noted as Llama-3-8B-Instruct-80K-QLoRA given its max context length during fine-tuning.\\nHowever, users could apply the model for even longer contexts via extrapolation.\\n3https://llama.meta.com/llama3/\\nPreprint. Under review.arXiv:2404.19553v1  [cs.CL]  30 Apr 2024')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_docs = PyPDFLoader(folder_path).load_and_split()\n",
    "pdf_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding and Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OllamaEmbeddings(model=\"nomic-embed-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = Chroma.from_documents(pdf_docs, embedding=embedding, persist_directory=\"rag-pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1:8b\", chat_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"A delicious and relevant question! As an AI engineer, you likely appreciate a good pancake breakfast. Here are 5 reasons why pancakes stand out as the ultimate breakfast choice:\\n\\n1. **Flexibility**: Pancakes can be topped with a wide variety of sweet and savory ingredients, making them adaptable to individual tastes. From classic buttermilk and maple syrup to fresh fruits, nuts, or even bacon - the possibilities are endless!\\n2. **Satisfying carbs**: As AI engineers (or anyone who's spent hours staring at code), you likely appreciate a carb-boost to start your day. Pancakes provide a satisfying dose of complex carbohydrates, which can help with focus and mental clarity.\\n3. **Quick energy boost**: A stack of fluffy pancakes can be whipped up in no time, making them an ideal breakfast for those with busy schedules (ahem... AI engineers). They'll give you the quick energy boost you need to tackle a morning of coding or project work.\\n4. **Mood-boosting goodness**: Let's face it - pancake breakfasts are basically a hug for your taste buds and mood. The combination of fluffy pancakes, sweet toppings, and possibly some crispy bacon can be just what you need to start the day on a positive note.\\n5. **Customizable and indulgent**: Pancakes are the ultimate blank canvas for your breakfast creativity! Want to try something new? Add fresh berries or whipped cream. Feeling fancy? Top with caramelized bananas or even ice cream (because, why not?). The possibilities are endless, making pancake breakfasts a fun and indulgent treat.\\n\\nHow's that? Do these reasons resonate with you as an AI engineer who appreciates a good pancake breakfast?\", response_metadata={'model': 'llama3.1:8b', 'created_at': '2024-08-08T11:50:38.318751Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 7581990792, 'load_duration': 1520314792, 'prompt_eval_count': 27, 'prompt_eval_duration': 75797000, 'eval_count': 348, 'eval_duration': 5981643000}, id='run-6b9017fb-888a-44e3-9d97-ddcbdb8cd525-0', usage_metadata={'input_tokens': 27, 'output_tokens': 348, 'total_tokens': 375})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Hi tell me 5 reasons why pancakes are the best breakfast for AI engineers like myself.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q&A with Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Convert loaded documents into strings by concatenating their content\n",
    "# and ignoring metadata\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "RAG_TEMPLATE = \"\"\"\n",
    "You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Answer the following question:\n",
    "\n",
    "{question}\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)\n",
    "\n",
    "retriever = vector_db.as_retriever()\n",
    "\n",
    "qa_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the text, on the LongBench evaluation task, the extended context model (Llama-3-8B-Instruct-80K-QLoRA) significantly and consistently outperforms all baselines except on the code completion task. However, its zero-shot performance is lower than that of the original Llama-3-8B-Instruct model.\\n\\nOn the InfBench evaluation task, specifically the Long-Book QA task, Llama-3-8B-Instruct-80K-QLoRA excels in answering questions based on long context. However, its zero-shot performance is lower than that of the original Llama-3-8B-Instruct model.\\n\\nThe text does not provide a direct comparison to the community model Llama-3-8B-Instruct-262K on LongBench and InfBench evaluation tasks.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"In the reported experiments, how does the performance of the extended context model (Llama-3-8B-Instruct-80K-QLoRA) compare to both the original Llama-3-8B-Instruct model and the community model Llama-3-8B-Instruct-262K on the LongBench and InfBench evaluation tasks?\"\n",
    "qa_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "queries = [\n",
    "    \"What method did the authors use to extend the context length of Llama-3-8B-Instruct from 8K to 80K?\",\n",
    "    \"What are the three long-context tasks covered in the training data synthesized by GPT-4?\",\n",
    "    \"What contributions do the authors highlight in their work on extending the context length of Llama-3-8B-Instruct?\",\n",
    "    \"How does the performance of Llama-3-8B-Instruct-80K-QLoRA compare with other long-context models on popular benchmarks?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean latency in seconds: 6.165865898132324\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "latencies = []\n",
    "for query in queries:\n",
    "    start = time.time()\n",
    "    output = qa_chain.invoke(query)\n",
    "    outputs.append(output)\n",
    "    end = time.time()\n",
    "    latencies.append(end - start)\n",
    "\n",
    "mean_latency = np.mean(latencies)\n",
    "print(f\"Mean latency in seconds: {mean_latency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'How did the Llama-3-8B-Instruct-80K-QLoRA model compare to the original Llama-3-8B-Instruct model?',\n",
       " 'context': [Document(metadata={'page': 1, 'source': './pdfs/llama3-80k.pdf'}, page_content='800014315 20631 26947 33263 39578 45894 52210 58526 64842 71157 77473 83789 90105 96421102736 109052 115368 121684 128000\\nContext Length0\\n11\\n22\\n33\\n44\\n55\\n66\\n77\\n88\\n100Depth Percent1.0Needle In A HayStack\\n12345678910\\nAccuracy Score from GPT3.5Figure 1: The accuracy score of Llama-3-8B-Instruct-80K-QLoRA on Needle-In-A-HayStack task.\\nThe blue vertical line indicates the training length, i.e. 80K.\\nthe same cluster to form each heterogeneous context. Therefore, the grouped texts share\\nsome semantic similarity. We then prompt GPT-4 to ask about the similarities/dissimilarities\\nacross these texts.\\n3.Biography Summarization : we prompt GPT-4 to write a biography for each main character\\nin a given book.\\nFor all three tasks, the length of context is between 64K to 80K. Note that longer data can also be\\nsynthesized following the same methodology. When training, we organize the question-answer pairs\\nfor the same context in one multi-turn conversation then fine-tune the LLM to correctly answer the\\nquestions given the entire long context as input. Following previous work4, we mix 5K instances\\nrandomly chosen from RedPajama [ 6] to mitigate forgetting. We also mix LongAlpaca [ 5] in the\\ntraining set, which contains 12K instruction tuning instances with 16K length at maximum. Therefore,\\nthe entire training dataset contains 20K instances.\\nWe use QLoRA [ 7] to efficiently fine-tune the model. We apply LoRA on all Q,K,V ,O projections\\nand additionally train the embedding layer. We set LoRA rank to 32 and alpha to 16. The learning\\nrate is 5e-5 with linear decay and no warmups. The batch size is 8. Gradient checkpointing is enabled.\\nNo parallel strategy is required thanks to the efficient implementation from Unsloth [ 1]. We train the\\nmodel for 1 epoch, which takes 8 hours to complete on a 8xA800 (80G) machine. Importantly, we\\nexpand the RoPE base from 500K to 200M in training.\\nOur contributions are highlighted as follows:\\n•We release Llama-3-8B-Instruct-80K-QLoRA, which extends the context length of Llama-\\n3-8B-Instruct from 8K to 80K. The entire resources including the model, training data, and\\ncode are all publicly available, which may advance the field of training long-context LLMs.\\n•Our training recipe is simple and efficient, while the resulted model demonstrates remark-\\nable performance on downstream long-context tasks. Further research can be made to\\nimprove our approach.\\n2 Experiments\\nWe evaluate our model on popular long-context benchmarks, then compare it with the original\\nLlama-3-8B-Instruct model and the long-context Llama-3-8B-Instruct-262K from the community5.\\n4https://www.together.ai/blog/llama-2-7b-32k\\n5https://huggingface.co/gradientai/Llama-3-8B-Instruct-262k\\n2'),\n",
       "  Document(metadata={'page': 0, 'source': './pdfs/llama3-80k.pdf'}, page_content='Extending Llama-3’s Context Ten-Fold Overnight\\nPeitian Zhang1,2, Ninglu Shao1,2, Zheng Liu1∗, Shitao Xiao1, Hongjin Qian1,2,\\nQiwei Ye1, Zhicheng Dou2\\n1Beijing Academy of Artificial Intelligence\\n2Gaoling School of Artificial Intelligence, Renmin University of China\\nnamespace.pt@gmail.com zhengliu1026@gmail.com\\nAbstract\\nWe extend the context length of Llama-3-8B-Instruct from 8K to 80K via QLoRA\\nfine-tuning2. The entire training cycle is super efficient, which takes 8 hours on one\\n8xA800 (80G) GPU machine. The resulted model exhibits superior performances\\nacross a broad range of evaluation tasks, such as NIHS, topic retrieval, and long-\\ncontext language understanding; meanwhile, it also well preserves the original\\ncapability over short contexts. The dramatic context extension is mainly attributed\\nto merely 3.5K synthetic training samples generated by GPT-4 , which indicates\\nthe LLMs’ inherent (yet largely underestimated) potential to extend its original\\ncontext length. In fact, the context length could be extended far beyond 80K\\nwith more computation resources. Therefore, the team will publicly release the\\nentire resources (including data, model, data generation pipeline, training code) so\\nas to facilitate the future research from the community: https://github.com/\\nFlagOpen/FlagEmbedding .\\n1 Introduction\\nRecently, considerable attention has been directed towards long-context large language models,\\nwhere different approaches are adopted to establish long-context capabilities for large language\\nmodels [ 4,14,5,8,9,16,2]. However, most of them require significant compute and resources to\\naccomplish.\\nIn this technical report, we propose an efficient solution for entitling the long-context capabilities for\\nLLMs, with which we extend the context length of Llama-3-8B-Instruct3from 8K to 80K. Specifically,\\nwe use GPT-4 [13] to synthesize 3.5K long-context training data, covering three long-context tasks:\\n1.Single-Detail QA : the inquiry targets on one specific detail in a long context. To construct\\ndata for this task, we slice out a short segment (e.g., a chunk with less than 4096 tokens)\\nfrom a long context (e.g., a book or a long paper) and prompt GPT-4 to generate multiple\\nquestion-answer pairs based on this segment.\\n2.Multi-Detail QA : the inquiry requires information aggregation and reasoning over multiple\\ndetails in a long context. We define two types of long context. The homogeneous\\ncontext contains a coherent text, such as a book or a long paper. We prompt GPT-4 to\\ngenerate multiple question-answer pairs that require aggregating and analyzing information\\nfrom different locations in the context. The heterogeneous context consists of multiple\\nindependent texts. Notably, we perform clustering over a large corpus then extract texts from\\n∗Corresponding author.\\n2The model is noted as Llama-3-8B-Instruct-80K-QLoRA given its max context length during fine-tuning.\\nHowever, users could apply the model for even longer contexts via extrapolation.\\n3https://llama.meta.com/llama3/\\nPreprint. Under review.arXiv:2404.19553v1  [cs.CL]  30 Apr 2024'),\n",
       "  Document(metadata={'page': 2, 'source': './pdfs/llama3-80k.pdf'}, page_content='3K 6K 9K 11K 14K 16K 21K 26K 31K 36K\\nContext Length0.00.20.40.60.81.0Accuracy\\nLlama-3-8B-Instruct\\nLlama-3-8B-Instruct-262k\\nLlama-3-8B-Instruct-80K-QLoRAFigure 2: The accuracy of Topic Retrieval task.\\nModel Single-Doc Multi-Doc Summ. Few-Shot Synthetic Code Avg\\nLlama-3-8B-Instruct 37.33 36.04 26.83 69.56 37.75 53.24 43.20\\nLlama-3-8B-Instruct-262K 37.29 31.20 26.18 67.25 44.25 62.71 43.73\\nLlama-3-8B-Instruct-80K-QLoRA 43.57 43.07 28.93 69.15 48.50 51.95 47.19\\nTable 1: Evaluation results on LongBench. For Llama-3-8B-Instruct, we use 8K context length.\\nModel LongBookQA Eng LongBookSum Eng\\nGPT-4 22.22 14.73\\nLlama-3-8B-Instruct 7.00 16.40\\nLlama-3-8B-Instruct-262K 20.30 10.34\\nLlama-3-8B-Instruct-80K-QLoRA 30.92 14.73\\nTable 2: Evaluation results on InfBench. For Llama-3-8B-Instruct, we use 8K context length. The\\nresults of GPT-4 is copied from the paper [17].\\nModel STEM Social Humanities Others Avg\\nLlama-2-7B-Chat 35.92 54.37 51.74 51.42 47.22\\nMistral-7B-v0.2-Instruct 48.79 69.95 64.99 61.64 60.10\\nLlama-3-8B-Instruct 53.87 75.66 69.44 69.75 65.91\\nLlama-3-8B-Instruct-262K 52.10 73.26 67.15 69.80 64.34\\nLlama-3-8B-Instruct-80K-QLoRA 53.10 73.24 67.32 68.79 64.44\\nTable 3: Zero-shot performance on MMLU.\\nFirstly, we leverage the Needle-In-A-Haystack task, which aims to recall an irrelevant piece of\\ninformation (a.k.a. needle) inserted into a lengthy context (a.k.a. haystack). The accuracy is evaluated\\nwith GPT3.5. We use the same needle and haystack as in the official repository6. Our model achieves\\n100% accuracy over all its training context length. Besides, the model generalizes well to the unseen\\npositions (80K ∼128K).\\nSecondly, we report the Topic Retrieval [ 12] accuracy in Figure 2. This task synthesizes a long\\nconversation with multiple independent discussions of a certain topic between the user and the\\nassistant. Then the LLM is required to repeat the first topic as is in the conversation. We use the\\nconversations made up of [5,10,15,20,25,30,40,50,60,70] topics for evaluation. It can be observed\\nthat Llama-3-8B-Instruct fails to remember the topic when the context is longer than 9K. However,\\nthe accuracy of our model remains 100% throughout all context lengths.\\n6https://github.com/gkamradt/LLMTest_NeedleInAHaystack\\n3'),\n",
       "  Document(metadata={'page': 4, 'source': './pdfs/llama3-80k.pdf'}, page_content='I. Molybog, Y . Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M.\\nSmith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan,\\nI. Zarov, Y . Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and\\nT. Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.\\n[16] P. Zhang, Z. Liu, S. Xiao, N. Shao, Q. Ye, and Z. Dou. Soaring from 4k to 400k: Extending\\nllm’s context with activation beacon, 2024.\\n[17] X. Zhang, Y . Chen, S. Hu, Z. Xu, J. Chen, M. K. Hao, X. Han, Z. L. Thai, S. Wang, Z. Liu, and\\nM. Sun. ∞bench: Extending long context evaluation beyond 100k tokens, 2024.\\n5')],\n",
       " 'answer': None}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from typing import List\n",
    "\n",
    "# from langchain_core.runnables import RunnablePassthrough\n",
    "# from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "# # llm = ChatOllama(model=\"llama3.1:8b\", temperature=0)\n",
    "\n",
    "# # 2. Incorporate the retriever into a question-answering chain.\n",
    "# system_prompt = (\n",
    "#     \"You are an assistant for question-answering tasks. \"\n",
    "#     \"Use the following pieces of retrieved context to answer \"\n",
    "#     \"the question. If you don't know the answer, say that you \"\n",
    "#     \"don't know. Use three sentences maximum and keep the \"\n",
    "#     \"answer concise.\"\n",
    "#     \"\\n\\n\"\n",
    "#     \"{context}\"\n",
    "# )\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\", system_prompt),\n",
    "#         (\"human\", \"{input}\"),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "\n",
    "# # Desired schema for response\n",
    "# class AnswerWithSources(TypedDict):\n",
    "#     \"\"\"An answer to the question, with sources.\"\"\"\n",
    "\n",
    "#     answer: str\n",
    "#     sources: Annotated[\n",
    "#         List[str],\n",
    "#         ...,\n",
    "#         \"List of sources (author + year) used to answer the question\",\n",
    "#     ]\n",
    "\n",
    "\n",
    "# # Our rag_chain_from_docs has the following changes:\n",
    "# # - add `.with_structured_output` to the LLM;\n",
    "# # - remove the output parser\n",
    "# rag_chain_from_docs = (\n",
    "#     {\n",
    "#         \"input\": lambda x: x[\"input\"],\n",
    "#         \"context\": lambda x: format_docs(x[\"context\"]),\n",
    "#     }\n",
    "#     | prompt\n",
    "#     | llm.with_structured_output(AnswerWithSources)\n",
    "# )\n",
    "\n",
    "# retrieve_docs = (lambda x: x[\"input\"]) | retriever\n",
    "\n",
    "# chain = RunnablePassthrough.assign(context=retrieve_docs).assign(\n",
    "#     answer=rag_chain_from_docs\n",
    "# )\n",
    "\n",
    "# response = chain.invoke({\"input\": \"How did the Llama-3-8B-Instruct-80K-QLoRA model compare to the original Llama-3-8B-Instruct model?\"})\n",
    "# response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(json.dumps(response[\"answer\"], indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-llama3",
   "language": "python",
   "name": "oreilly-llama3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
