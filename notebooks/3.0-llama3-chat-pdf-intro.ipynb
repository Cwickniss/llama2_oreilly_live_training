{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets-resources/optimal-chunk-size.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the pdf\n",
    "2. Chunk that pdf (split that into pieces)\n",
    "3. Embed each piece\n",
    "4. Create the vector database, index\n",
    "5. Query (retrieving from that vector database using a llama3 model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "\n",
    "folder_path = \"./pdfs/llama3-80k.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Extending Llama-3’s Context Ten-Fold Overnight\\nPeitian Zhang1,2, Ninglu Shao1,2, Zheng Liu1∗, Shitao Xiao1, Hongjin Qian1,2,\\nQiwei Ye1, Zhicheng Dou2\\n1Beijing Academy of Artificial Intelligence\\n2Gaoling School of Artificial Intelligence, Renmin University of China\\nnamespace.pt@gmail.com zhengliu1026@gmail.com\\nAbstract\\nWe extend the context length of Llama-3-8B-Instruct from 8K to 80K via QLoRA\\nfine-tuning2. The entire training cycle is super efficient, which takes 8 hours on one\\n8xA800 (80G) GPU machine. The resulted model exhibits superior performances\\nacross a broad range of evaluation tasks, such as NIHS, topic retrieval, and long-\\ncontext language understanding; meanwhile, it also well preserves the original\\ncapability over short contexts. The dramatic context extension is mainly attributed\\nto merely 3.5K synthetic training samples generated by GPT-4 , which indicates\\nthe LLMs’ inherent (yet largely underestimated) potential to extend its original\\ncontext length. In fact, the context length could be extended far beyond 80K\\nwith more computation resources. Therefore, the team will publicly release the\\nentire resources (including data, model, data generation pipeline, training code) so\\nas to facilitate the future research from the community: https://github.com/\\nFlagOpen/FlagEmbedding .\\n1 Introduction\\nRecently, considerable attention has been directed towards long-context large language models,\\nwhere different approaches are adopted to establish long-context capabilities for large language\\nmodels [ 4,14,5,8,9,16,2]. However, most of them require significant compute and resources to\\naccomplish.\\nIn this technical report, we propose an efficient solution for entitling the long-context capabilities for\\nLLMs, with which we extend the context length of Llama-3-8B-Instruct3from 8K to 80K. Specifically,\\nwe use GPT-4 [13] to synthesize 3.5K long-context training data, covering three long-context tasks:\\n1.Single-Detail QA : the inquiry targets on one specific detail in a long context. To construct\\ndata for this task, we slice out a short segment (e.g., a chunk with less than 4096 tokens)\\nfrom a long context (e.g., a book or a long paper) and prompt GPT-4 to generate multiple\\nquestion-answer pairs based on this segment.\\n2.Multi-Detail QA : the inquiry requires information aggregation and reasoning over multiple\\ndetails in a long context. We define two types of long context. The homogeneous\\ncontext contains a coherent text, such as a book or a long paper. We prompt GPT-4 to\\ngenerate multiple question-answer pairs that require aggregating and analyzing information\\nfrom different locations in the context. The heterogeneous context consists of multiple\\nindependent texts. Notably, we perform clustering over a large corpus then extract texts from\\n∗Corresponding author.\\n2The model is noted as Llama-3-8B-Instruct-80K-QLoRA given its max context length during fine-tuning.\\nHowever, users could apply the model for even longer contexts via extrapolation.\\n3https://llama.meta.com/llama3/\\nPreprint. Under review.arXiv:2404.19553v1  [cs.CL]  30 Apr 2024', metadata={'source': './pdfs/llama3-80k.pdf', 'page': 0})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_docs = PyPDFLoader(folder_path).load_and_split()\n",
    "pdf_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding and Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OllamaEmbeddings(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = Chroma.from_documents(pdf_docs, embedding=embedding, persist_directory=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building blocks in langchain to connect llama3 to documents\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"llama3\", chat_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"A pancake lover, I see! As an AI engineer, you likely appreciate efficiency, versatility, and a boost of energy to tackle complex problems. Here are five reasons why pancakes are the perfect breakfast for AI engineers like yourself:\\n\\n1. **Fuel for Problem-Solving**: Pancakes provide a quick and satisfying source of carbohydrates, which is essential for fueling your brain's problem-solving abilities. The complex sugars in pancakes will keep you focused and energized throughout your morning coding sessions or meetings.\\n2. **Customization Station**: AI engineers value flexibility and adaptability. Pancakes offer the perfect canvas for customization! Add your favorite toppings, from sweet fruits to savory nuts, and create a flavor profile that suits your unique taste buds. This mirrors the iterative process of developing AI models – you can experiment with different approaches until you find what works best.\\n3. **Structured Complexity**: Just as AI engineers work with complex algorithms and data structures, pancakes possess their own intricate architecture. The layering of batter, the balance of ingredients, and the art of flipping are all examples of structured complexity. This mirrors the logical thinking required to design and develop AI systems.\\n4. **Batch Processing**: As an AI engineer, you're likely familiar with batch processing – executing a series of tasks simultaneously or in sequence. Pancakes can be cooked in batches, allowing you to efficiently fuel up for your day. This efficiency is mirrored in AI's ability to process large datasets quickly and accurately.\\n5. **Error Tolerance**: Let's face it – even the best pancake recipes can go awry sometimes (e.g., burnt edges or too much syrup). As an AI engineer, you're accustomed to dealing with errors and exceptions. Pancakes offer a similar experience: even if they don't turn out perfectly, you can still enjoy them (just as you can work around bugs in your code).\\n\\nThere you have it – five compelling reasons why pancakes are the best breakfast for AI engineers like yourself!\", response_metadata={'model': 'llama3', 'created_at': '2024-06-11T19:16:02.602276Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 7056767917, 'load_duration': 1817584, 'prompt_eval_count': 27, 'prompt_eval_duration': 201380000, 'eval_count': 397, 'eval_duration': 6851786000}, id='run-dcfaf9fa-ee12-44cd-ad16-c501c0f527b1-0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Hi tell me 5 reasons why pancakes are the best breakfast for AI engineers like myself.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=llm, retriever=vector_db.as_retriever(), return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = qa.invoke(\"How did the authors increased the context length of the llama3 model? Cite the specific section source of the paper.\", return_sources=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'How did the authors increased the context length of the llama3 model? Cite the specific section source of the paper.',\n",
       " 'result': 'According to the text, the authors increased the context length of Llama 3 by using Grouped-Query Attention (GQA), which is an efficient representation that should help with longer contexts.\\n\\nSource: \"small model: it goes from 7B in Llama 2 to 8B in Llama 3. In addition, the 8B version of the model now uses Grouped-Query Attention (GQA), which is an efficient representation that should help with longer contexts.\"',\n",
       " 'source_documents': [Document(page_content='Regarding the licensing terms, Llama 3 comes with a permissive license that allows redistribution, fine-tuning, and derivative works. The requirement for explicit attribution is new in the Llama 3 license and was not present in Llama 2. Derived models, for instance, need to include \"Llama 3\" at the beginning of their name, and you also need to mention \"Built with Meta Llama 3\" in derivative works or services. For full details, please make sure to read the official license.', metadata={'description': 'We’re on a journey to advance and democratize artificial intelligence through open source and open science.', 'language': 'No language found.', 'source': 'https://huggingface.co/blog/llama3', 'title': \"Welcome Llama 3 - Meta's new open LLM\"}),\n",
       "  Document(page_content='We loaded the model in bfloat16. This is the type used by the original checkpoint published by Meta, so it’s the recommended way to run to ensure the best precision or to conduct evaluations. For real world use, it’s also safe to use float16, which may be faster depending on your hardware.', metadata={'description': 'We’re on a journey to advance and democratize artificial intelligence through open source and open science.', 'language': 'No language found.', 'source': 'https://huggingface.co/blog/llama3', 'title': \"Welcome Llama 3 - Meta's new open LLM\"}),\n",
       "  Document(page_content=\"You can deploy Llama 3 on Hugging Face's\\xa0Inference Endpoints, which uses Text Generation Inference as the backend. Text Generation Inference\\xa0is a production-ready inference container developed by Hugging Face to enable easy deployment of large language models. It has features such as continuous batching, token streaming, tensor parallelism for fast inference on multiple GPUs, and production-ready logging and tracing.\", metadata={'description': 'We’re on a journey to advance and democratize artificial intelligence through open source and open science.', 'language': 'No language found.', 'source': 'https://huggingface.co/blog/llama3', 'title': \"Welcome Llama 3 - Meta's new open LLM\"}),\n",
       "  Document(page_content='small model: it goes from 7B in Llama 2 to 8B in Llama 3. In addition, the 8B version of the model now uses Grouped-Query Attention (GQA), which is an efficient representation that should help with longer contexts.', metadata={'description': 'We’re on a journey to advance and democratize artificial intelligence through open source and open science.', 'language': 'No language found.', 'source': 'https://huggingface.co/blog/llama3', 'title': \"Welcome Llama 3 - Meta's new open LLM\"})]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do a simple semi-manual benchmark for the quality of this rag setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "queries = [\n",
    "    \"What method did the authors use to extend the context length of Llama-3-8B-Instruct from 8K to 80K?\",\n",
    "    \"How long did the entire training cycle take, and what hardware was used?\",\n",
    "    \"What are the three long-context tasks covered in the training data synthesized by GPT-4?\",\n",
    "    \"What contributions do the authors highlight in their work on extending the context length of Llama-3-8B-Instruct?\",\n",
    "    \"How does the performance of Llama-3-8B-Instruct-80K-QLoRA compare with other long-context models on popular benchmarks?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean latency in seconds: 5.006371784210205\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "latencies = []\n",
    "for query in queries:\n",
    "    start = time.time()\n",
    "    output = qa.invoke(query)\n",
    "    outputs.append(output)\n",
    "    end = time.time()\n",
    "    latencies.append(end - start)\n",
    "\n",
    "mean_latency = np.mean(latencies)\n",
    "print(f\"Mean latency in seconds: {mean_latency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What method did the authors use to extend the context length of Llama-3-8B-Instruct from 8K to 80K?\n",
      "According to the text, the authors used a method called QLoRA (Quantization, Loosening, and Reconstruction) fine-tuning to extend the context length of Llama-3-8B-Instruct from 8K to 80K. They also synthesized 3.5K long-context training data using GPT-4 to cover three long-context tasks: Single-Detail QA and Multi-Detail QA.\n",
      "[Document(page_content='I. Molybog, Y . Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M.\\nSmith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan,\\nI. Zarov, Y . Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and\\nT. Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.\\n[16] P. Zhang, Z. Liu, S. Xiao, N. Shao, Q. Ye, and Z. Dou. Soaring from 4k to 400k: Extending\\nllm’s context with activation beacon, 2024.\\n[17] X. Zhang, Y . Chen, S. Hu, Z. Xu, J. Chen, M. K. Hao, X. Han, Z. L. Thai, S. Wang, Z. Liu, and\\nM. Sun. ∞bench: Extending long context evaluation beyond 100k tokens, 2024.\\n5', metadata={'page': 4, 'source': './pdfs/llama3-80k.pdf'}), Document(page_content='I. Molybog, Y . Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M.\\nSmith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan,\\nI. Zarov, Y . Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and\\nT. Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.\\n[16] P. Zhang, Z. Liu, S. Xiao, N. Shao, Q. Ye, and Z. Dou. Soaring from 4k to 400k: Extending\\nllm’s context with activation beacon, 2024.\\n[17] X. Zhang, Y . Chen, S. Hu, Z. Xu, J. Chen, M. K. Hao, X. Han, Z. L. Thai, S. Wang, Z. Liu, and\\nM. Sun. ∞bench: Extending long context evaluation beyond 100k tokens, 2024.\\n5', metadata={'page': 4, 'source': './pdfs/llama3-80k.pdf'}), Document(page_content='Extending Llama-3’s Context Ten-Fold Overnight\\nPeitian Zhang1,2, Ninglu Shao1,2, Zheng Liu1∗, Shitao Xiao1, Hongjin Qian1,2,\\nQiwei Ye1, Zhicheng Dou2\\n1Beijing Academy of Artificial Intelligence\\n2Gaoling School of Artificial Intelligence, Renmin University of China\\nnamespace.pt@gmail.com zhengliu1026@gmail.com\\nAbstract\\nWe extend the context length of Llama-3-8B-Instruct from 8K to 80K via QLoRA\\nfine-tuning2. The entire training cycle is super efficient, which takes 8 hours on one\\n8xA800 (80G) GPU machine. The resulted model exhibits superior performances\\nacross a broad range of evaluation tasks, such as NIHS, topic retrieval, and long-\\ncontext language understanding; meanwhile, it also well preserves the original\\ncapability over short contexts. The dramatic context extension is mainly attributed\\nto merely 3.5K synthetic training samples generated by GPT-4 , which indicates\\nthe LLMs’ inherent (yet largely underestimated) potential to extend its original\\ncontext length. In fact, the context length could be extended far beyond 80K\\nwith more computation resources. Therefore, the team will publicly release the\\nentire resources (including data, model, data generation pipeline, training code) so\\nas to facilitate the future research from the community: https://github.com/\\nFlagOpen/FlagEmbedding .\\n1 Introduction\\nRecently, considerable attention has been directed towards long-context large language models,\\nwhere different approaches are adopted to establish long-context capabilities for large language\\nmodels [ 4,14,5,8,9,16,2]. However, most of them require significant compute and resources to\\naccomplish.\\nIn this technical report, we propose an efficient solution for entitling the long-context capabilities for\\nLLMs, with which we extend the context length of Llama-3-8B-Instruct3from 8K to 80K. Specifically,\\nwe use GPT-4 [13] to synthesize 3.5K long-context training data, covering three long-context tasks:\\n1.Single-Detail QA : the inquiry targets on one specific detail in a long context. To construct\\ndata for this task, we slice out a short segment (e.g., a chunk with less than 4096 tokens)\\nfrom a long context (e.g., a book or a long paper) and prompt GPT-4 to generate multiple\\nquestion-answer pairs based on this segment.\\n2.Multi-Detail QA : the inquiry requires information aggregation and reasoning over multiple\\ndetails in a long context. We define two types of long context. The homogeneous\\ncontext contains a coherent text, such as a book or a long paper. We prompt GPT-4 to\\ngenerate multiple question-answer pairs that require aggregating and analyzing information\\nfrom different locations in the context. The heterogeneous context consists of multiple\\nindependent texts. Notably, we perform clustering over a large corpus then extract texts from\\n∗Corresponding author.\\n2The model is noted as Llama-3-8B-Instruct-80K-QLoRA given its max context length during fine-tuning.\\nHowever, users could apply the model for even longer contexts via extrapolation.\\n3https://llama.meta.com/llama3/\\nPreprint. Under review.arXiv:2404.19553v1  [cs.CL]  30 Apr 2024', metadata={'page': 0, 'source': './pdfs/llama3-80k.pdf'}), Document(page_content='Extending Llama-3’s Context Ten-Fold Overnight\\nPeitian Zhang1,2, Ninglu Shao1,2, Zheng Liu1∗, Shitao Xiao1, Hongjin Qian1,2,\\nQiwei Ye1, Zhicheng Dou2\\n1Beijing Academy of Artificial Intelligence\\n2Gaoling School of Artificial Intelligence, Renmin University of China\\nnamespace.pt@gmail.com zhengliu1026@gmail.com\\nAbstract\\nWe extend the context length of Llama-3-8B-Instruct from 8K to 80K via QLoRA\\nfine-tuning2. The entire training cycle is super efficient, which takes 8 hours on one\\n8xA800 (80G) GPU machine. The resulted model exhibits superior performances\\nacross a broad range of evaluation tasks, such as NIHS, topic retrieval, and long-\\ncontext language understanding; meanwhile, it also well preserves the original\\ncapability over short contexts. The dramatic context extension is mainly attributed\\nto merely 3.5K synthetic training samples generated by GPT-4 , which indicates\\nthe LLMs’ inherent (yet largely underestimated) potential to extend its original\\ncontext length. In fact, the context length could be extended far beyond 80K\\nwith more computation resources. Therefore, the team will publicly release the\\nentire resources (including data, model, data generation pipeline, training code) so\\nas to facilitate the future research from the community: https://github.com/\\nFlagOpen/FlagEmbedding .\\n1 Introduction\\nRecently, considerable attention has been directed towards long-context large language models,\\nwhere different approaches are adopted to establish long-context capabilities for large language\\nmodels [ 4,14,5,8,9,16,2]. However, most of them require significant compute and resources to\\naccomplish.\\nIn this technical report, we propose an efficient solution for entitling the long-context capabilities for\\nLLMs, with which we extend the context length of Llama-3-8B-Instruct3from 8K to 80K. Specifically,\\nwe use GPT-4 [13] to synthesize 3.5K long-context training data, covering three long-context tasks:\\n1.Single-Detail QA : the inquiry targets on one specific detail in a long context. To construct\\ndata for this task, we slice out a short segment (e.g., a chunk with less than 4096 tokens)\\nfrom a long context (e.g., a book or a long paper) and prompt GPT-4 to generate multiple\\nquestion-answer pairs based on this segment.\\n2.Multi-Detail QA : the inquiry requires information aggregation and reasoning over multiple\\ndetails in a long context. We define two types of long context. The homogeneous\\ncontext contains a coherent text, such as a book or a long paper. We prompt GPT-4 to\\ngenerate multiple question-answer pairs that require aggregating and analyzing information\\nfrom different locations in the context. The heterogeneous context consists of multiple\\nindependent texts. Notably, we perform clustering over a large corpus then extract texts from\\n∗Corresponding author.\\n2The model is noted as Llama-3-8B-Instruct-80K-QLoRA given its max context length during fine-tuning.\\nHowever, users could apply the model for even longer contexts via extrapolation.\\n3https://llama.meta.com/llama3/\\nPreprint. Under review.arXiv:2404.19553v1  [cs.CL]  30 Apr 2024', metadata={'page': 0, 'source': './pdfs/llama3-80k.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "print(outputs[i][\"query\"])\n",
    "print(outputs[i][\"result\"])\n",
    "print(outputs[i][\"source_documents\"])\n",
    "i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-llama3",
   "language": "python",
   "name": "oreilly-llama3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
