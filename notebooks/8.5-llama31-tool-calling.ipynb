{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 3.1 Tool Calling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets-resources/llama-tool-calling-flow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U transformers accelerate bitsandbytes huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5c9200538154ffb8d40d623f75b43c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an information extraction tool. Extract all names and dates mentioned in the following text. <|eot_id|> <|start_header_id|>user<|end_header_id|> Text: \"John Doe was born on January 1, 1990. Jane Smith graduated on June 15, 2010.\" <|eot_id|> <|start_header_id|>assistant<|end_header_id|> \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " system You are an information extraction tool. Extract all names and dates mentioned in the following text.  user Text: \"John Doe was born on January 1, 1990. Jane Smith graduated on June 15, 2010.\"  assistant  Here are the extracted names and dates:\n",
      "\n",
      "**Names:**\n",
      "\n",
      "1. John Doe\n",
      "2. Jane Smith\n",
      "\n",
      "**Dates:**\n",
      "\n",
      "1. January 1, 1990\n",
      "2. June 15, 2010\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(PROMPT, return_tensors=\"pt\")\n",
    "response = model.generate(**input_ids, max_length=512)\n",
    "extracted_text = tokenizer.decode(response[0], skip_special_tokens=True)\n",
    "print(extracted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool-Calling with Llama 3.1\n",
    "\n",
    "Llama 3.1 can also use tool-calling capabilities to execute specific functions. For instance, you can create a function to execute Python code within a Jupyter Notebook environment. This can be useful for running more complex extraction logic or data processing scripts directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"you are a python data scientist. You run python code to solve tasks. Execute the code in Jupyter Notebook cells.\"\"\"\n",
    "tools = [     {         \"type\": \"function\",         \"function\": {             \"name\": \"execute_python\",             \"description\": \"Execute python code in a Jupyter notebook cell and returns any result, stdout, stderr, display_data, and error.\",             \"parameters\": {                 \"type\": \"object\",                 \"properties\": {                     \"code\": {                         \"type\": \"string\",                         \"description\": \"The python code to execute in a single cell.\",                     }                 },                 \"required\": [\"code\"],             },         },     } ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install e2b_code_interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an AI assistant with the ability to call external functions to get real-time data. The following functions are available for you to use:  - `get_weather`: Gets the current weather for a given location.   Parameters:   - `location`: The city to get the weather for.  - `get_time`: Gets the current time for a given location.   Parameters:   - `location`: The city to get the time for.  To call a function, use the following syntax: <function=function_name>{\"parameter1\": \"value1\", \"parameter2\": \"value2\"}</function> <|eot_id|> \"\"\"\n",
    "USER_PROMPT = \"\"\" <|start_header_id|>user<|end_header_id|> What is the weather in San Francisco? <|eot_id|> \"\"\"\n",
    "\n",
    "PROMPT = SYSTEM_PROMPT + USER_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(location):\n",
    "    return f\"The weather in {location} is sunny.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|> <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an AI assistant with the ability to call external functions to get real-time data. The following functions are available for you to use:  - `get_weather`: Gets the current weather for a given location.   Parameters:   - `location`: The city to get the weather for.  - `get_time`: Gets the current time for a given location.   Parameters:   - `location`: The city to get the time for.  To call a function, use the following syntax: <function=function_name>{\"parameter1\": \"value1\", \"parameter2\": \"value2\"}</function> <|eot_id|>  <|start_header_id|>user<|end_header_id|> What is the weather in San Francisco? <|eot_id|> assistant<|end_header_id|>\n",
      "\n",
      "{get_weather:{\"location\": \"San Francisco\"}}<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(PROMPT, return_tensors=\"pt\")\n",
    "response = model.generate(**input_ids, max_length=512)\n",
    "extracted_text = tokenizer.decode(response[0])\n",
    "print(extracted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating into a FastAPI app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, Request\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/chat\")\n",
    "async def chat(request: Request):\n",
    "    data = await request.json()\n",
    "    user_message = data['message']\n",
    "    prompt = f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>{user_message}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    response = model.generate(**input_ids, max_length=512)\n",
    "    generated_text = tokenizer.decode(response[0], skip_special_tokens=True)\n",
    "    return {\"response\": generated_text}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-env",
   "language": "python",
   "name": "test-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
