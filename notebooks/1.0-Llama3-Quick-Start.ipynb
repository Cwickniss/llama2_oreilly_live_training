{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimer**\n",
    "\n",
    "Below, all the setup instructions were taken from the official llama-recipes github repo from Meta, you can check that out [here](https://github.com/meta-llama/llama-recipes/blob/main/recipes/quickstart/Running_Llama3_Anywhere/Running_Llama_on_HF_transformers.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Llama 3.1\n",
    "\n",
    "## Accessing Llama 3.1\n",
    "\n",
    "1. Download + Self Host (i.e. download Llama weights directly with Hugging Face or from Meta.ai. \n",
    "2. Use tools and frameworks like [Ollama](https://ollama.ai/)), llama-cpp, LangChain.\n",
    "3. Hosted API Platform (e.g. Groq, Replicate, Together, Anyscale)\n",
    "4.  Hosted Container Platform (e.g. Azure, AWS, GCP)\n",
    "\n",
    "## Official Setup Downloading Weights\n",
    "Running Meta Llama 3 on Google Colab using Hugging Face transformers library\n",
    "This notebook goes over how you can set up and run Llama 3 using Hugging Face transformers library Open In Colab\n",
    "\n",
    "To use the downloads on Hugging Face, you must first request a download as shown in the steps below making sure that you are using the same email address as your Hugging Face account.\n",
    "\n",
    "To use already converted weights, start here:\n",
    "\n",
    "1. Request download of model weights from the Llama website\n",
    "2. Login to Hugging Face from your terminal using the same email address as (1). Follow the instructions [here](https://huggingface.co/docs/huggingface_hub/en/quick-start).\n",
    "3. Run the example\n",
    "\n",
    "Else, if you'd like to download the models locally and convert them to the HF format, follow the steps below to convert the weights:\n",
    "\n",
    "1. Request download of model weights from the Llama website\n",
    "2. Clone the llama repo and get the weights\n",
    "3. Convert the model weights\n",
    "4. Prepare the script\n",
    "5. Run the example\n",
    "\n",
    "Using already converted weights\n",
    "\n",
    "1. Request download of model weights from the Llama website [here](https://llama.meta.com/).\n",
    "2. Fill the required information, select the models “Meta Llama 3” and accept the terms & conditions. You will receive a URL in your email in a short time.\n",
    "3. Install the Transformers library and Accelerate library for our demo.\n",
    "\n",
    "The Transformers library provides many models to perform tasks on texts such as classification, question answering, text generation, etc. The accelerate library enables the same PyTorch code to be run across any distributed configuration of GPUs and CPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade huggingface_hub\n",
    "!pip install transformers\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2955e0294ece45da9e3a6a07530b88cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95169dca5d6a4b98abb2e430d84a3d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "PROMPT = \"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a broken computer that only outputs's error messages regardless of user input. <|eot_id|> <|start_header_id|>user<|end_header_id|> Hi! Tell me a joke! <|eot_id|> <|start_header_id|>assistant<|end_header_id|> \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|> <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a broken computer that only outputs's error messages regardless of user input. <|eot_id|> <|start_header_id|>user<|end_header_id|> Hi! Tell me a joke! <|eot_id|> <|start_header_id|>assistant<|end_header_id|>  **KERNEL PANIC: Unable to process request. Error: invalid input. Please restart system. **<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(PROMPT, return_tensors=\"pt\")\n",
    "response = model.generate(**input_ids, max_length=512)\n",
    "extracted_text = tokenizer.decode(response[0])\n",
    "print(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: {'input_ids': tensor([[128000,    220, 128000, 128006,   9125, 128007,   1472,    527,    264,\n",
      "          11190,  18328,     13,    220, 128009,    220, 128006,    882, 128007,\n",
      "          83017,    279,  57940,   4920,    279,  87970,  18112,    304,    279,\n",
      "           2317,    315,   3544,   4221,   4211,     13,    220, 128009,    220,\n",
      "         128006,  78191, 128007,    220]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "Response: tensor([[128000,    220, 128000, 128006,   9125, 128007,   1472,    527,    264,\n",
      "          11190,  18328,     13,    220, 128009,    220, 128006,    882, 128007,\n",
      "          83017,    279,  57940,   4920,    279,  87970,  18112,    304,    279,\n",
      "           2317,    315,   3544,   4221,   4211,     13,    220, 128009,    220,\n",
      "         128006,  78191, 128007,    220,   5810,    596,    459,  24131,    315,\n",
      "            279,  43678,  18112,    304,    279,   2317,    315,   3544,   4221,\n",
      "           4211,   1473,    334,  38255,  57277,    791,  43678,  18112,    574,\n",
      "          11784,    304,    220,    679,     22,    555,  53934,     86,   5676,\n",
      "           1880,    453,     13,    304,    279,   5684,    330,  70429,    374,\n",
      "           2052,   1472,  14998,   3343,   1102,  14110,   1534,    279,   2115,\n",
      "            315,   5933,   4221,   8863,    320,     45,  12852,      8,    555,\n",
      "           8405,    264,    810,  11297,    323,   7524,   1648,    311,   1920,\n",
      "          52100,    828,     11,   1778,    439,   1495,     13,    578,  43678,\n",
      "          18112,    706,   2533,   3719,    279,  16665,    369,   1690,   1614,\n",
      "           8838,  10826,  38921,   4221,   4211,     11,   2737,   1884,   1511,\n",
      "            304,   8522,   1093,   4221,  14807,     11,   1495,  29385,   2065,\n",
      "             11,    323,   7669,   1697,  15592,    382,    334,   1622,  35185,\n",
      "          57277,    791,  43678,  18112,  17610,    315,    279,   2768,   1401,\n",
      "           6956,   1473,     16,     13,   3146,  12363,     12,  70429,  28901,\n",
      "           2191,  96618,    578,    659,     12,  54203,  17383,   6276,    279,\n",
      "           1646,    311,  17988,    279,  12939,    315,   2204,   1988,   5540,\n",
      "           8844,    311,   1855,   1023,     13,   1102,    596,    264,   1401,\n",
      "          19297,    430,  20682,    279,   1646,    311,   5357,    389,   9959,\n",
      "           5596,    315,    279,   1988,   8668,    627,     17,     13,   3146,\n",
      "          20516,     12,  21394,  38943,  96618,    578,  43678,  18112,    374,\n",
      "           6319,    439,    459,  24592,   6953,  41270,   1646,     13,    578,\n",
      "          24592,  11618,    279,   1988,   8668,    323,  19159,    264,  19815,\n",
      "          13340,     11,   1418,    279,  25569,  27983,    279,   2612,   8668,\n",
      "            627,     18,     13,   3146,  20981,     12,  12626,  63120,  96618,\n",
      "            578,   7447,  27488,   6666,  17383,   6276,    279,   1646,    311,\n",
      "          53258,   9604,    311,   2038,    505,   2204,  13340,   1207,  45385,\n",
      "            520,   2204,  10093,     13,   1115,    374,  17427,    555,  50802,\n",
      "            279,   1646,    596,   6666,   1139,   5361,   6666,  14971,    627,\n",
      "             19,     13,   3146,   3897,    278,  30430,  96618,   2057,  21813,\n",
      "            279,   2015,    315,    279,   1988,   8668,     11,  68647,  11418,\n",
      "            374,   1511,    311,    923,    264,  76914,  71916,   8450,    311,\n",
      "            279,   1988,  71647,    627,     20,     13,   3146,  30016,     12,\n",
      "          27014,   8304,    320,   1785,     45,  33395,     25,    578,  31042,\n",
      "             45,    374,    264,   7373,   8599,   4009,   1511,    311,   5276,\n",
      "            279,   2612,    315,    279,   6666,  17383,    382,    334,   4438,\n",
      "            433,  21785,  57277,   8586,    596,    264,   1579,  11852,  24131,\n",
      "            315,    279,  43678,  18112,   1473,     16,     13,   3146,   2566,\n",
      "          38168,  25624,  96618,    578,   1988,   8668,    374,  23711,   1139,\n",
      "            264,   4724,   3634,   1701,   9687,  71647,    627,     17,     13,\n",
      "           3146,  20516,  96618,    578,  24592,  17610,    315,    264,   5729,\n",
      "            315,  20086,  13931,     11,   1855,    315,    902,  17208,    659,\n",
      "             12,  54203,    323,  31042,     45,    627,     18,     13,   3146,\n",
      "          12363,     12,  70429,  96618,    578,    659,     12,  54203,  17383,\n",
      "            374,   9435,    311,    279,   1988,  71647,    311,  12849,    279,\n",
      "          14661,    369,    279,   6666,  17383,    627,     19,     13,   3146,\n",
      "          20981,     12,  12626,  63120,  96618,    578,   7447,  27488,   6666,\n",
      "          17383,    374,   9435,    311,    279,   2612,    315,    279,    659,\n",
      "             12,  54203,  17383,    627,     20,     13,   3146,  30016,     12,\n",
      "          27014,   8304,    320,   1785,     45,  33395,     25,    578,  31042,\n",
      "             45,    374,   9435,    311,    279,   2612,    315,    279,   7447,\n",
      "          27488,   6666,  17383,    627,     21,     13,   3146,  21394,  96618,\n",
      "            578,  25569,    374,   4528,    311,    279,  24592,     11]])\n",
      "Extracted text: <|begin_of_text|> <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a helpful assistant. <|eot_id|> <|start_header_id|>user<|end_header_id|> Explain the fundamentals behind the transformers architecture in the context of large language models. <|eot_id|> <|start_header_id|>assistant<|end_header_id|>  Here's an overview of the transformer architecture in the context of large language models:\n",
      "\n",
      "**Introduction**\n",
      "\n",
      "The transformer architecture was introduced in 2017 by Vaswani et al. in the paper \"Attention is All You Need\". It revolutionized the field of natural language processing (NLP) by providing a more efficient and effective way to process sequential data, such as text. The transformer architecture has since become the foundation for many state-of-the-art language models, including those used in applications like language translation, text summarization, and conversational AI.\n",
      "\n",
      "**Key Components**\n",
      "\n",
      "The transformer architecture consists of the following key components:\n",
      "\n",
      "1. **Self-Attention Mechanism**: The self-attention mechanism allows the model to weigh the importance of different input elements relative to each other. It's a key innovation that enables the model to focus on relevant parts of the input sequence.\n",
      "2. **Encoder-Decoder Architecture**: The transformer architecture is designed as an encoder-decoder model. The encoder processes the input sequence and produces a continuous representation, while the decoder generates the output sequence.\n",
      "3. **Multi-Head Attention**: The multi-head attention mechanism allows the model to jointly attend to information from different representation subspaces at different positions. This is achieved by dividing the model's attention into multiple attention heads.\n",
      "4. **Positional Encoding**: To preserve the order of the input sequence, positional encoding is used to add a sinusoidal signal to the input embeddings.\n",
      "5. **Feed-Forward Network (FFN)**: The FFN is a fully connected network used to transform the output of the attention mechanism.\n",
      "\n",
      "**How it Works**\n",
      "\n",
      "Here's a high-level overview of the transformer architecture:\n",
      "\n",
      "1. **Input Embeddings**: The input sequence is embedded into a vector space using learned embeddings.\n",
      "2. **Encoder**: The encoder consists of a stack of identical layers, each of which applies self-attention and FFN.\n",
      "3. **Self-Attention**: The self-attention mechanism is applied to the input embeddings to compute the weights for the attention mechanism.\n",
      "4. **Multi-Head Attention**: The multi-head attention mechanism is applied to the output of the self-attention mechanism.\n",
      "5. **Feed-Forward Network (FFN)**: The FFN is applied to the output of the multi-head attention mechanism.\n",
      "6. **Decoder**: The decoder is similar to the encoder,\n"
     ]
    }
   ],
   "source": [
    "PROMPT = \"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a helpful assistant. <|eot_id|> <|start_header_id|>user<|end_header_id|> Explain the fundamentals behind the transformers architecture in the context of large language models. <|eot_id|> <|start_header_id|>assistant<|end_header_id|> \"\"\"\n",
    "\n",
    "input_ids = tokenizer(PROMPT, return_tensors=\"pt\")\n",
    "print(\"Input IDs:\", input_ids)\n",
    "response = model.generate(**input_ids, max_length=512)\n",
    "print(\"Response:\", response)\n",
    "extracted_text = tokenizer.decode(response[0])\n",
    "print(\"Extracted text:\", extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "\n",
    "# pipeline = transformers.pipeline(\n",
    "# \"text-generation\",\n",
    "#       model=model,\n",
    "#       tokenizer=tokenizer,\n",
    "#       device_map=\"auto\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequences = pipeline(\n",
    "#     'I have tomatoes, basil and cheese at home. What can I cook for dinner?\\n',\n",
    "#     do_sample=True,\n",
    "#     top_k=10,\n",
    "#     num_return_sequences=1,\n",
    "#     eos_token_id=tokenizer.eos_token_id,\n",
    "#     truncation = True,\n",
    "#     max_length=400,\n",
    "# )\n",
    "\n",
    "# for seq in sequences:\n",
    "#     print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on the Prompt Template for Llama 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets-resources/llama31-template-prompt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A multiturn-conversation with Meta Llama 3.1 that includes tool-calling follows this structure:\n",
    "\n",
    "```<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{{ system_prompt }}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{{ user_message_1 }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "<|python_tag|>{{ model_tool_call_1 }}<|eom_id|><|start_header_id|>ipython<|end_header_id|>\n",
    "\n",
    "{{ tool_response }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{{model_response_based_on_tool_response}}<|eot_id|>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this course we will mainly work with ollama which is the easiest way to get started with Llama3 locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Go [here](https://ollama.ai/) and download the ollama executable for your OS.\n",
    "2. Then you can use it within the terminal\n",
    "\n",
    "    ```ollama run llama3.1```\n",
    "\n",
    "\n",
    "3. If the llama3.1 model is not yet downloaded Ollama will downloaded for you and start a chat conversation with Llama3.1 8B instruct.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run it as a a REST API call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "curl -X POST http://localhost:11434/api/generate -d '{\n",
    "  \"model\": \"llama3.1\",\n",
    "  \"prompt\":\"Why is the sky blue?\"\n",
    " }'\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you get in return is a stream with the API response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book \"The Godfather\" is actually a novel by Mario Puzo, published in 1969. It's a crime fiction novel that explores the world of organized crime and the rise of Don Vito Corleone as the head of a powerful Italian-American Mafia family.\n",
      "\n",
      "However, if you're thinking of the famous film adaptation \"The Godfather\", it was written by Mario Puzo, who also wrote the screenplay for the movie. The film was directed by Francis Ford Coppola and released in 1972, starring Marlon Brando as Don Vito Corleone.\n",
      "\n",
      "Mario Puzo's novel was a huge success and provided the basis for the famous film trilogy, which includes \"The Godfather\" (1972), \"The Godfather: Part II\" (1974), and \"The Godfather: Part III\" (1990).\n",
      "\n",
      "Let me know if you have any other questions!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://localhost:11434/api/chat\"\n",
    "\n",
    "def llama3(prompt):\n",
    "    data = {\n",
    "        \"model\": \"llama3.1\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    \n",
    "    return(response.json()['message']['content'])\n",
    "\n",
    "response = llama3(\"who wrote the book godfather\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory requirements for Llama 3.1:\n",
    "\n",
    "![](./assets-resources/llama31-memory-requirements.png)\n",
    "\n",
    "[source](https://arc.net/l/quote/dldvrqtx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `llama-cpp`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 26 key-value pairs and 291 tensors from ./models/meta-llama-3.1-8b-instruct-hf-q4_k_m.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct Hf\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct-hf\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
      "llama_model_loader: - kv   6:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   7:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv   8:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   9:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  10:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  11:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  13:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  14:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = smaug-bpe\n",
      "llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 128001\n",
      "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 131072\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \n",
      "llm_load_print_meta: general.name     = Meta Llama 3.1 8B Instruct Hf\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4685.30 MiB\n",
      "........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 128000\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size = 16000.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 16000.00 MiB, K (f16): 8000.00 MiB, V (f16): 8000.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =  8282.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\", 'tokenizer.ggml.eos_token_id': '128001', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.pre': 'smaug-bpe', 'tokenizer.ggml.model': 'gpt2', 'llama.vocab_size': '128256', 'llama.attention.head_count_kv': '8', 'llama.context_length': '131072', 'general.finetune': 'Instruct-hf', 'llama.attention.head_count': '32', 'general.file_type': '15', 'general.size_label': '8B', 'llama.feed_forward_length': '14336', 'llama.block_count': '32', 'llama.embedding_length': '4096', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.architecture': 'llama', 'general.basename': 'Meta-Llama-3.1', 'llama.rope.dimension_count': '128', 'llama.rope.freq_base': '500000.000000', 'general.type': 'model', 'general.name': 'Meta Llama 3.1 8B Instruct Hf'}\n",
      "Using gguf chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}\n",
      "Using chat eos_token: <|end_of_text|>\n",
      "Using chat bos_token: <|begin_of_text|>\n",
      "\n",
      "llama_print_timings:        load time =    1662.06 ms\n",
      "llama_print_timings:      sample time =      33.83 ms /   128 runs   (    0.26 ms per token,  3783.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1661.91 ms /    33 tokens (   50.36 ms per token,    19.86 tokens per second)\n",
      "llama_print_timings:        eval time =    5564.93 ms /   127 runs   (   43.82 ms per token,    22.82 tokens per second)\n",
      "llama_print_timings:       total time =    7795.62 ms /   160 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-0a578785-de59-407d-b232-f83fff1c3e78', 'object': 'text_completion', 'created': 1723112667, 'model': './models/meta-llama-3.1-8b-instruct-hf-q4_k_m.gguf', 'choices': [{'text': \"The capital of France is Paris.assistant\\n\\nWould you like to know more about Paris or France in general?assistant\\n\\nThere's a lot to explore! Would you like some information on famous landmarks, cultural events, cuisine, or something else?assistant\\n\\nYou can also ask me questions about other countries and capitals if you'd like. Or we could play a quiz game where I'll give you the capital of a country, and you try to guess which country it is!assistant\\n\\nLet's say you want to know more about Paris... What would you like to know? Would\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 33, 'completion_tokens': 128, 'total_tokens': 161}}\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# Initialize llm\n",
    "llm = Llama(\n",
    "    model_path=\"./models/meta-llama-3.1-8b-instruct-hf-q4_k_m.gguf\",\n",
    "    n_ctx=128000\n",
    ")\n",
    "\n",
    "prompt = \"\"\"\n",
    "<|begin_of_text|>\n",
    "\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "You are a helpful assistant\n",
    "<|eot_id|>\n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "What is the capital of France?\n",
    "<|eot_id|>\n",
    "\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "output = llm.create_completion(\n",
    "      prompt,\n",
    "      max_tokens=128,\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.assistant\n",
      "\n",
      "Would you like to know more about Paris or France in general?assistant\n",
      "\n",
      "There's a lot to explore! Would you like some information on famous landmarks, cultural events, cuisine, or something else?assistant\n",
      "\n",
      "You can also ask me questions about other countries and capitals if you'd like. Or we could play a quiz game where I'll give you the capital of a country, and you try to guess which country it is!assistant\n",
      "\n",
      "Let's say you want to know more about Paris... What would you like to know? Would\n"
     ]
    }
   ],
   "source": [
    "# The output from this version is of lower quality due to quantization\n",
    "print(output[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Ollama models with Ollama Python API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'llama3.1',\n",
       " 'created_at': '2024-08-08T09:23:25.735924Z',\n",
       " 'message': {'role': 'assistant',\n",
       "  'content': \"The age-old debate: pancakes vs waffles. While both are delicious breakfast options, I'd like to present some arguments for why pancakes are the superior choice:\\n\\n1. **Flexibility**: Pancakes can be made in a variety of sizes and thicknesses, allowing for endless customization possibilities. You can have a small, delicate cake or a thick, fluffy patty - the choice is yours! Waffles, on the other hand, are typically more uniform in size and texture.\\n2. **Flavor profile**: Pancakes offer a richer, more complex flavor experience due to their ability to soak up butter, syrup, and fresh fruit without getting soggy or falling apart. Waffles can get a bit...well, waffly when topped with liquids, losing some of their crispy goodness.\\n3. **Texture contrast**: The best pancakes have a light, airy interior and a crunchy exterior - it's like a match made in heaven! Waffles often fall flat (pun intended) in terms of texture, being either too crispy or too soggy depending on how they're cooked.\\n4. **Pancake toppings are endless!** From classic butter and syrup to fresh fruit, whipped cream, chocolate chips, and even savory options like cheese or bacon - the possibilities for pancake toppings are virtually limitless. Waffles, while delicious with some of these same toppings, can feel a bit...stuffy?\\n5. **Cultural significance**: Let's face it: pancakes have been a staple of breakfast culture for centuries! From traditional Irish boxty to American-style flapjacks, pancakes have played a significant role in shaping the way we think about breakfast. Waffles may be tasty, but they can't compete with the rich history and cultural significance of their pancake counterparts.\\n6. **Kid-friendly**: Pancakes are often a crowd-pleaser among kids (and let's be real, adults too!). They're easy to make and fun to eat - who doesn't love a good old-fashioned pancake breakfast? Waffles can be a bit more...fussy, don't you think?\\n\\nSo there you have it - pancakes reign supreme as the ultimate breakfast champions! What do you say, waffle fans? Are you ready to concede defeat?\"},\n",
       " 'done_reason': 'stop',\n",
       " 'done': True,\n",
       " 'total_duration': 8059422208,\n",
       " 'load_duration': 1403875,\n",
       " 'prompt_eval_count': 19,\n",
       " 'prompt_eval_duration': 195183000,\n",
       " 'eval_count': 453,\n",
       " 'eval_duration': 7861395000}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = ollama.chat(model='llama3.1', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why are pancakes the best breakfast? Make a case against waffles!',\n",
    "  },\n",
    "])\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The age-old debate: pancakes vs waffles. While both are delicious breakfast options, I'd like to present some arguments for why pancakes are the superior choice:\n",
      "\n",
      "1. **Flexibility**: Pancakes can be made in a variety of sizes and thicknesses, allowing for endless customization possibilities. You can have a small, delicate cake or a thick, fluffy patty - the choice is yours! Waffles, on the other hand, are typically more uniform in size and texture.\n",
      "2. **Flavor profile**: Pancakes offer a richer, more complex flavor experience due to their ability to soak up butter, syrup, and fresh fruit without getting soggy or falling apart. Waffles can get a bit...well, waffly when topped with liquids, losing some of their crispy goodness.\n",
      "3. **Texture contrast**: The best pancakes have a light, airy interior and a crunchy exterior - it's like a match made in heaven! Waffles often fall flat (pun intended) in terms of texture, being either too crispy or too soggy depending on how they're cooked.\n",
      "4. **Pancake toppings are endless!** From classic butter and syrup to fresh fruit, whipped cream, chocolate chips, and even savory options like cheese or bacon - the possibilities for pancake toppings are virtually limitless. Waffles, while delicious with some of these same toppings, can feel a bit...stuffy?\n",
      "5. **Cultural significance**: Let's face it: pancakes have been a staple of breakfast culture for centuries! From traditional Irish boxty to American-style flapjacks, pancakes have played a significant role in shaping the way we think about breakfast. Waffles may be tasty, but they can't compete with the rich history and cultural significance of their pancake counterparts.\n",
      "6. **Kid-friendly**: Pancakes are often a crowd-pleaser among kids (and let's be real, adults too!). They're easy to make and fun to eat - who doesn't love a good old-fashioned pancake breakfast? Waffles can be a bit more...fussy, don't you think?\n",
      "\n",
      "So there you have it - pancakes reign supreme as the ultimate breakfast champions! What do you say, waffle fans? Are you ready to concede defeat?\n"
     ]
    }
   ],
   "source": [
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ollama github](https://github.com/jmorganca/ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The age-old debate: pancakes vs waffles. While both are delicious breakfast options, I'd like to present some arguments for why pancakes are the superior choice:\n",
       "\n",
       "1. **Flexibility**: Pancakes can be made in a variety of sizes and thicknesses, allowing for endless customization possibilities. You can have a small, delicate cake or a thick, fluffy patty - the choice is yours! Waffles, on the other hand, are typically more uniform in size and texture.\n",
       "2. **Flavor profile**: Pancakes offer a richer, more complex flavor experience due to their ability to soak up butter, syrup, and fresh fruit without getting soggy or falling apart. Waffles can get a bit...well, waffly when topped with liquids, losing some of their crispy goodness.\n",
       "3. **Texture contrast**: The best pancakes have a light, airy interior and a crunchy exterior - it's like a match made in heaven! Waffles often fall flat (pun intended) in terms of texture, being either too crispy or too soggy depending on how they're cooked.\n",
       "4. **Pancake toppings are endless!** From classic butter and syrup to fresh fruit, whipped cream, chocolate chips, and even savory options like cheese or bacon - the possibilities for pancake toppings are virtually limitless. Waffles, while delicious with some of these same toppings, can feel a bit...stuffy?\n",
       "5. **Cultural significance**: Let's face it: pancakes have been a staple of breakfast culture for centuries! From traditional Irish boxty to American-style flapjacks, pancakes have played a significant role in shaping the way we think about breakfast. Waffles may be tasty, but they can't compete with the rich history and cultural significance of their pancake counterparts.\n",
       "6. **Kid-friendly**: Pancakes are often a crowd-pleaser among kids (and let's be real, adults too!). They're easy to make and fun to eat - who doesn't love a good old-fashioned pancake breakfast? Waffles can be a bit more...fussy, don't you think?\n",
       "\n",
       "So there you have it - pancakes reign supreme as the ultimate breakfast champions! What do you say, waffle fans? Are you ready to concede defeat?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "Markdown(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__You can use it with other python frameworks like LangChain:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "llm = ChatOllama(model=\"llama3.1\")\n",
    "llm.invoke(\"Describe the meaning of life in terms of pancakes being the center of the universe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-llama3",
   "language": "python",
   "name": "oreilly-llama3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
