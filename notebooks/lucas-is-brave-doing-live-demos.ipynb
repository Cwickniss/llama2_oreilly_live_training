{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Stablish a system message for the model to produce good summaries of content\n",
    "2. We are going to load some text content\n",
    "3. We are going to produce the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_contents = \"\"\"\n",
    "Meta\n",
    "Large Language Model\n",
    "Introducing Meta Llama 3: The most capable openly available LLM to date\n",
    "April 18, 2024\n",
    "\n",
    "Takeaways:\n",
    "\n",
    "RECOMMENDED READS\n",
    "\n",
    "5 Steps to Getting Started with Llama 2\n",
    "The Llama Ecosystem: Past, Present, and Future\n",
    "Introducing Code Llama, a state-of-the-art large language model for coding\n",
    "Meta and Microsoft Introduce the Next Generation of Llama\n",
    "\n",
    "Today, we’re introducing Meta Llama 3, the next generation of our state-of-the-art open source large language model.\n",
    "Llama 3 models will soon be available on AWS, Databricks, Google Cloud, Hugging Face, Kaggle, IBM WatsonX, Microsoft Azure, NVIDIA NIM, and Snowflake, and with support from hardware platforms offered by AMD, AWS, Dell, Intel, NVIDIA, and Qualcomm.\n",
    "We’re dedicated to developing Llama 3 in a responsible way, and we’re offering various resources to help others use it responsibly as well. This includes introducing new trust and safety tools with Llama Guard 2, Code Shield, and CyberSec Eval 2.\n",
    "In the coming months, we expect to introduce new capabilities, longer context windows, additional model sizes, and enhanced performance, and we’ll share the Llama 3 research paper.\n",
    "Meta AI, built with Llama 3 technology, is now one of the world’s leading AI assistants that can boost your intelligence and lighten your load—helping you learn, get things done, create content, and connect to make the most out of every moment. You can try Meta AI here.\n",
    "\n",
    "Today, we’re excited to share the first two models of the next generation of Llama, Meta Llama 3, available for broad use. This release features pretrained and instruction-fine-tuned language models with 8B and 70B parameters that can support a broad range of use cases. This next generation of Llama demonstrates state-of-the-art performance on a wide range of industry benchmarks and offers new capabilities, including improved reasoning. We believe these are the best open source models of their class, period. In support of our longstanding open approach, we’re putting Llama 3 in the hands of the community. We want to kickstart the next wave of innovation in AI across the stack—from applications to developer tools to evals to inference optimizations and more. We can’t wait to see what you build and look forward to your feedback.\n",
    "\n",
    "Our goals for Llama 3\n",
    "\n",
    "\n",
    "With Llama 3, we set out to build the best open models that are on par with the best proprietary models available today. We wanted to address developer feedback to increase the overall helpfulness of Llama 3 and are doing so while continuing to play a leading role on responsible use and deployment of LLMs. We are embracing the open source ethos of releasing early and often to enable the community to get access to these models while they are still in development. The text-based models we are releasing today are the first in the Llama 3 collection of models. Our goal in the near future is to make Llama 3 multilingual and multimodal, have longer context, and continue to improve overall performance across core LLM capabilities such as reasoning and coding.\n",
    "\n",
    "State-of-the-art performance\n",
    "\n",
    "Our new 8B and 70B parameter Llama 3 models are a major leap over Llama 2 and establish a new state-of-the-art for LLM models at those scales. Thanks to improvements in pretraining and post-training, our pretrained and instruction-fine-tuned models are the best models existing today at the 8B and 70B parameter scale. Improvements in our post-training procedures substantially reduced false refusal rates, improved alignment, and increased diversity in model responses. We also saw greatly improved capabilities like reasoning, code generation, and instruction following making Llama 3 more steerable.\n",
    "\n",
    "\n",
    "\n",
    "*Please see evaluation details for setting and parameters with which these evaluations are calculated.\n",
    "\n",
    "\n",
    "In the development of Llama 3, we looked at model performance on standard benchmarks and also sought to optimize for performance for real-world scenarios. To this end, we developed a new high-quality human evaluation set. This evaluation set contains 1,800 prompts that cover 12 key use cases: asking for advice, brainstorming, classification, closed question answering, coding, creative writing, extraction, inhabiting a character/persona, open question answering, reasoning, rewriting, and summarization. To prevent accidental overfitting of our models on this evaluation set, even our own modeling teams do not have access to it. The chart below shows aggregated results of our human evaluations across of these categories and prompts against Claude Sonnet, Mistral Medium, and GPT-3.5.\n",
    "\n",
    "\n",
    "\n",
    "Preference rankings by human annotators based on this evaluation set highlight the strong performance of our 70B instruction-following model compared to competing models of comparable size in real-world scenarios.\n",
    "\n",
    "Our pretrained model also establishes a new state-of-the-art for LLM models at those scales.\n",
    "\n",
    "\n",
    "*Please see evaluation details for setting and parameters with which these evaluations are calculated.\n",
    "\n",
    "\n",
    "To develop a great language model, we believe it’s important to innovate, scale, and optimize for simplicity. We adopted this design philosophy throughout the Llama 3 project with a focus on four key ingredients: the model architecture, the pretraining data, scaling up pretraining, and instruction fine-tuning.\n",
    "\n",
    "Model architecture\n",
    "\n",
    "In line with our design philosophy, we opted for a relatively standard decoder-only transformer architecture in Llama 3. Compared to Llama 2, we made several key improvements. Llama 3 uses a tokenizer with a vocabulary of 128K tokens that encodes language much more efficiently, which leads to substantially improved model performance. To improve the inference efficiency of Llama 3 models, we’ve adopted grouped query attention (GQA) across both the 8B and 70B sizes. We trained the models on sequences of 8,192 tokens, using a mask to ensure self-attention does not cross document boundaries.\n",
    "\n",
    "Training data\n",
    "\n",
    "To train the best language model, the curation of a large, high-quality training dataset is paramount. In line with our design principles, we invested heavily in pretraining data. Llama 3 is pretrained on over 15T tokens that were all collected from publicly available sources. Our training dataset is seven times larger than that used for Llama 2, and it includes four times more code. To prepare for upcoming multilingual use cases, over 5% of the Llama 3 pretraining dataset consists of high-quality non-English data that covers over 30 languages. However, we do not expect the same level of performance in these languages as in English.\n",
    "\n",
    "To ensure Llama 3 is trained on data of the highest quality, we developed a series of data-filtering pipelines. These pipelines include using heuristic filters, NSFW filters, semantic deduplication approaches, and text classifiers to predict data quality. We found that previous generations of Llama are surprisingly good at identifying high-quality data, hence we used Llama 2 to generate the training data for the text-quality classifiers that are powering Llama 3.\n",
    "\n",
    "We also performed extensive experiments to evaluate the best ways of mixing data from different sources in our final pretraining dataset. These experiments enabled us to select a data mix that ensures that Llama 3 performs well across use cases including trivia questions, STEM, coding, historical knowledge, etc.\n",
    "\n",
    "Scaling up pretraining\n",
    "\n",
    "To effectively leverage our pretraining data in Llama 3 models, we put substantial effort into scaling up pretraining. Specifically, we have developed a series of detailed scaling laws for downstream benchmark evaluations. These scaling laws enable us to select an optimal data mix and to make informed decisions on how to best use our training compute. Importantly, scaling laws allow us to predict the performance of our largest models on key tasks (for example, code generation as evaluated on the HumanEval benchmark—see above) before we actually train the models. This helps us ensure strong performance of our final models across a variety of use cases and capabilities.\n",
    "\n",
    "We made several new observations on scaling behavior during the development of Llama 3. For example, while the Chinchilla-optimal amount of training compute for an 8B parameter model corresponds to ~200B tokens, we found that model performance continues to improve even after the model is trained on two orders of magnitude more data. Both our 8B and 70B parameter models continued to improve log-linearly after we trained them on up to 15T tokens. Larger models can match the performance of these smaller models with less training compute, but smaller models are generally preferred because they are much more efficient during inference.\n",
    "\n",
    "To train our largest Llama 3 models, we combined three types of parallelization: data parallelization, model parallelization, and pipeline parallelization. Our most efficient implementation achieves a compute utilization of over 400 TFLOPS per GPU when trained on 16K GPUs simultaneously. We performed training runs on two custom-built 24K GPU clusters. To maximize GPU uptime, we developed an advanced new training stack that automates error detection, handling, and maintenance. We also greatly improved our hardware reliability and detection mechanisms for silent data corruption, and we developed new scalable storage systems that reduce overheads of checkpointing and rollback. Those improvements resulted in an overall effective training time of more than 95%. Combined, these improvements increased the efficiency of Llama 3 training by ~three times compared to Llama 2.\n",
    "\n",
    "Instruction fine-tuning\n",
    "\n",
    "To fully unlock the potential of our pretrained models in chat use cases, we innovated on our approach to instruction-tuning as well. Our approach to post-training is a combination of supervised fine-tuning (SFT), rejection sampling, proximal policy optimization (PPO), and direct preference optimization (DPO). The quality of the prompts that are used in SFT and the preference rankings that are used in PPO and DPO has an outsized influence on the performance of aligned models. Some of our biggest improvements in model quality came from carefully curating this data and performing multiple rounds of quality assurance on annotations provided by human annotators.\n",
    "\n",
    "Learning from preference rankings via PPO and DPO also greatly improved the performance of Llama 3 on reasoning and coding tasks. We found that if you ask a model a reasoning question that it struggles to answer, the model will sometimes produce the right reasoning trace: The model knows how to produce the right answer, but it does not know how to select it. Training on preference rankings enables the model to learn how to select it.\n",
    "\n",
    "Building with Llama 3\n",
    "\n",
    "Our vision is to enable developers to customize Llama 3 to support relevant use cases and to make it easier to adopt best practices and improve the open ecosystem. With this release, we’re providing new trust and safety tools including updated components with both Llama Guard 2 and Cybersec Eval 2, and the introduction of Code Shield—an inference time guardrail for filtering insecure code produced by LLMs.\n",
    "\n",
    "We’ve also co-developed Llama 3 with torchtune, the new PyTorch-native library for easily authoring, fine-tuning, and experimenting with LLMs. torchtune provides memory efficient and hackable training recipes written entirely in PyTorch. The library is integrated with popular platforms such as Hugging Face, Weights & Biases, and EleutherAI and even supports Executorch for enabling efficient inference to be run on a wide variety of mobile and edge devices. For everything from prompt engineering to using Llama 3 with LangChain we have a comprehensive getting started guide and takes you from downloading Llama 3 all the way to deployment at scale within your generative AI application.\n",
    "\n",
    "A system-level approach to responsibility\n",
    "\n",
    "We have designed Llama 3 models to be maximally helpful while ensuring an industry leading approach to responsibly deploying them. To achieve this, we have adopted a new, system-level approach to the responsible development and deployment of Llama. We envision Llama models as part of a broader system that puts the developer in the driver’s seat. Llama models will serve as a foundational piece of a system that developers design with their unique end goals in mind.\n",
    "\n",
    "\n",
    "Instruction fine-tuning also plays a major role in ensuring the safety of our models. Our instruction-fine-tuned models have been red-teamed (tested) for safety through internal and external efforts. ​​Our red teaming approach leverages human experts and automation methods to generate adversarial prompts that try to elicit problematic responses. For instance, we apply comprehensive testing to assess risks of misuse related to Chemical, Biological, Cyber Security, and other risk areas. All of these efforts are iterative and used to inform safety fine-tuning of the models being released. You can read more about our efforts in the model card.\n",
    "\n",
    "Llama Guard models are meant to be a foundation for prompt and response safety and can easily be fine-tuned to create a new taxonomy depending on application needs. As a starting point, the new Llama Guard 2 uses the recently announced MLCommons taxonomy, in an effort to support the emergence of industry standards in this important area. Additionally, CyberSecEval 2 expands on its predecessor by adding measures of an LLM’s propensity to allow for abuse of its code interpreter, offensive cybersecurity capabilities, and susceptibility to prompt injection attacks (learn more in our technical paper). Finally, we’re introducing Code Shield which adds support for inference-time filtering of insecure code produced by LLMs. This offers mitigation of risks around insecure code suggestions, code interpreter abuse prevention, and secure command execution.\n",
    "\n",
    "With the speed at which the generative AI space is moving, we believe an open approach is an important way to bring the ecosystem together and mitigate these potential harms. As part of that, we’re updating our Responsible Use Guide (RUG) that provides a comprehensive guide to responsible development with LLMs. As we outlined in the RUG, we recommend that all inputs and outputs be checked and filtered in accordance with content guidelines appropriate to the application. Additionally, many cloud service providers offer content moderation APIs and other tools for responsible deployment, and we encourage developers to also consider using these options.\n",
    "\n",
    "Deploying Llama 3 at scale\n",
    "\n",
    "Llama 3 will soon be available on all major platforms including cloud providers, model API providers, and much more. Llama 3 will be everywhere.\n",
    "\n",
    "Our benchmarks show the tokenizer offers improved token efficiency, yielding up to 15% fewer tokens compared to Llama 2. Also, Group Query Attention (GQA) now has been added to Llama 3 8B as well. As a result, we observed that despite the model having 1B more parameters compared to Llama 2 7B, the improved tokenizer efficiency and GQA contribute to maintaining the inference efficiency on par with Llama 2 7B.\n",
    "\n",
    "For examples of how to leverage all of these capabilities, check out Llama Recipes which contains all of our open source code that can be leveraged for everything from fine-tuning to deployment to model evaluation.\n",
    "\n",
    "What’s next for Llama 3?\n",
    "\n",
    "The Llama 3 8B and 70B models mark the beginning of what we plan to release for Llama 3. And there’s a lot more to come.\n",
    "\n",
    "Our largest models are over 400B parameters and, while these models are still training, our team is excited about how they’re trending. Over the coming months, we’ll release multiple models with new capabilities including multimodality, the ability to converse in multiple languages, a much longer context window, and stronger overall capabilities. We will also publish a detailed research paper once we are done training Llama 3.\n",
    "\n",
    "To give you a sneak preview for where these models are today as they continue training, we thought we could share some snapshots of how our largest LLM model is trending. Please note that this data is based on an early checkpoint of Llama 3 that is still training and these capabilities are not supported as part of the models released today.\n",
    "\n",
    "\n",
    "*Please see evaluation details for setting and parameters with which these evaluations are calculated.\n",
    "\n",
    "\n",
    "We’re committed to the continued growth and development of an open AI ecosystem for releasing our models responsibly. We have long believed that openness leads to better, safer products, faster innovation, and a healthier overall market. This is good for Meta, and it is good for society. We’re taking a community-first approach with Llama 3, and starting today, these models are available on the leading cloud, hosting, and hardware platforms with many more to come.\n",
    "\n",
    "Try Meta Llama 3 today\n",
    "\n",
    "We’ve integrated our latest models into Meta AI, which we believe is the world’s leading AI assistant. It’s now built with Llama 3 technology and it’s available in more countries across our apps.\n",
    "\n",
    "You can use Meta AI on Facebook, Instagram, WhatsApp, Messenger, and the web to get things done, learn, create, and connect with the things that matter to you. You can read more about the Meta AI experience here.\n",
    "\n",
    "Visit the Llama 3 website to download the models and reference the Getting Started Guide for the latest list of all available platforms.\n",
    "\n",
    "You’ll also soon be able to test multimodal Meta AI on our Ray-Ban Meta smart glasses.\n",
    "\n",
    "As always, we look forward to seeing all the amazing products and experiences you will build with Meta Llama 3.\n",
    "\n",
    "Share:\n",
    "Our latest updates delivered to your inbox\n",
    "\n",
    "Subscribe to our newsletter to keep up with Meta AI news, events, research breakthroughs, and more.\n",
    "\n",
    "Join us in the pursuit of what’s possible with AI.\n",
    "\n",
    "See all open positions\n",
    "Related Posts\n",
    "\n",
    "Computer Vision\n",
    "Introducing Segment Anything: Working toward the first foundation model for image segmentation\n",
    "April 5, 2023\n",
    "Read post\n",
    "FEATURED\n",
    "\n",
    "Research\n",
    "MultiRay: Optimizing efficiency for large-scale AI models\n",
    "November 18, 2022\n",
    "Read post\n",
    "FEATURED\n",
    "\n",
    "ML Applications\n",
    "MuAViC: The first audio-video speech translation benchmark\n",
    "March 8, 2023\n",
    "Read post\n",
    "Our approach\n",
    "Research\n",
    "Product experiences\n",
    "\n",
    "Latest news\n",
    "Foundational models\n",
    "\n",
    "\n",
    "\n",
    "Search AI content\n",
    "Privacy Policy\n",
    "Terms\n",
    "Cookies\n",
    "Meta © 2024\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here is a summary of the article in bullet points:\\n\\n**Quality of Prompts and Preference Rankings**\\n\\n* Quality of prompts used in SFT has an outsized influence on the performance of aligned models.\\n* Preference rankings via PPO and DPO greatly improved the performance of Llama 3 on reasoning and coding tasks.\\n\\n**Building with Llama 3**\\n\\n* New trust and safety tools, including updated components with Llama Guard 2 and Cybersec Eval 2.\\n* Introduction of Code Shield for filtering insecure code produced by LLMs.\\n* Co-development with torchtune, a PyTorch-native library for training LLMs.\\n\\n**System-Level Approach to Responsibility**\\n\\n* Designed models to be maximally helpful while ensuring an industry-leading approach to responsibly deploying them.\\n* Adopted a new, system-level approach to the responsible development and deployment of Llama models.\\n\\n**Instruction Fine-Tuning and Red Teaming**\\n\\n* Instruction fine-tuned models have been red-teamed (tested) for safety through internal and external efforts.\\n* Comprehensive testing to assess risks of misuse related to Chemical, Biological, Cyber Security, and other risk areas.\\n\\n**Deploying Llama 3 at Scale**\\n\\n* Available on all major platforms, including cloud providers, model API providers, and more.\\n* Benchmarks show the tokenizer offers improved token efficiency, yielding up to 15% fewer tokens compared to Llama 2.\\n\\n**What's Next for Llama 3?**\\n\\n* Largest models are over 400B parameters and still training.\\n* Release multiple models with new capabilities, including multimodality, longer context window, and stronger overall capabilities.\\n\\n**Try Meta Llama 3 Today**\\n\\n* Integrated latest models into Meta AI, available in more countries across Facebook, Instagram, WhatsApp, Messenger, and the web.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = ollama.chat(model='llama3', messages=[\n",
    "    {'role': 'system', \n",
    "     'content':\"You are a summarization engine. You take in information and output bullet point summaries.\"\n",
    "     },\n",
    "    {\n",
    "    'role': 'user',\n",
    "    'content': f'Summarize the following article: {article_contents}',\n",
    "    },\n",
    "])\n",
    "output = response['message']['content']\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here is a summary of the article in bullet points:\n",
       "\n",
       "**Quality of Prompts and Preference Rankings**\n",
       "\n",
       "* Quality of prompts used in SFT has an outsized influence on the performance of aligned models.\n",
       "* Preference rankings via PPO and DPO greatly improved the performance of Llama 3 on reasoning and coding tasks.\n",
       "\n",
       "**Building with Llama 3**\n",
       "\n",
       "* New trust and safety tools, including updated components with Llama Guard 2 and Cybersec Eval 2.\n",
       "* Introduction of Code Shield for filtering insecure code produced by LLMs.\n",
       "* Co-development with torchtune, a PyTorch-native library for training LLMs.\n",
       "\n",
       "**System-Level Approach to Responsibility**\n",
       "\n",
       "* Designed models to be maximally helpful while ensuring an industry-leading approach to responsibly deploying them.\n",
       "* Adopted a new, system-level approach to the responsible development and deployment of Llama models.\n",
       "\n",
       "**Instruction Fine-Tuning and Red Teaming**\n",
       "\n",
       "* Instruction fine-tuned models have been red-teamed (tested) for safety through internal and external efforts.\n",
       "* Comprehensive testing to assess risks of misuse related to Chemical, Biological, Cyber Security, and other risk areas.\n",
       "\n",
       "**Deploying Llama 3 at Scale**\n",
       "\n",
       "* Available on all major platforms, including cloud providers, model API providers, and more.\n",
       "* Benchmarks show the tokenizer offers improved token efficiency, yielding up to 15% fewer tokens compared to Llama 2.\n",
       "\n",
       "**What's Next for Llama 3?**\n",
       "\n",
       "* Largest models are over 400B parameters and still training.\n",
       "* Release multiple models with new capabilities, including multimodality, longer context window, and stronger overall capabilities.\n",
       "\n",
       "**Try Meta Llama 3 Today**\n",
       "\n",
       "* Integrated latest models into Meta AI, available in more countries across Facebook, Instagram, WhatsApp, Messenger, and the web."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is a summary of the article in bullet points:\\n\\n**Quality of Prompts and Preference Rankings**\\n\\n* The quality of prompts used in SFT and preference rankings used in PPO and DPO have an outsize influence on the performance of aligned models.\\n* Carefully curating this data and performing multiple rounds of quality assurance on annotations provided by human annotators led to significant improvements in model quality.\\n\\n**Llama 3 Features**\\n\\n* Llama 3 can be customized to support relevant use cases and adopt best practices.\\n* New trust and safety tools include:\\n\\t+ Updated components with Llama Guard 2 and Cybersec Eval 2.\\n\\t+ Code Shield: an inference time guardrail for filtering insecure code produced by LLMs.\\n\\n**Responsibility and Safety**\\n\\n* A system-level approach to responsibility has been designed to ensure the safe deployment of Llama models.\\n* Instruction fine-tuning plays a major role in ensuring safety, with red teaming efforts used to test models for safety.\\n\\n**Deployment at Scale**\\n\\n* Llama 3 will be available on all major platforms, including cloud providers and model API providers.\\n* Benchmarks show improved token efficiency and Group Query Attention (GQA) in Llama 3 8B.\\n\\n**Future Plans**\\n\\n* Larger models with over 400B parameters are still training, but early results look promising.\\n* Multiple models with new capabilities will be released in the coming months, including multimodality and stronger overall capabilities.\\n\\n**Availability and Integration**\\n\\n* Llama 3 is available on Meta AI, which is built with Llama 3 technology and available in more countries across Facebook, Instagram, WhatsApp, Messenger, and the web.\\n* Meta AI is integrated into Ray-Ban Meta smart glasses.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_nice_summaries(article_contents):\n",
    "    response = ollama.chat(model='llama3', messages=[\n",
    "        {'role': 'system', \n",
    "         'content':\"You are a summarization engine. You take in information and output bullet point summaries.\"\n",
    "         },\n",
    "        {\n",
    "        'role': 'user',\n",
    "        'content': f'Summarize the following article: {article_contents}',\n",
    "        },\n",
    "    ])\n",
    "    output = response['message']['content']\n",
    "    return output\n",
    "\n",
    "create_nice_summaries(article_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_contents = \"\"\"\n",
    "\n",
    "\n",
    "25\n",
    "Method MNLI-m (Val. Acc./%) RTE (Val. Acc./%)\n",
    "GPT-3 Few-Shot 40.6 69.0\n",
    "GPT-3 Fine-Tuned 89.5 85.4\n",
    "Table 8: Fine-tuning significantly outperforms few-shot learning on GPT-3 (Brown et al., 2020).\n",
    "B INFERENCE LATENCY INTRODUCED BY ADAPTER LAYERS\n",
    "Adapter layers are external modules added to a pre-trained model in a sequential manner, whereas\n",
    "our proposal, LoRA, can be seen as external modules added in a parallel manner. Consequently,\n",
    "adapter layers must be computed in addition to the base model, inevitably introducing additional\n",
    "latency. While as pointed out in R  ̈uckl ́e et al. (2020), the latency introduced by adapter layers can\n",
    "be mitigated when the model batch size and/or sequence length is large enough to full utilize the\n",
    "hardware parallelism. We confirm their observation with a similar latency study on GPT-2 medium\n",
    "and point out that there are scenarios, notably online inference where the batch size is small, where\n",
    "the added latency can be significant.\n",
    "We measure the latency of a single forward pass on an NVIDIA Quadro RTX8000 by averaging\n",
    "over 100 trials. We vary the input batch size, sequence length, and the adapter bottleneck dimension\n",
    "r. We test two adapter designs: the original one by Houlsby et al. (2019), which we call AdapterH,\n",
    "and a recent, more efficient variant by Lin et al. (2020), which we call AdapterL. See Section 5.1\n",
    "for more details on the designs. We plot the slow-down in percentage compared to the no-adapter\n",
    "baseline in Figure 5.0\n",
    "5\n",
    "10\n",
    "15\n",
    "20\n",
    "25\n",
    "30\n",
    "35\n",
    "0\n",
    "10\n",
    "100\n",
    "250\n",
    "AdapterH  r\n",
    "Seq Len = 128 Seq Len = 256 Seq Len = 512\n",
    "1 2 4 8 16 32\n",
    "Batch Size\n",
    "0\n",
    "10\n",
    "100\n",
    "250\n",
    "AdapterL  r\n",
    "1 2 4 8 16 32\n",
    "Batch Size\n",
    "1 2 4 8 16 32\n",
    "Batch Size\n",
    "Figure 5: Percentage slow-down of inference latency compared to the no-adapter (r = 0) baseline.\n",
    "The top row shows the result for AdapterH and the bottom row AdapterL. Larger batch size and\n",
    "sequence length help to mitigate the latency, but the slow-down can be as high as over 30% in an\n",
    "online, short-sequence-length scenario. We tweak the colormap for better visibility.\n",
    "C DATASET DETAILS\n",
    "GLUE Benchmark is a wide-ranging collection of natural language understanding tasks. It includes\n",
    "MNLI (inference, Williams et al. (2018)), SST-2 (sentiment analysis, Socher et al. (2013)), MRPC\n",
    "(paraphrase detection, Dolan & Brockett (2005)), CoLA (linguistic acceptability, Warstadt et al.\n",
    "(2018)), QNLI (inference, Rajpurkar et al. (2018)), QQP8 (question-answering), RTE (inference),\n",
    "8 https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs\n",
    "17\n",
    "and STS-B (textual similarity, Cer et al. (2017)). The broad coverage makes GLUE benchmark a\n",
    "standard metric to evaluate NLU models such as RoBERTa and DeBERTa. The individual datasets\n",
    "are released under different permissive licenses.\n",
    "WikiSQL is introduced in Zhong et al. (2017) and contains 56,355/8,421 training/validation ex-\n",
    "amples. The task is to generate SQL queries from natural language questions and table schemata.\n",
    "We encode context as x = {table schema,query}and target as y = {SQL}. The dataset is release\n",
    "under the BSD 3-Clause License.\n",
    "SAMSum is introduced in Gliwa et al. (2019) and contains 14,732/819 training/test examples. It\n",
    "consists of staged chat conversations between two people and corresponding abstractive summaries\n",
    "written by linguists. We encode context as ”\\n” concatenated utterances followed by a ”\\n\\n”,\n",
    "and target as y = {summary}. The dataset is released under the non-commercial licence: Creative\n",
    "Commons BY-NC-ND 4.0.\n",
    "E2E NLG Challenge was first introduced in Novikova et al. (2017) as a dataset for training end-to-\n",
    "end, data-driven natural language generation systems and is commonly used for data-to-text evalua-\n",
    "tion. The E2E dataset consists of roughly 42,000 training, 4,600 validation, and 4,600 test exam-\n",
    "ples from the restaurant domain. Each source table used as input can have multiple references. Each\n",
    "sample input (x,y) consists of a sequence of slot-value pairs, along with a corresponding natural\n",
    "language reference text. The dataset is released under Creative Commons BY-NC-SA 4.0.\n",
    "DART is an open-domain data-to-text dataset described in Nan et al. (2020). DART inputs are\n",
    "structured as sequences of ENTITY — RELATION — ENTITY triples. With 82K examples in\n",
    "total, DART is a significantly larger and more complex data-to-text task compared to E2E. The\n",
    "dataset is released under the MIT license.\n",
    "WebNLG is another commonly used dataset for data-to-text evaluation (Gardent et al., 2017). With\n",
    "22K examples in total WebNLG comprises 14 distinct categories, nine of which are seen during\n",
    "training. Since five of the 14 total categories are not seen during training, but are represented in\n",
    "the test set, evaluation is typically broken out by “seen” categories (S), “unseen” categories (U)\n",
    "and “all” (A). Each input example is represented by a sequence of SUBJECT — PROPERTY —\n",
    "OBJECT triples. The dataset is released under Creative Commons BY-NC-SA 4.0.\n",
    "D HYPERPARAMETERS USED IN EXPERIMENTS\n",
    "D.1 ROBERTA\n",
    "We train using AdamW with a linear learning rate decay schedule. We sweep learning rate, number\n",
    "of training epochs, and batch size for LoRA. Following Liu et al. (2019), we initialize the LoRA\n",
    "modules to our best MNLI checkpoint when adapting to MRPC, RTE, and STS-B, instead of the\n",
    "usual initialization; the pre-trained model stays frozen for all tasks. We report the median over 5\n",
    "random seeds; the result for each run is taken from the best epoch. For a fair comparison with the\n",
    "setup in Houlsby et al. (2019) and Pfeiffer et al. (2021), we restrict the model sequence length to 128\n",
    "and used a fixed batch size for all tasks. Importantly, we start with the pre-trained RoBERTa large\n",
    "model when adapting to MRPC, RTE, and STS-B, instead of a model already adapted to MNLI.\n",
    "The runs with this restricted setup are marked with †. See the hyperparameters used in our runs\n",
    "in Table 9.\n",
    "D.2 DEBERTA\n",
    "We again train using AdamW with a linear learning rate decay schedule. Following He et al. (2021),\n",
    "we tune learning rate, dropout probability, warm-up steps, and batch size. We use the same model\n",
    "sequence length used by (He et al., 2021) to keep our comparison fair. Following He et al. (2021),\n",
    "we initialize the LoRA modules to our best MNLI checkpoint when adapting to MRPC, RTE, and\n",
    "STS-B, instead of the usual initialization; the pre-trained model stays frozen for all tasks. We report\n",
    "the median over 5 random seeds; the result for each run is taken from the best epoch. See the\n",
    "hyperparameters used in our runs in Table 10.\n",
    "18\n",
    "Method Dataset MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B\n",
    "Optimizer AdamW\n",
    "Warmup Ratio 0.06\n",
    "LR Schedule Linear\n",
    "RoBERTa base\n",
    "LoRA\n",
    "Batch Size 16 16 16 32 32 16 32 16\n",
    "# Epochs 30 60 30 80 25 25 80 40\n",
    "Learning Rate 5E-04 5E-04 4E-04 4E-04 4E-04 5E-04 5E-04 4E-04\n",
    "LoRA Config. rq =rv =8\n",
    "LoRA α 8\n",
    "Max Seq. Len. 512\n",
    "RoBERTa large\n",
    "LoRA\n",
    "Batch Size 4 4 4 4 4 4 8 8\n",
    "# Epochs 10 10 20 20 10 20 20 30\n",
    "Learning Rate 3E-04 4E-04 3E-04 2E-04 2E-04 3E-04 4E-04 2E-04\n",
    "LoRA Config. rq =rv =8\n",
    "LoRA α 16\n",
    "Max Seq. Len. 128 128 512 128 512 512 512 512\n",
    "RoBERTa large\n",
    "LoRA†\n",
    "Batch Size 4\n",
    "# Epochs 10 10 20 20 10 20 20 10\n",
    "Learning Rate 3E-04 4E-04 3E-04 2E-04 2E-04 3E-04 4E-04 2E-04\n",
    "LoRA Config. rq =rv =8\n",
    "LoRA α 16\n",
    "Max Seq. Len. 128\n",
    "RoBERTa large\n",
    "AdptP (3M)†\n",
    "Batch Size 32\n",
    "# Epochs 10 20 20 20 10 20 20 20\n",
    "Learning Rate 3E-05 3E-05 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04\n",
    "Bottleneck r 64\n",
    "Max Seq. Len. 128\n",
    "RoBERTa large\n",
    "AdptP (0.8M)†\n",
    "Batch Size 32\n",
    "# Epochs 5 20 20 20 10 20 20 20\n",
    "Learning Rate 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04\n",
    "Bottleneck r 16\n",
    "Max Seq. Len. 128\n",
    "RoBERTa large\n",
    "AdptH (6M)†\n",
    "Batch Size 32\n",
    "# Epochs 10 5 10 10 5 20 20 10\n",
    "Learning Rate 3E-05 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04\n",
    "Bottleneck r 64\n",
    "Max Seq. Len. 128\n",
    "RoBERTa large\n",
    "AdptH (0.8M)†\n",
    "Batch Size 32\n",
    "# Epochs 10 5 10 10 5 20 20 10\n",
    "Learning Rate 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04\n",
    "Bottleneck r 8\n",
    "Max Seq. Len. 128\n",
    "Table 9: The hyperparameters we used for RoBERTa on the GLUE benchmark.\n",
    "D.3 GPT-2\n",
    "We train all of our GPT-2 models using AdamW (Loshchilov & Hutter, 2017) with a linear learning\n",
    "rate schedule for 5 epochs. We use the batch size, learning rate, and beam search beam size described\n",
    "in Li & Liang (2021). Accordingly, we also tune the above hyperparameters for LoRA. We report the\n",
    "mean over 3 random seeds; the result for each run is taken from the best epoch. The hyperparameters\n",
    "used for LoRA in GPT-2 are listed in Table 11. For those used for other baselines, see Li & Liang\n",
    "(2021).\n",
    "D.4 GPT-3\n",
    "For all GPT-3 experiments, we train using AdamW (Loshchilov & Hutter, 2017) for 2 epochs with\n",
    "a batch size of 128 samples and a weight decay factor of 0.1. We use a sequence length of 384 for\n",
    "19\n",
    "Method Dataset MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B\n",
    "Optimizer AdamW\n",
    "Warmup Ratio 0.1\n",
    "LR Schedule Linear\n",
    "DeBERTa XXL\n",
    "LoRA\n",
    "Batch Size 8 8 32 4 6 8 4 4\n",
    "# Epochs 5 16 30 10 8 11 11 10\n",
    "Learning Rate 1E-04 6E-05 2E-04 1E-04 1E-04 1E-04 2E-04 2E-04\n",
    "Weight Decay 0 0.01 0.01 0 0.01 0.01 0.01 0.1\n",
    "CLS Dropout 0.15 0 0 0.1 0.1 0.2 0.2 0.2\n",
    "LoRA Config. rq =rv =8\n",
    "LoRA α 8\n",
    "Max Seq. Len. 256 128 128 64 512 320 320 128\n",
    "Table 10: The hyperparameters for DeBERTa XXL on tasks included in the GLUE benchmark.\n",
    "Dataset E2E WebNLG DART\n",
    "Training\n",
    "Optimizer AdamW\n",
    "Weight Decay 0.01 0.01 0.0\n",
    "Dropout Prob 0.1 0.1 0.0\n",
    "Batch Size 8\n",
    "# Epoch 5\n",
    "Warmup Steps 500\n",
    "Learning Rate Schedule Linear\n",
    "Label Smooth 0.1 0.1 0.0\n",
    "Learning Rate 0.0002\n",
    "Adaptation rq = rv = 4\n",
    "LoRA α 32\n",
    "Inference\n",
    "Beam Size 10\n",
    "Length Penalty 0.9 0.8 0.8\n",
    "no repeat ngram size 4\n",
    "Table 11: The hyperparameters for GPT-2 LoRA on E2E, WebNLG and DART.\n",
    "WikiSQL (Zhong et al., 2017), 768 for MNLI (Williams et al., 2018), and 2048 for SAMSum (Gliwa\n",
    "et al., 2019). We tune learning rate for all method-dataset combinations. See Section D.4 for more\n",
    "details on the hyperparameters used. For prefix-embedding tuning, we find the optimal lp and li\n",
    "to be 256 and 8, respectively, totalling 3.2M trainable parameters. We use lp = 8 and li = 8 for\n",
    "prefix-layer tuning with 20.2M trainable parameters to obtain the overall best performance. We\n",
    "present two parameter budgets for LoRA: 4.7M (rq = rv = 1 or rv = 2) and 37.7M (rq = rv = 8\n",
    "or rq = rk = rv = ro = 2). We report the best validation performance from each run. The training\n",
    "hyperparameters used in our GPT-3 experiments are listed in Table 12.\n",
    "E COMBINING LORA WITH PREFIX TUNING\n",
    "LoRA can be naturally combined with existing prefix-based approaches. In this section, we evaluate\n",
    "two combinations of LoRA and variants of prefix-tuning on WikiSQL and MNLI.\n",
    "LoRA+PrefixEmbed (LoRA+PE) combines LoRA with prefix-embedding tuning, where we insert\n",
    "lp + li special tokens whose embeddings are treated as trainable parameters. For more on prefix-\n",
    "embedding tuning, see Section 5.1.\n",
    "LoRA+PrefixLayer (LoRA+PL) combines LoRA with prefix-layer tuning. We also insert lp + li\n",
    "special tokens; however, instead of letting the hidden representations of these tokens evolve natu-\n",
    "20\n",
    "Hyperparameters Fine-Tune PreEmbed PreLayer BitFit AdapterH LoRA\n",
    "Optimizer AdamW\n",
    "Batch Size 128\n",
    "# Epoch 2\n",
    "Warmup Tokens 250,000\n",
    "LR Schedule Linear\n",
    "Learning Rate 5.00E-06 5.00E-04 1.00E-04 1.6E-03 1.00E-04 2.00E-04\n",
    "Table 12: The training hyperparameters used for different GPT-3 adaption methods. We use the\n",
    "same hyperparameters for all datasets after tuning learning rate.\n",
    "rally, we replace them after every Transformer block with an input agnostic vector. Thus, both the\n",
    "embeddings and subsequent Transformer block activations are treated as trainable parameters. For\n",
    "more on prefix-layer tuning, see Section 5.1.\n",
    "In Table 15, we show the evaluation results of LoRA+PE and LoRA+PL on WikiSQL and MultiNLI.\n",
    "First of all, LoRA+PE significantly outperforms both LoRA and prefix-embedding tuning on\n",
    "WikiSQL, which indicates that LoRA is somewhat orthogonal to prefix-embedding tuning. On\n",
    "MultiNLI, the combination of LoRA+PE doesn’t perform better than LoRA, possibly because LoRA\n",
    "on its own already achieves performance comparable to the human baseline. Secondly, we notice\n",
    "that LoRA+PL performs slightly worse than LoRA even with more trainable parameters. We at-\n",
    "tribute this to the fact that prefix-layer tuning is very sensitive to the choice of learning rate and thus\n",
    "makes the optimization of LoRA weights more difficult in LoRA+PL.\n",
    "F ADDITIONAL EMPIRICAL EXPERIMENTS\n",
    "F.1 ADDITIONAL EXPERIMENTS ON GPT-2\n",
    "We also repeat our experiment on DART (Nan et al., 2020) and WebNLG (Gardent et al., 2017)\n",
    "following the setup of Li & Liang (2021). The result is shown in Table 13. Similar to our result\n",
    "on E2E NLG Challenge, reported in Section 5, LoRA performs better than or at least on-par with\n",
    "prefix-based approaches given the same number of trainable parameters.\n",
    "Method # Trainable DART\n",
    "Parameters BLEU↑ MET↑ TER↓\n",
    "GPT-2 Medium\n",
    "Fine-Tune 354M 46.2 0.39 0.46\n",
    "AdapterL 0.37M 42.4 0.36 0.48\n",
    "AdapterL 11M 45.2 0.38 0.46\n",
    "FTTop2 24M 41.0 0.34 0.56\n",
    "PrefLayer 0.35M 46.4 0.38 0.46\n",
    "LoRA 0.35M 47.1±.2 0.39 0.46\n",
    "GPT-2 Large\n",
    "Fine-Tune 774M 47.0 0.39 0.46\n",
    "AdapterL 0.88M 45.7±.1 0.38 0.46\n",
    "AdapterL 23M 47.1±.1 0.39 0.45\n",
    "PrefLayer 0.77M 46.7 0.38 0.45\n",
    "LoRA 0.77M 47.5±.1 0.39 0.45\n",
    "Table 13: GPT-2 with different adaptation methods on DART. The variances of MET and TER are\n",
    "less than 0.01 for all adaption approaches.\n",
    "21\n",
    "Method WebNLG\n",
    "BLEU↑ MET↑ TER↓\n",
    "U S A U S A U S A\n",
    "GPT-2 Medium\n",
    "Fine-Tune (354M) 27.7 64.2 46.5 .30 .45 .38 .76 .33 .53\n",
    "AdapterL (0.37M) 45.1 54.5 50.2 .36 .39 .38 .46 .40 .43\n",
    "AdapterL (11M) 48.3 60.4 54.9 .38 .43 .41 .45 .35 .39\n",
    "FTTop2 (24M) 18.9 53.6 36.0 .23 .38 .31 .99 .49 .72\n",
    "Prefix (0.35M) 45.6 62.9 55.1 .38 .44 .41 .49 .35 .40\n",
    "LoRA (0.35M) 46.7±.4 62.1±.2 55.3±.2 .38 .44 .41 .46 .33 .39\n",
    "GPT-2 Large\n",
    "Fine-Tune (774M) 43.1 65.3 55.5 .38 .46 .42 .53 .33 .42\n",
    "AdapterL (0.88M) 49.8±.0 61.1±.0 56.0±.0 .38 .43 .41 .44 .35 .39\n",
    "AdapterL (23M) 49.2±.1 64.7±.2 57.7±.1 .39 .46 .43 .46 .33 .39\n",
    "Prefix (0.77M) 47.7 63.4 56.3 .39 .45 .42 .48 .34 .40\n",
    "LoRA (0.77M) 48.4±.3 64.0±.3 57.0±.1 .39 .45 .42 .45 .32 .38\n",
    "Table 14: GPT-2 with different adaptation methods on WebNLG. The variances of MET and TER\n",
    "are less than 0.01 for all the experiments we ran. “U” indicates unseen categories, “S” indicates seen\n",
    "categories, and “A” indicates all categories in the test set of WebNLG.\n",
    "F.2 ADDITIONAL EXPERIMENTS ON GPT-3\n",
    "We present additional runs on GPT-3 with different adaptation methods in Table 15. The focus is on\n",
    "identifying the trade-off between performance and the number of trainable parameters.\n",
    "F.3 LOW-DATA REGIME\n",
    "To evaluate the performance of different adaptation approaches in the low-data regime. we randomly\n",
    "sample 100, 1k and 10k training examples from the full training set of MNLI to form the low-data\n",
    "MNLI-n tasks. In Table 16, we show the performance of different adaptation approaches on MNLI-\n",
    "n. To our surprise, PrefixEmbed and PrefixLayer performs very poorly on MNLI-100 dataset, with\n",
    "PrefixEmbed performing only slightly better than random chance (37.6% vs. 33.3%). PrefixLayer\n",
    "performs better than PrefixEmbed but is still significantly worse than Fine-Tune or LoRA on MNLI-\n",
    "100. The gap between prefix-based approaches and LoRA/Fine-tuning becomes smaller as we in-\n",
    "crease the number of training examples, which might suggest that prefix-based approaches are not\n",
    "suitable for low-data tasks in GPT-3. LoRA achieves better performance than fine-tuning on both\n",
    "MNLI-100 and MNLI-Full, and comparable results on MNLI-1k and MNLI-10K considering the\n",
    "(±0.3) variance due to random seeds.\n",
    "The training hyperparameters of different adaptation approaches on MNLI-n are reported in Ta-\n",
    "ble 17. We use a smaller learning rate for PrefixLayer on the MNLI-100 set, as the training loss does\n",
    "not decrease with a larger learning rate.\n",
    "G MEASURING SIMILARITY BETWEEN SUBSPACES\n",
    "In this paper we use the measure φ(A,B,i,j) = ψ(UiA,Uj\n",
    "B) = ‖Ui>\n",
    "A UB‖2\n",
    "F\n",
    "min{i,j} to measure the subspace\n",
    "similarity between two column orthonormal matrices UiA ∈ Rd×i and Uj\n",
    "B ∈ Rd×j, obtained by\n",
    "taking columns of the left singular matrices of A and B. We point out that this similarity is simply\n",
    "a reverse of the standard Projection Metric that measures distance between subspaces Ham & Lee\n",
    "(2008).\n",
    "22\n",
    "Method Hyperparameters # Trainable Parameters WikiSQL MNLI-m\n",
    "Fine-Tune - 175B 73.8 89.5\n",
    "PrefixEmbed\n",
    "lp = 32,li = 8 0.4 M 55.9 84.9\n",
    "lp = 64,li = 8 0.9 M 58.7 88.1\n",
    "lp = 128,li = 8 1.7 M 60.6 88.0\n",
    "lp = 256,li = 8 3.2 M 63.1 88.6\n",
    "lp = 512,li = 8 6.4 M 55.9 85.8\n",
    "PrefixLayer\n",
    "lp = 2,li = 2 5.1 M 68.5 89.2\n",
    "lp = 8,li = 0 10.1 M 69.8 88.2\n",
    "lp = 8,li = 8 20.2 M 70.1 89.5\n",
    "lp = 32,li = 4 44.1 M 66.4 89.6\n",
    "lp = 64,li = 0 76.1 M 64.9 87.9\n",
    "AdapterH\n",
    "r = 1 7.1 M 71.9 89.8\n",
    "r = 4 21.2 M 73.2 91.0\n",
    "r = 8 40.1 M 73.2 91.5\n",
    "r = 16 77.9 M 73.2 91.5\n",
    "r = 64 304.4 M 72.6 91.5\n",
    "LoRA\n",
    "rv = 2 4.7 M 73.4 91.7\n",
    "rq = rv = 1 4.7 M 73.4 91.3\n",
    "rq = rv = 2 9.4 M 73.3 91.4\n",
    "rq = rk = rv = ro = 1 9.4 M 74.1 91.2\n",
    "rq = rv = 4 18.8 M 73.7 91.3\n",
    "rq = rk = rv = ro = 2 18.8 M 73.7 91.7\n",
    "rq = rv = 8 37.7 M 73.8 91.6\n",
    "rq = rk = rv = ro = 4 37.7 M 74.0 91.7\n",
    "rq = rv = 64 301.9 M 73.6 91.4\n",
    "rq = rk = rv = ro = 64 603.8 M 73.9 91.4\n",
    "LoRA+PE\n",
    "rq = rv = 8,lp = 8,li = 4 37.8 M 75.0 91.4\n",
    "rq = rv = 32,lp = 8,li = 4 151.1 M 75.9 91.1\n",
    "rq = rv = 64,lp = 8,li = 4 302.1 M 76.2 91.3\n",
    "LoRA+PL rq = rv = 8,lp = 8,li = 4 52.8 M 72.9 90.2\n",
    "Table 15: Hyperparameter analysis of different adaptation approaches on WikiSQL and MNLI. Both\n",
    "prefix-embedding tuning (PrefixEmbed) and prefix-layer tuning (PrefixLayer) perform worse as we\n",
    "increase the number of trainable parameters, while LoRA’s performance stabilizes. Performance is\n",
    "measured in validation accuracy.\n",
    "Method MNLI(m)-100 MNLI(m)-1k MNLI(m)-10k MNLI(m)-392K\n",
    "GPT-3 (Fine-Tune) 60.2 85.8 88.9 89.5\n",
    "GPT-3 (PrefixEmbed) 37.6 75.2 79.5 88.6\n",
    "GPT-3 (PrefixLayer) 48.3 82.5 85.9 89.6\n",
    "GPT-3 (LoRA) 63.8 85.6 89.2 91.7\n",
    "Table 16: Validation accuracy of different methods on subsets of MNLI using GPT-3 175B. MNLI-\n",
    "n describes a subset with n training examples. We evaluate with the full validation set. LoRA\n",
    "performs exhibits favorable sample-efficiency compared to other methods, including fine-tuning.\n",
    "To be concrete, let the singular values of Ui>A Uj\n",
    "B to be σ1,σ2,··· ,σp where p = min{i,j}. We\n",
    "know that the Projection Metric Ham & Lee (2008) is defined as:\n",
    "d(UiA,Uj\n",
    "B) =\n",
    "√√√√p −\n",
    "p∑\n",
    "i=1\n",
    "σ2i ∈[0,√p]\n",
    "23\n",
    "Hyperparameters Adaptation MNLI-100 MNLI-1k MNLI-10K MNLI-392K\n",
    "Optimizer - AdamW\n",
    "Warmup Tokens - 250,000\n",
    "LR Schedule - Linear\n",
    "Batch Size - 20 20 100 128\n",
    "# Epoch - 40 40 4 2\n",
    "Learning Rate\n",
    "FineTune 5.00E-6\n",
    "PrefixEmbed 2.00E-04 2.00E-04 4.00E-04 5.00E-04\n",
    "PrefixLayer 5.00E-05 5.00E-05 5.00E-05 1.00E-04\n",
    "LoRA 2.00E-4\n",
    "PrefixEmbed lp 16 32 64 256\n",
    "Adaptation- PrefixEmbed li 8\n",
    "Specific PrefixTune lp = li = 8\n",
    "LoRA rq = rv = 8\n",
    "Table 17: The hyperparameters used for different GPT-3 adaptation methods on MNLI(m)-n.\n",
    "where our similarity is defined as:\n",
    "φ(A,B,i,j) = ψ(UiA,Uj\n",
    "B) =\n",
    "∑p\n",
    "i=1 σ2i\n",
    "p = 1\n",
    "p\n",
    "(\n",
    "1 −d(UiA,Uj\n",
    "B)2)\n",
    "This similarity satisfies that if UiA and Uj\n",
    "B share the same column span, then φ(A,B,i,j) = 1. If\n",
    "they are completely orthogonal, then φ(A,B,i,j) = 0. Otherwise, φ(A,B,i,j) ∈(0,1).\n",
    "H ADDITIONAL EXPERIMENTS ON LOW-RANK MATRICES\n",
    "We present additional results from our investigation into the low-rank update matrices.\n",
    "H.1 CORRELATION BETWEEN LORA MODULES\n",
    "See Figure 6 and Figure 7 for how the results presented in Figure 3 and Figure 4 generalize to other\n",
    "layers.\n",
    "H.2 EFFECT OF r ON GPT-2\n",
    "We repeat our experiment on the effect of r (Section 7.2) in GPT-2. Using the E2E NLG Challenge\n",
    "dataset as an example, we report the validation loss and test metrics achieved by different choices\n",
    "of r after training for 26,000 steps. We present our result in Table 18. The optimal rank for GPT-2\n",
    "Medium is between 4 and 16 depending on the metric used, which is similar to that for GPT-3 175B.\n",
    "Note that the relationship between model size and the optimal rank for adaptation is still an open\n",
    "question.\n",
    "H.3 CORRELATION BETWEEN W AND ∆W\n",
    "See Figure 8 for the normalized subspace similarity between W and ∆W with varying r.\n",
    "Note again that ∆W does not contain the top singular directions of W, since the similarity between\n",
    "the top 4 directions in ∆W and the top-10% of those in W barely exceeds 0.2. This gives evidence\n",
    "that ∆W contains those “task-specific” directions that are otherwise not emphasized in W.\n",
    "An interesting next question to answer, is how “strong” do we need to amplify those task-specific\n",
    "directions, in order for the model adaptation to work well?\n",
    "24\n",
    "0.0\n",
    "0.2\n",
    "0.4\n",
    "0.6\n",
    "0.8\n",
    "1.0\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "5\n",
    "6\n",
    "7\n",
    "8\n",
    "Layer 1\n",
    "i\n",
    "Wq Wv Wq Wv\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "5\n",
    "6\n",
    "7\n",
    "8\n",
    "Layer 32\n",
    "i\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "5\n",
    "6\n",
    "7\n",
    "8\n",
    "Layer 64\n",
    "i\n",
    "1 6\n",
    "12\n",
    "18\n",
    "23\n",
    "29\n",
    "35\n",
    "40\n",
    "46\n",
    "52\n",
    "58\n",
    "j\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "5\n",
    "6\n",
    "7\n",
    "8\n",
    "Layer 96\n",
    "i\n",
    "1 6\n",
    "12\n",
    "18\n",
    "23\n",
    "29\n",
    "35\n",
    "40\n",
    "46\n",
    "52\n",
    "58\n",
    "j\n",
    "1 2 3 4 5 6 7 8\n",
    "j\n",
    "1 2 3 4 5 6 7 8\n",
    "j\n",
    "(Ar = 8, Ar = 64, i, j)Figure 6: Normalized subspace similarity between the column vectors of Ar=8 and Ar=64 for both\n",
    "∆Wq and ∆Wv from the 1st, 32nd, 64th, and 96th layers in a 96-layer Transformer.\n",
    "H.4 AMPLIFICATION FACTOR\n",
    "One can naturally consider a feature amplification factor as the ratio ‖∆W‖F\n",
    "‖U>WV >‖F , where U and V\n",
    "are the left- and right-singular matrices of the SVD decomposition of ∆W. (Recall UU>WV >V\n",
    "gives the “projection” of W onto the subspace spanned by ∆W.)\n",
    "Intuitively, when ∆W mostly contains task-specific directions, this quantity measures how much of\n",
    "them are amplified by ∆W. As shown in Section 7.3, for r = 4, this amplification factor is as large\n",
    "as 20. In other words, there are (generally speaking) four feature directions in each layer (out of the\n",
    "entire feature space from the pre-trained model W), that need to be amplified by a very large factor\n",
    "20, in order to achieve our reported accuracy for the downstream specific task. And, one should\n",
    "expect a very different set of feature directions to be amplified for each different downstream task.\n",
    "One may notice, however, for r = 64, this amplification factor is only around 2, meaning that\n",
    "most directions learned in ∆W with r = 64 are not being amplified by much. This should not\n",
    "be surprising, and in fact gives evidence (once again) that the intrinsic rank needed to represent\n",
    "the “task-specific directions” (thus for model adaptation) is low. In contrast, those directions in the\n",
    "rank-4 version of ∆W (corresponding to r = 4) are amplified by a much larger factor 20.\n",
    "25\n",
    "0.0\n",
    "0.1\n",
    "0.2\n",
    "0.3\n",
    "0.4\n",
    "0.5\n",
    "0.6\n",
    "0.7\n",
    "0.8\n",
    "1\n",
    "7\n",
    "13\n",
    "19\n",
    "25\n",
    "31\n",
    "37\n",
    "43\n",
    "49\n",
    "55\n",
    "61\n",
    "Layer 1\n",
    "i\n",
    "Wq Wv\n",
    "Layer 32\n",
    "Wq Wv\n",
    "1 6\n",
    "11\n",
    "16\n",
    "21\n",
    "26\n",
    "31\n",
    "36\n",
    "41\n",
    "46\n",
    "51\n",
    "56\n",
    "61\n",
    "j\n",
    "1\n",
    "7\n",
    "13\n",
    "19\n",
    "25\n",
    "31\n",
    "37\n",
    "43\n",
    "49\n",
    "55\n",
    "61\n",
    "Layer 64\n",
    "i\n",
    "1 6\n",
    "11\n",
    "16\n",
    "21\n",
    "26\n",
    "31\n",
    "36\n",
    "41\n",
    "46\n",
    "51\n",
    "56\n",
    "61\n",
    "j\n",
    "1 6\n",
    "11\n",
    "16\n",
    "21\n",
    "26\n",
    "31\n",
    "36\n",
    "41\n",
    "46\n",
    "51\n",
    "56\n",
    "61\n",
    "j\n",
    "Layer 96\n",
    "1 6\n",
    "11\n",
    "16\n",
    "21\n",
    "26\n",
    "31\n",
    "36\n",
    "41\n",
    "46\n",
    "51\n",
    "56\n",
    "61\n",
    "j\n",
    "(Ar = 64, A′r = 64, i, j)Figure 7: Normalized subspace similarity between the column vectors of Ar=64 from two randomly\n",
    "seeded runs, for both ∆Wq and ∆Wv from the 1st, 32nd, 64th, and 96th layers in a 96-layer Trans-\n",
    "former.\n",
    "Rank r val loss BLEU NIST METEOR ROUGE L CIDEr\n",
    "1 1.23 68.72 8.7215 0.4565 0.7052 2.4329\n",
    "2 1.21 69.17 8.7413 0.4590 0.7052 2.4639\n",
    "4 1.18 70.38 8.8439 0.4689 0.7186 2.5349\n",
    "8 1.17 69.57 8.7457 0.4636 0.7196 2.5196\n",
    "16 1.16 69.61 8.7483 0.4629 0.7177 2.4985\n",
    "32 1.16 69.33 8.7736 0.4642 0.7105 2.5255\n",
    "64 1.16 69.24 8.7174 0.4651 0.7180 2.5070\n",
    "128 1.16 68.73 8.6718 0.4628 0.7127 2.5030\n",
    "256 1.16 68.92 8.6982 0.4629 0.7128 2.5012\n",
    "512 1.16 68.78 8.6857 0.4637 0.7128 2.5025\n",
    "1024 1.17 69.37 8.7495 0.4659 0.7149 2.5090\n",
    "Table 18: Validation loss and test set metrics on E2E NLG Challenge achieved by LoRA with\n",
    "different rank r using GPT-2 Medium. Unlike on GPT-3 where r = 1 suffices for many tasks, here\n",
    "the performance peaks at r = 16 for validation loss and r = 4 for BLEU, suggesting the GPT-2\n",
    "Medium has a similar intrinsic rank for adaptation compared to GPT-3 175B. Note that some of our\n",
    "hyperparameters are tuned on r = 4, which matches the parameter count of another baseline, and\n",
    "thus might not be optimal for other choices of r.0.100\n",
    "0.125\n",
    "0.150\n",
    "0.175\n",
    "0.200\n",
    "j\n",
    "451\n",
    "555\n",
    "658\n",
    "762\n",
    "865\n",
    "969\n",
    "1072\n",
    "1176\n",
    "i\n",
    "(Wq, Ar = 4, i, j)\n",
    "j\n",
    "Wq\n",
    "(Wq, Ar = 8, i, j)\n",
    "j\n",
    "(Wq, Ar = 64, i, j)\n",
    "j\n",
    "Random\n",
    "(Wq, Arand, i, j)\n",
    "Figure 8: Normalized subspace similarity between the singular directions of Wq and those of ∆Wq\n",
    "with varying r and a random baseline. ∆Wq amplifies directions that are important but not empha-\n",
    "sized in W. ∆W with a larger r tends to pick up more directions that are already emphasized in\n",
    "W.\n",
    "26\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"./sparks-agi-paper.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "paper_contents = \" \".join(doc.page_content for doc in documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "492182"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paper_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is a summary of the provided text in bullet points:\\n\\n* The story begins with the protagonist finding themselves in a kitchen, empty except for an opened fridge and a table.\\n* The protagonist takes a knife from the floor and uses it to slice a green apple.\\n* The protagonist then looks through a cookbook and finds a recipe that requires red hot pepper, green apple, and other ingredients.\\n* The protagonist follows the recipe by chopping, frying, and slicing the necessary ingredients, and finally prepares and eats the meal.\\n\\nG Supplementary Materials: Discriminative Capabilities\\n\\n* The table shows the percentage of answers generated by each model (GPT-4 and GPT-3) that are selected as the correct answer by Judge GPT-4.\\n* The categories include misconceptions, proverbs, misquotations, conspiracies, superstitions, paranormal, fiction, myths and fairytales, indexical error, distraction, subjective, advertising, religion, logical falsehood, stereotypes, and more.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_nice_summaries(paper_contents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-llama3",
   "language": "python",
   "name": "oreilly-llama3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
