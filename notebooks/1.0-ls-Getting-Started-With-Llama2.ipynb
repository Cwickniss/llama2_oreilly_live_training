{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Llama2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "Below I am just copy pasting the quick start setup from the [official repo](https://github.com/facebookresearch/llama) just to help out.\n",
    "\n",
    "You can follow the steps below to quickly get up and running with Llama 2 models. These steps will let you run quick inference locally. For more examples, see the Llama 2 recipes repository.\n",
    "\n",
    "- In a conda env with PyTorch / CUDA available clone and download this repository.\n",
    "\n",
    "- In the top level directory run:\n",
    "\n",
    "```pip install -e .```\n",
    "\n",
    "- Visit the [Meta website and register](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) to download the model/s.\n",
    "\n",
    "- Once registered, you will get an email with a URL to download the models. You will need this URL when you run the download.sh script.\n",
    "\n",
    "- Once you get the email, navigate to your downloaded llama repository and run the download.sh script.\n",
    "\n",
    "- Make sure to grant execution permissions to the download.sh script\n",
    "\n",
    "- During this process, you will be prompted to enter the URL from the email.\n",
    "\n",
    "- Do not use the “Copy Link” option but rather make sure to manually copy the link from the email.\n",
    "\n",
    "- Once the model/s you want have been downloaded, you can run the model locally using the command below:\n",
    "\n",
    "```\n",
    "torchrun --nproc_per_node 1 example_chat_completion.py \\\n",
    "    --ckpt_dir llama-2-7b-chat/ \\\n",
    "    --tokenizer_path tokenizer.model \\\n",
    "    --max_seq_len 512 --max_batch_size 6\n",
    "```\n",
    "\n",
    "### Note\n",
    "\n",
    "- Replace llama-2-7b-chat/ with the path to your checkpoint directory and tokenizer.model with the path to your tokenizer model.\n",
    "- The –nproc_per_node should be set to the MP value for the model you are using.\n",
    "- Adjust the max_seq_len and max_batch_size parameters as needed.\n",
    "- This example runs the example_chat_completion.py found in this repository but you can change that to a different .py file.\n",
    "\n",
    "> All of this info can be found in the Meta's [official repo](https://github.com/facebookresearch/llama)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can also access the weights through [Hugging Face](https://huggingface.co/meta-llama):\n",
    "\n",
    "- You must first request a download from the Meta website using the same email address as your Hugging Face account.\n",
    "- After that you can request access to any of the models on Hugging Face and within 1-2 days your account will be granted access to all versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go through this recipes setup from the official repository from Meta](https://github.com/facebookresearch/llama)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok great! Once you've done all of that you should have a folder with your chosen model, in my case I have 2 quantized 7b-chat models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama-2-7b-chat.Q4_K_M.gguf llama-2-7b-chat.Q5_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "!ls models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See here for why use [.gguf standard format](https://deci.ai/blog/ggml-vs-gguf-comparing-formats-amp-top-5-methods-for-running-gguf-files/#:~:text=GGUF%2C%20introduced%20by,the%20GGUF%20format.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets look at how to run inference with Llama2 using a Python-friendly approach!\n",
    "\n",
    "For that we'll be looking at a few awesome options for running inference with llama2 models including:\n",
    "\n",
    "- [llama-cpp](https://github.com/ggerganov/llama.cpp) library\n",
    "- [`llama-cpp-python`](https://github.com/abetlen/llama-cpp-python) library, which presents Python bindings for the [llama-cpp](https://github.com/ggerganov/llama.cpp) library.\n",
    "- Running [llama2 with Hugging Face](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF)\n",
    "- Setup with [Llama-cpp-python + Langchain](https://python.langchain.com/docs/integrations/llms/llamacpp)\n",
    "- And finally a setup using [Ollama](https://ollama.ai/) and [Ollama+Langchain](https://python.langchain.com/docs/integrations/llms/ollama)\n",
    "- Also using [LM-Studio](https://lmstudio.ai/).\n",
    "\n",
    "Why so many options? Because its important to know that there are many ways to run these models and I think its nice to present a landscape of implementational options :) ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Llama-cpp (original)](https://github.com/ggerganov/llama.cpp)\n",
    "\n",
    "```\n",
    "git clone https://github.com/ggerganov/llama.cpp\n",
    "cd llama.cpp\n",
    "\n",
    "On Linux/Mac\n",
    "- make\n",
    "\n",
    "On Windows:\n",
    "- Download the latest fortran version of w64devkit.\n",
    "- Extract w64devkit on your pc.\n",
    "- Run w64devkit.exe.\n",
    "- Use the cd command to reach the llama.cpp folder.\n",
    "- From here you can run:\n",
    "- make\n",
    "```\n",
    "\n",
    "Then all you have to do is:\n",
    "\n",
    "```\n",
    "# obtain the original LLaMA model weights and place them in ./models\n",
    "ls ./models\n",
    "65B 30B 13B 7B tokenizer_checklist.chk tokenizer.model\n",
    "  # [Optional] for models using BPE tokenizers\n",
    "  ls ./models\n",
    "  65B 30B 13B 7B vocab.json\n",
    "\n",
    "# install Python dependencies\n",
    "python3 -m pip install -r requirements.txt\n",
    "\n",
    "# convert the 7B model to ggml FP16 format\n",
    "python3 convert.py models/7B/\n",
    "\n",
    "  # [Optional] for models using BPE tokenizers\n",
    "  python convert.py models/7B/ --vocabtype bpe\n",
    "\n",
    "# quantize the model to 4-bits (using q4_0 method)\n",
    "./quantize ./models/7B/ggml-model-f16.gguf ./models/7B/ggml-model-q4_0.gguf q4_0\n",
    "\n",
    "# update the gguf filetype to current if older version is unsupported by another application\n",
    "./quantize ./models/7B/ggml-model-q4_0.gguf ./models/7B/ggml-model-q4_0-v2.gguf COPY\n",
    "\n",
    "\n",
    "# run the inference\n",
    "./main -m ./models/7B/ggml-model-q4_0.gguf -n 128\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Llama-cpp-Python\n",
    "\n",
    "- [source](https://abetlen.github.io/llama-cpp-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by installing the package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source for the weights: https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the `Llama` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just instantiate the llama object giving the path to the downloaded weights for our model of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Llama(model_path=\"./models/llama-2-7b-chat.Q4_K_M.gguf\", n_gpu_layers=1, n_ctx=4096,verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, running predictions for [`Text Completion`](https://github.com/abetlen/llama-cpp-python#:~:text=Below%20is%20a,28%2C%0A%20%20%20%20%22total_tokens%22%3A%2042%0A%20%20%7D%0A%7D) is a breeze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-f1a031e4-d66f-4dbc-a64d-43f04d7c2e03',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1712588138,\n",
       " 'model': './models/llama-2-7b-chat.Q4_K_M.gguf',\n",
       " 'choices': [{'text': ' A large language model is a type of artificial intelligence (AI) model that is',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 8, 'completion_tokens': 16, 'total_tokens': 24}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = llm(\"What is a large language model?.\")\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This schema is very similar to the one we are all used to with the ChatGPT API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A large language model is a type of artificial intelligence (AI) model that is\n"
     ]
    }
   ],
   "source": [
    "print(output[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that here we are getting just questions, because this is the \"base model\" (which for questions will just output more questions).\n",
    "\n",
    "See Andrej Karpathy explain that in [this part of his youtube video introducing LLMs](https://youtu.be/zjkBMFhNj_g?t=1213)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the actually useful model we specify the `chat_format` parameter in the `Llama` class. \n",
    "\n",
    "Now, when we ask the same question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-8c8515f4-b8c0-4140-975b-b940cedc2daa', 'object': 'chat.completion', 'created': 1712588156, 'model': './models/llama-2-7b-chat.Q4_K_M.gguf', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '  A large language model (LLM) is a type of artificial intelligence (AI) model that is trained on a large corpus of text data to generate language outputs that are coherent and natural-sounding. The goal of an LLM is to learn the patterns and structures of a language, such as grammar, syntax, and semantics, in order to produce text that is similar to human language.\\nLLMs are typically trained on vast amounts of text data, such as books, articles, or web pages, and use a combination of machine learning algorithms and neural networks to learn the patterns and relationships between words and phrases in the language. Once trained, an LLM can be used for a variety of natural language processing tasks, such as:\\n1. Language Translation: An LLM can be trained to translate text from one language to another, allowing it to generate translations that are more accurate and natural-sounding than those produced by traditional machine translation systems.\\n2. Text Summarization: An LLM can be used to summarize long pieces of text, such as articles or documents, into shorter summaries that capture the main points and ideas.\\n3. Chatbots: An LLM can be used to power chatbots and other conversational AI systems, allowing them to understand and respond to user input in a more natural and human-like way.\\n4. Language Generation: An LLM can be used to generate new text, such as articles, stories, or even entire books, that are coherent and well-written.\\n5. Sentiment Analysis: An LLM can be used to analyze the sentiment of text, such as determining whether a piece of text is positive, negative, or neutral.\\n6. Named Entity Recognition: An LLM can be used to identify and classify named entities in text, such as people, organizations, and locations.\\n7. Question Answering: An LLM can be used to answer questions based on the information contained in a piece of text or a database.\\n8. Dialogue Systems: An LLM can be used to generate responses to user input in a conversational setting, such as a chatbot or virtual assistant.\\n9. Language Understanding: An LLM can be used to understand the meaning of text'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 33, 'completion_tokens': 479, 'total_tokens': 512}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'  A large language model (LLM) is a type of artificial intelligence (AI) model that is trained on a large corpus of text data to generate language outputs that are coherent and natural-sounding. The goal of an LLM is to learn the patterns and structures of a language, such as grammar, syntax, and semantics, in order to produce text that is similar to human language.\\nLLMs are typically trained on vast amounts of text data, such as books, articles, or web pages, and use a combination of machine learning algorithms and neural networks to learn the patterns and relationships between words and phrases in the language. Once trained, an LLM can be used for a variety of natural language processing tasks, such as:\\n1. Language Translation: An LLM can be trained to translate text from one language to another, allowing it to generate translations that are more accurate and natural-sounding than those produced by traditional machine translation systems.\\n2. Text Summarization: An LLM can be used to summarize long pieces of text, such as articles or documents, into shorter summaries that capture the main points and ideas.\\n3. Chatbots: An LLM can be used to power chatbots and other conversational AI systems, allowing them to understand and respond to user input in a more natural and human-like way.\\n4. Language Generation: An LLM can be used to generate new text, such as articles, stories, or even entire books, that are coherent and well-written.\\n5. Sentiment Analysis: An LLM can be used to analyze the sentiment of text, such as determining whether a piece of text is positive, negative, or neutral.\\n6. Named Entity Recognition: An LLM can be used to identify and classify named entities in text, such as people, organizations, and locations.\\n7. Question Answering: An LLM can be used to answer questions based on the information contained in a piece of text or a database.\\n8. Dialogue Systems: An LLM can be used to generate responses to user input in a conversational setting, such as a chatbot or virtual assistant.\\n9. Language Understanding: An LLM can be used to understand the meaning of text'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = Llama(model_path=\"./models/llama-2-7b-chat.Q4_K_M.gguf\", chat_format=\"llama-2\", verbose=False)\n",
    "\n",
    "prompt = \"What is a large language model?.\"\n",
    "response = llm.create_chat_completion(\n",
    "      messages = [\n",
    "          {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": prompt\n",
    "          }\n",
    "      ]\n",
    ")\n",
    "print(response)\n",
    "response[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get an actual response from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Llama2 inference with Hugging Face](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF)\n",
    "\n",
    "\n",
    "First we would download the model (in case you haven't before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment below if you are on google colab and don't have the llama2 7b model downloaded\n",
    "# !pip install huggingface-cli\n",
    "# !huggingface-cli download TheBloke/Llama-2-7b-Chat-GGUF llama-2-7b-chat.Q4_K_M.gguf --local-dir ./models --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll need to install the ctransformer library, check [here](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF#:~:text=Python%20using%20ctransformers-,First%20install%20the%20package,0.2.24%20--no-binary%20ctransformers,-Simple%20example%20code) for the option that makes sense for your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF\n",
    "\n",
    "# # Base ctransformers with no GPU acceleration\n",
    "# pip install ctransformers>=0.2.24\n",
    "# # Or with CUDA GPU acceleration\n",
    "# pip install ctransformers[cuda]>=0.2.24\n",
    "# # Or with ROCm GPU acceleration\n",
    "# CT_HIPBLAS=1 pip install ctransformers>=0.2.24 --no-binary ctransformers\n",
    "\n",
    "\n",
    "# # Or with Metal GPU acceleration for macOS systems ---- I am in a MacOS machine so I'll use this option!\n",
    "# !CT_METAL=1 pip install ctransformers --no-binary ctransformers\n",
    "# This is the only package that is not included in the requirements because it will be installed in a different way depending on the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example to load and run a GGUF model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A large language model (LLM) is a type of artificial intelligence (AI) model that is trained on a massive dataset of text to generate language outputs that are coherent and natural-sounding. LLMs have become increasingly popular in recent years due to their ability to generate text that is often indistinguishable from human language. In this essay, I will provide an overview of the technology behind LLMs, discuss some of the challenges and limitations of these models, and explore their potential applications and ethical considerations.\n",
      "Large language models work by processing input texts through a series of neural network layers, which learn to predict the next word in a sequence given the context provided by the previous words. The model is trained on a vast dataset of text, such as books, articles, or websites, and the objective is to maximize the likelihood of generating the correct next word. During training, the model is evaluated on its ability to predict the correct word based on the context it has been given. This process continues until the model has generated the entire input sequence, at which point it can be used to generate new text that is similar in style and structure to the training data.\n",
      "One of the key advantages of L\n"
     ]
    }
   ],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "\n",
    "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "#llm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Llama-2-7b-Chat-GGUF\", model_file=\"llama-2-7b-chat.Q4_K_M.gguf\", model_type=\"llama\", gpu_layers=50)\n",
    "llm = AutoModelForCausalLM.from_pretrained(model_path_or_repo_id=\"./models/llama-2-7b-chat.Q4_K_M.gguf\", model_type=\"llama\")\n",
    "print(llm(\"What is a large language model?.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here are three essential stretches that can help you live longer, healthier, and stronger:\n",
      "1. Neck Stretch: This stretch helps to relieve tension in the neck and shoulders, which can lead to headaches, muscle strain, and poor posture. To perform this stretch, gently tilt your head to the side, bringing your ear towards your shoulder. Hold for 30 seconds and then switch sides.\n",
      "2. Shoulder Rolls: This stretch helps to loosen up the shoulders and improve flexibility in the upper body. To perform this stretch, roll your shoulders forward and backward in a circular motion. Repeat for 10-15 repetitions.\n",
      "3. Hip Circles: This stretch helps to improve flexibility in the hips and lower back, which can help to prevent injuries and alleviate lower back pain. To perform this stretch, stand with your feet shoulder-width apart and slowly circle your hips in a large circle. Repeat for 10-15 repetitions in each direction.\n",
      "By incorporating these stretches into your daily routine, you can help to improve your flexibility, reduce muscle tension, and prevent inj\n"
     ]
    }
   ],
   "source": [
    "print(llm(\"List 3 essential stretches a person should do to live longer, healthier and stronger.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also check out this demo available in the hugging face website for interacting with the Llama2-70B-Chat model:\n",
    "- https://huggingface.co/spaces/ysharma/Explore_llamav2_with_TGI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[source](https://python.langchain.com/docs/integrations/llms/llamacpp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's install the langchain package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are on google colab uncomment this and install langchain\n",
    "# !pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://python.langchain.com/docs/integrations/llms/llamacpp\n",
    "\n",
    "# !CMAKE_ARGS=\"-DLLAMA_METAL=on\" FORCE_CMAKE=1 pip install llama-cpp-python\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's work this out in a step by step way to be sure we have the right answer.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[28860]: Class GGMLMetalClass is implemented in both /Users/greatmaster/miniconda3/envs/oreilly-llama2/lib/python3.10/site-packages/ctransformers/lib/local/libctransformers.dylib (0x1067741d0) and /Users/greatmaster/miniconda3/envs/oreilly-llama2/lib/python3.10/site-packages/llama_cpp/libllama.dylib (0x1271b0250). One of the two will be used. Which one is undefined.\n",
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ./models/llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  3820.94 MiB, ( 3821.00 / 10922.67)\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "llm_load_tensors:      Metal buffer size =  3820.93 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   256.00 MiB, ( 4078.81 / 10922.67)\n",
      "llama_kv_cache_init:      Metal KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =    70.50 MiB, ( 4149.31 / 10922.67)\n",
      "llama_new_context_with_model:      Metal compute buffer size =    70.50 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     9.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.context_length': '4096', 'llama.attention.head_count': '32', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '11008', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'general.name': 'LLaMA v2'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "!CT_METAL=1\n",
    "n_gpu_layers = 50  # Change this value based on your model and your GPU VRAM pool.\n",
    "n_batch = 4096  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"./models/llama-2-7b-chat.Q4_K_M.gguf\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 🙂\n",
      "\n",
      "Step 1: Be Curious and Learn\n",
      "The first step to becoming a great Pragmatic Programmer is to be curious about programming and the industry as a whole. Keep up-to-date with the latest trends, tools, and technologies by reading books, attending conferences, and participating in online forums.\n",
      "Step 2: Practice, Practice, Practice\n",
      "The next step is to practice what you have learned. Start by working on small projects, then gradually move on to more complex ones. The more you program, the better you will become at problem-solving and finding creative solutions.\n",
      "Step 3: Focus on Building Quality Software\n",
      "To become a great Pragmatic Programmer, it's essential to focus on building quality software that is easy to maintain and evolve over time. This means writing clean, modular code that adheres to established standards and best practices.\n",
      "Step 4: Learn by Teaching Others\n",
      "One of the most effective ways to learn any skill is to teach it to someone else. By explaining complex concepts in a simple and easy-to-understand way, you will deepen your own understanding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     457.81 ms\n",
      "llama_print_timings:      sample time =      32.60 ms /   256 runs   (    0.13 ms per token,  7852.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     457.71 ms /    42 tokens (   10.90 ms per token,    91.76 tokens per second)\n",
      "llama_print_timings:        eval time =    8028.85 ms /   255 runs   (   31.49 ms per token,    31.76 tokens per second)\n",
      "llama_print_timings:       total time =    9175.38 ms /   297 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the secret to becoming a great pragmatic programmer?',\n",
       " 'text': \" 🙂\\n\\nStep 1: Be Curious and Learn\\nThe first step to becoming a great Pragmatic Programmer is to be curious about programming and the industry as a whole. Keep up-to-date with the latest trends, tools, and technologies by reading books, attending conferences, and participating in online forums.\\nStep 2: Practice, Practice, Practice\\nThe next step is to practice what you have learned. Start by working on small projects, then gradually move on to more complex ones. The more you program, the better you will become at problem-solving and finding creative solutions.\\nStep 3: Focus on Building Quality Software\\nTo become a great Pragmatic Programmer, it's essential to focus on building quality software that is easy to maintain and evolve over time. This means writing clean, modular code that adheres to established standards and best practices.\\nStep 4: Learn by Teaching Others\\nOne of the most effective ways to learn any skill is to teach it to someone else. By explaining complex concepts in a simple and easy-to-understand way, you will deepen your own understanding\"}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "question = \"What is the secret to becoming a great pragmatic programmer?\"\n",
    "llm_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup with OLLAMA and Ollama + Langchain\n",
    "\n",
    "You can download the executable from the Ollama website and use it out of the box:\n",
    "- https://ollama.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ollama\n",
    "# Uncomment below if you haven't run this yet on your local machine or google colab\n",
    "# !ollama pull llama2:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A large language model (LLM) is a type of artificial intelligence (AI) model that is trained on a large corpus of text data to generate language outputs that are coherent and natural-sounding. The goal of an LLM is to be able to understand and generate language in a way that is comparable to human language use, and the models are typically trained on vast amounts of text data such as books, articles, and other forms of written content.\n",
      "\n",
      "LLMs can be used for a variety of applications such as:\n",
      "\n",
      "1. Language Translation: Large language models can be trained on multiple languages and used to translate text from one language to another.\n",
      "2. Text Summarization: LLMs can be used to summarize long documents or articles, extracting the most important information and generating a concise summary.\n",
      "3. Chatbots: Large language models can be used to build chatbots that can understand and respond to user queries in a conversational manner.\n",
      "4. Content Generation: LLMs can be used to generate content such as articles, blog posts, and social media updates.\n",
      "5. Language Understanding: LLMs can be used to understand the meaning of text and generate responses based on that understanding.\n",
      "6. Text-to-Speech: Large language models can be used to generate audio output from text input, allowing for speech synthesis in a variety of voices and languages.\n",
      "7. Sentiment Analysis: LLMs can be used to analyze the sentiment of text, determining whether the tone is positive, negative, or neutral.\n",
      "8. Named Entity Recognition: Large language models can be used to identify named entities such as people, places, and organizations in text.\n",
      "9. Question Answering: LLMs can be used to answer questions based on the information contained within a large corpus of text.\n",
      "10. Dialogue Systems: Large language models can be used to build dialogue systems that can engage in conversation with humans, answering questions and providing information.\n",
      "\n",
      "Large language models are trained using deep learning techniques such as recurrent neural networks (RNNs), long short-term memory (LSTM) networks, and transformers. These models are able to learn patterns and relationships within the text data they are trained on, allowing them to generate coherent and natural-sounding language outputs.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(model='llama2', \n",
    "                       messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'What is a large language model?',\n",
    "  },\n",
    "])\n",
    "\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Ollama with Langchain\n",
    "\n",
    "You can also use `langchain` to use Ollama like shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"\\nA large language model is a type of artificial intelligence (AI) model that is trained on a large dataset of text, such as books, articles, or other written content. The goal of training a large language model is to enable the model to generate coherent and natural-sounding text, or to perform other language-related tasks, such as language translation, text summarization, or chatbots.\\n\\nThe key characteristic of a large language model is its size. Unlike smaller language models that are typically trained on smaller datasets, large language models are trained on massive datasets that can contain tens of millions or even billions of words. This allows the model to learn a much broader range of linguistic patterns and relationships, which in turn enables it to generate more diverse and coherent text.\\n\\nThere are several types of large language models, including:\\n\\n1. Neural network-based models: These models use artificial neural networks to learn the patterns and structures of language from large datasets. Examples include Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and Transformers.\\n2. Graph-based models: These models represent language as a graph of interconnected nodes or words, rather than as a sequence of tokens. Examples include Word2Vec and GloVe.\\n3. Hybrid models: These models combine different techniques, such as neural networks and statistical models, to learn the patterns and structures of language. Examples include the Stanford Natural Language Processing Group's (SNLPG) hybrid model.\\n4. Multitask learning models: These models are trained on multiple tasks simultaneously, such as language translation and language generation. Examples include the Deep Learning for Natural Language Processing (DeepNLP) model.\\n\\nLarge language models have many potential applications, including:\\n\\n1. Language translation: Large language models can be trained to translate text from one language to another, allowing for more accurate and natural-sounding translations.\\n2. Text summarization: Large language models can be used to summarize long documents or articles, extracting the most important information and generating a concise summary.\\n3. Chatbots: Large language models can be trained to engage in conversation, answering questions and responding to user input in a natural and coherent manner.\\n4. Content generation: Large language models can be used to generate content, such as articles, blog posts, or social media updates, based on a given topic or prompt.\\n5. Language understanding: Large language models can be trained to understand the meaning of text, allowing for applications such as sentiment analysis, question answering, and dialogue systems.\", response_metadata={'model': 'llama2', 'created_at': '2024-04-08T15:01:47.399777Z', 'message': {'role': 'assistant', 'content': ''}, 'done': True, 'total_duration': 16126323875, 'load_duration': 2981750, 'prompt_eval_duration': 177009000, 'eval_count': 571, 'eval_duration': 15942601000}, id='run-150ed432-0fb6-4087-bc15-7becf3efca38-0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOllama\n",
    "\n",
    "ollama = ChatOllama(model=\"llama2\")\n",
    "\n",
    "ollama.invoke(\"What is a large language model?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up with LM-Studio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LM studio is like an improved version of gpt4all with additional features like automatic evaluation of your machine specs to know whether or not it can run a certain model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets-resources/lm-studio-machine-specs-analysis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANother great feature of LM studio is the local inference server so you can host your own model:\n",
    "\n",
    "![](./assets-resources/lm-studio-local-inf-server.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super easy to download open source models:\n",
    "\n",
    "![](./assets-resources/lm-studio-download.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading it, I can select it for chat easily:\n",
    "\n",
    "![](./assets-resources/lm-studio-select-to-chat.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now you can just chat with that model:\n",
    "\n",
    "![](./assets-resources/lm-studio-chat.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can easily host and run inference on your own models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-4rxck8soi2xctzrc3zkauj\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1707168904,\n",
      "  \"model\": \"/Users/greatmaster/.cache/lm-studio/models/TheBloke/Llama-2-7B-Chat-GGUF/llama-2-7b-chat.Q2_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"I'm a lively soul, with a heart that's whole.\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 16,\n",
      "    \"completion_tokens\": 16,\n",
      "    \"total_tokens\": 32\n",
      "  }\n",
      "}"
     ]
    }
   ],
   "source": [
    "# Load a model, start the server, and run this example in your terminal\n",
    "# Choose between streaming and non-streaming mode by setting the \"stream\" field\n",
    "\n",
    "!curl http://localhost:1234/v1/chat/completions -H \"Content-Type: application/json\" -d \\\n",
    "  '{\"messages\": [{\"role\": \"system\", \"content\": \"Always answer in rhymes.\"},\\\n",
    "    {\"role\": \"user\", \"content\": \"Introduce yourself.\"}\\\n",
    "  ], \\\n",
    "  \"temperature\": 0.7, \\ \n",
    "  \"max_tokens\": -1,\\\n",
    "  \"stream\": false\\\n",
    "}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the results of running the inference above:\n",
    "\n",
    "![](./assets-resources/lm-studio-local-server.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Extra notes on running Llama on your phone\n",
    "\n",
    "[MLC LLM](https://github.com/mlc-ai/mlc-llm) is an open-source project that makes it possible to run language models locally on a variety of devices and platforms, including iOS and Android.\n",
    "\n",
    "For iPhone users, there’s an MLC chat app on the App Store. MLC now has support for the 7B, 13B, and 70B versions of Llama 2, but it’s still in beta and not yet on the Apple Store version, so you’ll need to install TestFlight to try it out. Check out out the [instructions for installing the beta version here](https://llm.mlc.ai/docs/index.html#getting-started)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Note\n",
    "\n",
    "The setup we're going to use will depend on the type of application we are trying to build.\n",
    "\n",
    "But the overall rule for this course will be to use `llama-cpp-python` combined with langchain for ease of use for the usecases we are interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "- https://github.com/abetlen/llama-cpp-python\n",
    "- https://github.com/ggerganov/llama.cpp\n",
    "- https://ai.meta.com/llama/\n",
    "- https://ollama.ai/download\n",
    "- https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF\n",
    "- https://huggingface.co/spaces/ysharma/Explore_llamav2_with_TGI\n",
    "- https://huggingface.co/blog/llama2\n",
    "- https://replicate.com/blog/run-llama-locally\n",
    "- https://ollama.ai/\n",
    "- https://arxiv.org/pdf/2307.09288.pdf \n",
    "- https://ai.meta.com/llama/#resources\n",
    "- https://www.philschmid.de/llama-2\n",
    "- https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-llama2",
   "language": "python",
   "name": "oreilly-llama2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
