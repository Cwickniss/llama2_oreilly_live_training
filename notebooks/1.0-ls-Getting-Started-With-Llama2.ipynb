{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Llama2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "Below I am just copy pasting the quick start setup from the [official repo](https://github.com/facebookresearch/llama) just to help out.\n",
    "\n",
    "You can follow the steps below to quickly get up and running with Llama 2 models. These steps will let you run quick inference locally. For more examples, see the Llama 2 recipes repository.\n",
    "\n",
    "- In a conda env with PyTorch / CUDA available clone and download this repository.\n",
    "\n",
    "- In the top level directory run:\n",
    "\n",
    "```pip install -e .```\n",
    "\n",
    "- Visit the [Meta website and register](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) to download the model/s.\n",
    "\n",
    "- Once registered, you will get an email with a URL to download the models. You will need this URL when you run the download.sh script.\n",
    "\n",
    "- Once you get the email, navigate to your downloaded llama repository and run the download.sh script.\n",
    "\n",
    "- Make sure to grant execution permissions to the download.sh script\n",
    "\n",
    "- During this process, you will be prompted to enter the URL from the email.\n",
    "\n",
    "- Do not use the “Copy Link” option but rather make sure to manually copy the link from the email.\n",
    "\n",
    "- Once the model/s you want have been downloaded, you can run the model locally using the command below:\n",
    "\n",
    "```\n",
    "torchrun --nproc_per_node 1 example_chat_completion.py \\\n",
    "    --ckpt_dir llama-2-7b-chat/ \\\n",
    "    --tokenizer_path tokenizer.model \\\n",
    "    --max_seq_len 512 --max_batch_size 6\n",
    "```\n",
    "\n",
    "### Note\n",
    "\n",
    "- Replace llama-2-7b-chat/ with the path to your checkpoint directory and tokenizer.model with the path to your tokenizer model.\n",
    "- The –nproc_per_node should be set to the MP value for the model you are using.\n",
    "- Adjust the max_seq_len and max_batch_size parameters as needed.\n",
    "- This example runs the example_chat_completion.py found in this repository but you can change that to a different .py file.\n",
    "\n",
    "> All of this info can be found in the Meta's [official repo](https://github.com/facebookresearch/llama)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can also access the weights through [Hugging Face](https://huggingface.co/meta-llama):\n",
    "\n",
    "- You must first request a download from the Meta website using the same email address as your Hugging Face account.\n",
    "- After that you can request access to any of the models on Hugging Face and within 1-2 days your account will be granted access to all versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go through this recipes setup from the official repository from Meta](https://github.com/facebookresearch/llama)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok great! Once you've done all of that you should have a folder with your chosen model, in my case I have 2 quantized 7b-chat models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama-2-7b-chat.Q4_K_M.gguf llama-2-7b-chat.Q5_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "!ls models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets look at how to run inference with Llama2 using a Python-friendly approach!\n",
    "\n",
    "For that we'll be looking at a few awesome options for running inference with llama2 models including:\n",
    "\n",
    "- [llama-cpp](https://github.com/ggerganov/llama.cpp) library\n",
    "- [`llama-cpp-python`](https://github.com/abetlen/llama-cpp-python) library, which presents Python bindings for the [llama-cpp](https://github.com/ggerganov/llama.cpp) library.\n",
    "- Running [llama2 with Hugging Face](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF)\n",
    "- Setup with [Llama-cpp-python + Langchain](https://python.langchain.com/docs/integrations/llms/llamacpp)\n",
    "- And finally a setup using [Ollama](https://ollama.ai/) and [Ollama+Langchain](https://python.langchain.com/docs/integrations/llms/ollama)\n",
    "- Also using [LM-Studio](https://lmstudio.ai/).\n",
    "\n",
    "Why so many options? Because its important to know that there are many ways to run these models and I think its nice to present a landscape of implementational options :) ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Llama-cpp (original)](https://github.com/ggerganov/llama.cpp)\n",
    "\n",
    "```\n",
    "git clone https://github.com/ggerganov/llama.cpp\n",
    "cd llama.cpp\n",
    "\n",
    "On Linux/Mac\n",
    "- make\n",
    "\n",
    "On Windows:\n",
    "- Download the latest fortran version of w64devkit.\n",
    "- Extract w64devkit on your pc.\n",
    "- Run w64devkit.exe.\n",
    "- Use the cd command to reach the llama.cpp folder.\n",
    "- From here you can run:\n",
    "- make\n",
    "```\n",
    "\n",
    "Then all you have to do is:\n",
    "\n",
    "```\n",
    "# obtain the original LLaMA model weights and place them in ./models\n",
    "ls ./models\n",
    "65B 30B 13B 7B tokenizer_checklist.chk tokenizer.model\n",
    "  # [Optional] for models using BPE tokenizers\n",
    "  ls ./models\n",
    "  65B 30B 13B 7B vocab.json\n",
    "\n",
    "# install Python dependencies\n",
    "python3 -m pip install -r requirements.txt\n",
    "\n",
    "# convert the 7B model to ggml FP16 format\n",
    "python3 convert.py models/7B/\n",
    "\n",
    "  # [Optional] for models using BPE tokenizers\n",
    "  python convert.py models/7B/ --vocabtype bpe\n",
    "\n",
    "# quantize the model to 4-bits (using q4_0 method)\n",
    "./quantize ./models/7B/ggml-model-f16.gguf ./models/7B/ggml-model-q4_0.gguf q4_0\n",
    "\n",
    "# update the gguf filetype to current if older version is unsupported by another application\n",
    "./quantize ./models/7B/ggml-model-q4_0.gguf ./models/7B/ggml-model-q4_0-v2.gguf COPY\n",
    "\n",
    "\n",
    "# run the inference\n",
    "./main -m ./models/7B/ggml-model-q4_0.gguf -n 128\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Llama-cpp-Python\n",
    "\n",
    "- [source](https://abetlen.github.io/llama-cpp-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by installing the package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source for the weights: https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the `Llama` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just instantiate the llama object giving the path to the downloaded weights for our model of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Llama(model_path=\"./models/llama-2-7b-chat.Q4_K_M.gguf\", n_gpu_layers=1, n_ctx=4096,verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, running predictions for [`Text Completion`](https://github.com/abetlen/llama-cpp-python#:~:text=Below%20is%20a,28%2C%0A%20%20%20%20%22total_tokens%22%3A%2042%0A%20%20%7D%0A%7D) is a breeze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-b359b746-9e97-40b3-a6c1-7f299014cdab',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1712418188,\n",
       " 'model': './models/llama-2-7b-chat.Q4_K_M.gguf',\n",
       " 'choices': [{'text': ' A large language model is a deep learning model that is trained on a vast amount',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 8, 'completion_tokens': 16, 'total_tokens': 24}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = llm(\"What is a large language model?.\")\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This schema is very similar to the one we are all used to with the ChatGPT API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A large language model is a deep learning model that is trained on a vast amount\n"
     ]
    }
   ],
   "source": [
    "print(output[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that here we are getting just questions, because this is the \"base model\" (which for questions will just output more questions).\n",
    "\n",
    "See Andrej Karpathy explain that in [this part of his youtube video introducing LLMs](https://youtu.be/zjkBMFhNj_g?t=1213)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the actually useful model we specify the `chat_format` parameter in the `Llama` class. \n",
    "\n",
    "Now, when we ask the same question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-3d711b28-21c4-4f11-8ef5-9e4167b527c2', 'object': 'chat.completion', 'created': 1712418276, 'model': './models/llama-2-7b-chat.Q4_K_M.gguf', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '  A large language model (LLM) is a type of artificial intelligence (AI) model that is trained on a large corpus of text data to generate language outputs that are coherent and natural-sounding. The goal of an LLM is to learn the patterns and structures of a language, such as grammar, syntax, and semantics, in order to produce text that is similar to human language.\\nLLMs are typically trained on vast amounts of text data, such as books, articles, or websites, and use a combination of machine learning algorithms and neural networks to learn the patterns and relationships between words and phrases in the language. Once trained, an LLM can be used for a variety of natural language processing tasks, such as:\\n1. Language translation: An LLM can be trained to translate text from one language to another.\\n2. Text summarization: An LLM can be used to summarize long pieces of text into shorter, more digestible versions.\\n3. Sentiment analysis: An LLM can be trained to analyze the sentiment of text, such as determining whether a piece of text is positive, negative, or neutral.\\n4. Chatbots: LLMs can be used to build chatbots that can understand and respond to user input in a natural language.\\n5. Language generation: An LLM can be used to generate new text that is similar to a given prompt or input.\\n6. Question answering: An LLM can be trained to answer questions based on the information contained in a large corpus of text.\\n7. Text classification: An LLM can be used to classify text into categories such as spam/not spam, positive/negative review, etc.\\n8. Dialogue systems: LLMs can be used to build dialogue systems that can engage in conversation with humans in a natural language.\\n9. Language understanding: An LLM can be trained to understand the meaning of text, including the relationships between words, phrases, and sentences.\\n10. Text completion: An LLM can be used to complete partially written text, such as filling in the blanks in a sentence or completing a paragraph.\\nLarge language models have many applications in areas such as customer service, language translation, content creation, and'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 33, 'completion_tokens': 479, 'total_tokens': 512}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'  A large language model (LLM) is a type of artificial intelligence (AI) model that is trained on a large corpus of text data to generate language outputs that are coherent and natural-sounding. The goal of an LLM is to learn the patterns and structures of a language, such as grammar, syntax, and semantics, in order to produce text that is similar to human language.\\nLLMs are typically trained on vast amounts of text data, such as books, articles, or websites, and use a combination of machine learning algorithms and neural networks to learn the patterns and relationships between words and phrases in the language. Once trained, an LLM can be used for a variety of natural language processing tasks, such as:\\n1. Language translation: An LLM can be trained to translate text from one language to another.\\n2. Text summarization: An LLM can be used to summarize long pieces of text into shorter, more digestible versions.\\n3. Sentiment analysis: An LLM can be trained to analyze the sentiment of text, such as determining whether a piece of text is positive, negative, or neutral.\\n4. Chatbots: LLMs can be used to build chatbots that can understand and respond to user input in a natural language.\\n5. Language generation: An LLM can be used to generate new text that is similar to a given prompt or input.\\n6. Question answering: An LLM can be trained to answer questions based on the information contained in a large corpus of text.\\n7. Text classification: An LLM can be used to classify text into categories such as spam/not spam, positive/negative review, etc.\\n8. Dialogue systems: LLMs can be used to build dialogue systems that can engage in conversation with humans in a natural language.\\n9. Language understanding: An LLM can be trained to understand the meaning of text, including the relationships between words, phrases, and sentences.\\n10. Text completion: An LLM can be used to complete partially written text, such as filling in the blanks in a sentence or completing a paragraph.\\nLarge language models have many applications in areas such as customer service, language translation, content creation, and'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = Llama(model_path=\"./models/llama-2-7b-chat.Q4_K_M.gguf\", chat_format=\"llama-2\", verbose=False)\n",
    "\n",
    "prompt = \"What is a large language model?.\"\n",
    "response = llm.create_chat_completion(\n",
    "      messages = [\n",
    "          {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": prompt\n",
    "          }\n",
    "      ]\n",
    ")\n",
    "print(response)\n",
    "response[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get an actual response from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Llama2 inference with Hugging Face](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF)\n",
    "\n",
    "\n",
    "First we would download the model (in case you haven't before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment below if you are on google colab and don't have the llama2 7b model downloaded\n",
    "# !pip install huggingface-cli\n",
    "# !huggingface-cli download TheBloke/Llama-2-7b-Chat-GGUF llama-2-7b-chat.Q4_K_M.gguf --local-dir ./models --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll need to install the ctransformer library, check [here](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF#:~:text=Python%20using%20ctransformers-,First%20install%20the%20package,0.2.24%20--no-binary%20ctransformers,-Simple%20example%20code) for the option that makes sense for your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF\n",
    "\n",
    "# # Base ctransformers with no GPU acceleration\n",
    "# pip install ctransformers>=0.2.24\n",
    "# # Or with CUDA GPU acceleration\n",
    "# pip install ctransformers[cuda]>=0.2.24\n",
    "# # Or with ROCm GPU acceleration\n",
    "# CT_HIPBLAS=1 pip install ctransformers>=0.2.24 --no-binary ctransformers\n",
    "\n",
    "\n",
    "# # Or with Metal GPU acceleration for macOS systems ---- I am in a MacOS machine so I'll use this option!\n",
    "# !CT_METAL=1 pip install ctransformers --no-binary ctransformers\n",
    "# This is the only package that is not included in the requirements because it will be installed in a different way depending on the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example to load and run a GGUF model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[58244]: Class GGMLMetalClass is implemented in both /Users/greatmaster/miniconda3/envs/oreilly-llama2/lib/python3.10/site-packages/llama_cpp/libllama.dylib (0x11913c250) and /Users/greatmaster/miniconda3/envs/oreilly-llama2/lib/python3.10/site-packages/ctransformers/lib/local/libctransformers.dylib (0x12cc741d0). One of the two will be used. Which one is undefined.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " everyone has heard of chatbots, but what exactly are they? In this article, we will explore the concept of chatbots, how they work, and their potential applications. A chatbot is a computer program that mimics human conversation, either through text or voice interactions. The term \"chatbot\" was coined in 1987, but the technology has been around for much longer. In this article, we will explore the history of chatbots and how they have evolved over time.\n",
      "A large language model is a type of artificial intelligence (AI) model that is trained on a large corpus of text data to generate language outputs that are coherent and natural-sounding. These models have become increasingly popular in recent years due to their ability to generate text that is often indistinguishable from human-written text. In this article, we will explore the inner workings of a large language model, how it is trained, and some of its potential applications.\n",
      "The term \"large language model\" can be somewhat misleading, as these models are not necessarily limited to processing large amounts of data. Rather, they are defined by their ability to learn and generate language based on the data they are\n"
     ]
    }
   ],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "\n",
    "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "#llm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Llama-2-7b-Chat-GGUF\", model_file=\"llama-2-7b-chat.Q4_K_M.gguf\", model_type=\"llama\", gpu_layers=50)\n",
    "llm = AutoModelForCausalLM.from_pretrained(model_path_or_repo_id=\"./models/llama-2-7b-chat.Q4_K_M.gguf\", model_type=\"llama\")\n",
    "print(llm(\"What is a large language model?.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here are ten of the best stretches that you can do at home or in the office:\n",
      "Stretching is an important part of any fitness routine, as it helps improve flexibility, reduce muscle tension, and enhance overall well-being. Incorporating stretching into your daily routine can help you live longer, healthier, and stronger. Here are ten essential stretches that everyone should do to achieve these benefits:\n",
      "1. Neck Stretch: Slowly tilt your head to the side, bringing your ear towards your shoulder. Hold for 30 seconds and repeat on the other side. This stretch helps relieve tension in the neck and improves posture.\n",
      "2. Shoulder Rolls: Roll your shoulders forward and backward in a circular motion. Repeat for 10-15 repetitions to improve mobility in the shoulders.\n",
      "3. Chest Stretch: Place your hands on a wall or door frame and lean forward, stretching your chest. Hold for 30 seconds and repeat several times to improve flexibility in the upper body.\n",
      "4. Arm Circles: Hold your arms straight out to the sides and make small circles with your hands\n"
     ]
    }
   ],
   "source": [
    "print(llm(\"List 10 essential stretches a person should do to live longer, healthier and stronger.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also check out this demo available in the hugging face website for interacting with the Llama2-70B-Chat model:\n",
    "- https://huggingface.co/spaces/ysharma/Explore_llamav2_with_TGI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[source](https://python.langchain.com/docs/integrations/llms/llamacpp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's install the langchain package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are on google colab uncomment this and install langchain\n",
    "# !pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://python.langchain.com/docs/integrations/llms/llamacpp\n",
    "\n",
    "# !CMAKE_ARGS=\"-DLLAMA_METAL=on\" FORCE_CMAKE=1 pip install llama-cpp-python\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's work this out in a step by step way to be sure we have the right answer.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ./models/llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  3820.94 MiB, ( 3822.56 / 10922.67)\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "llm_load_tensors:      Metal buffer size =  3820.93 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   256.00 MiB, ( 4078.56 / 10922.67)\n",
      "llama_kv_cache_init:      Metal KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =    70.50 MiB, ( 4149.06 / 10922.67)\n",
      "llama_new_context_with_model:      Metal compute buffer size =    70.50 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     9.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.context_length': '4096', 'llama.attention.head_count': '32', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '11008', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'general.name': 'LLaMA v2'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "!CT_METAL=1\n",
    "n_gpu_layers = 50  # Change this value based on your model and your GPU VRAM pool.\n",
    "n_batch = 4096  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"./models/llama-2-7b-chat.Q4_K_M.gguf\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ☕\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1: Learn Pragmatic Programming Basics 📚\n",
      "To become a skilled pragmatic programmer, you must first understand the fundamentals of the field. Start by reading books like \"Pragmatic Programmer\" by Andrew Hunt and David Thomas or \"Clean Code\" by Robert C. Martin. These resources will give you a solid foundation in pragmatic programming principles and practices. 📚\n",
      "Step 2: Practice Pragmatism in Your Daily Work 💻\n",
      "The best way to become a great pragmatic programmer is to practice pragmatism in your daily work. This means focusing on delivering working software, writing clean and maintainable code, and constantly improving your craft. As you gain experience, you'll find yourself naturally incorporating pragmatic principles into your workflow. 💻\n",
      "Step 3: Learn from Others 🤝\n",
      "Seek out other pragmatic programmers and learn from their experiences. Attend conferences, join online communities, and participate in code reviews to share knowledge and best practices. By collaborating with"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2910.85 ms\n",
      "llama_print_timings:      sample time =      32.05 ms /   256 runs   (    0.13 ms per token,  7986.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    8123.52 ms /   256 runs   (   31.73 ms per token,    31.51 tokens per second)\n",
      "llama_print_timings:       total time =    8816.87 ms /   257 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the secret to becoming a great pragmatic programmer?',\n",
       " 'text': ' ☕\\n\\nStep 1: Learn Pragmatic Programming Basics 📚\\nTo become a skilled pragmatic programmer, you must first understand the fundamentals of the field. Start by reading books like \"Pragmatic Programmer\" by Andrew Hunt and David Thomas or \"Clean Code\" by Robert C. Martin. These resources will give you a solid foundation in pragmatic programming principles and practices. 📚\\nStep 2: Practice Pragmatism in Your Daily Work 💻\\nThe best way to become a great pragmatic programmer is to practice pragmatism in your daily work. This means focusing on delivering working software, writing clean and maintainable code, and constantly improving your craft. As you gain experience, you\\'ll find yourself naturally incorporating pragmatic principles into your workflow. 💻\\nStep 3: Learn from Others 🤝\\nSeek out other pragmatic programmers and learn from their experiences. Attend conferences, join online communities, and participate in code reviews to share knowledge and best practices. By collaborating with'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "question = \"What is the secret to becoming a great pragmatic programmer?\"\n",
    "llm_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup with OLLAMA and Ollama + Langchain\n",
    "\n",
    "You can download the executable from the Ollama website and use it out of the box:\n",
    "- https://ollama.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ollama\n",
    "# Uncomment below if you haven't run this yet on your local machine or google colab\n",
    "# !ollama pull llama2:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(model='llama2', \n",
    "                       messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'What is a large language model?',\n",
    "  },\n",
    "])\n",
    "\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Ollama with Langchain\n",
    "\n",
    "You can also use `langchain` to use Ollama like shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' A large language model is a type of artificial intelligence (AI) model that is designed to understand and generate human-like text. It\\'s called \"large\" because it typically involves training on a vast amount of data, which can range from millions to billions of words. These models use deep learning techniques, specifically a variant of neural networks called recurrent neural networks or transformers, to learn the statistical patterns in the data and generate coherent and contextually relevant responses. They are capable of performing various natural language processing tasks such as text completion, translation, summarization, and question answering. These models have achieved impressive results in understanding and generating human-like text and have found applications in various fields including customer service, content generation, and language translation.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOllama\n",
    "\n",
    "ollama = ChatOllama(model=\"llama2\")\n",
    "\n",
    "ollama.predict(\"What is a large language model?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up with LM-Studio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LM studio is like an improved version of gpt4all with additional features like automatic evaluation of your machine specs to know whether or not it can run a certain model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets-resources/lm-studio-machine-specs-analysis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANother great feature of LM studio is the local inference server so you can host your own model:\n",
    "\n",
    "![](./assets-resources/lm-studio-local-inf-server.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super easy to download open source models:\n",
    "\n",
    "![](./assets-resources/lm-studio-download.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading it, I can select it for chat easily:\n",
    "\n",
    "![](./assets-resources/lm-studio-select-to-chat.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now you can just chat with that model:\n",
    "\n",
    "![](./assets-resources/lm-studio-chat.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can easily host and run inference on your own models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-4rxck8soi2xctzrc3zkauj\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1707168904,\n",
      "  \"model\": \"/Users/greatmaster/.cache/lm-studio/models/TheBloke/Llama-2-7B-Chat-GGUF/llama-2-7b-chat.Q2_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"I'm a lively soul, with a heart that's whole.\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 16,\n",
      "    \"completion_tokens\": 16,\n",
      "    \"total_tokens\": 32\n",
      "  }\n",
      "}"
     ]
    }
   ],
   "source": [
    "# Load a model, start the server, and run this example in your terminal\n",
    "# Choose between streaming and non-streaming mode by setting the \"stream\" field\n",
    "\n",
    "!curl http://localhost:1234/v1/chat/completions -H \"Content-Type: application/json\" -d \\\n",
    "  '{\"messages\": [{\"role\": \"system\", \"content\": \"Always answer in rhymes.\"},\\\n",
    "    {\"role\": \"user\", \"content\": \"Introduce yourself.\"}\\\n",
    "  ], \\\n",
    "  \"temperature\": 0.7, \\ \n",
    "  \"max_tokens\": -1,\\\n",
    "  \"stream\": false\\\n",
    "}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the results of running the inference above:\n",
    "\n",
    "![](./assets-resources/lm-studio-local-server.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Extra notes on running Llama on your phone\n",
    "\n",
    "[MLC LLM](https://github.com/mlc-ai/mlc-llm) is an open-source project that makes it possible to run language models locally on a variety of devices and platforms, including iOS and Android.\n",
    "\n",
    "For iPhone users, there’s an MLC chat app on the App Store. MLC now has support for the 7B, 13B, and 70B versions of Llama 2, but it’s still in beta and not yet on the Apple Store version, so you’ll need to install TestFlight to try it out. Check out out the [instructions for installing the beta version here](https://llm.mlc.ai/docs/index.html#getting-started)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Note\n",
    "\n",
    "The setup we're going to use will depend on the type of application we are trying to build.\n",
    "\n",
    "But the overall rule for this course will be to use `llama-cpp-python` combined with langchain for ease of use for the usecases we are interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "- https://github.com/abetlen/llama-cpp-python\n",
    "- https://github.com/ggerganov/llama.cpp\n",
    "- https://ai.meta.com/llama/\n",
    "- https://ollama.ai/download\n",
    "- https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF\n",
    "- https://huggingface.co/spaces/ysharma/Explore_llamav2_with_TGI\n",
    "- https://huggingface.co/blog/llama2\n",
    "- https://replicate.com/blog/run-llama-locally\n",
    "- https://ollama.ai/\n",
    "- https://arxiv.org/pdf/2307.09288.pdf \n",
    "- https://ai.meta.com/llama/#resources\n",
    "- https://www.philschmid.de/llama-2\n",
    "- https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-llama2",
   "language": "python",
   "name": "oreilly-llama2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
