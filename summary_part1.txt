Here are the main points summarized in concise bullet points:

**Introduction**

* Meta released Llama 3 models with 8 billion and 70 billion parameters
* A 405 billion parameter model is expected to be released soon

**Key Features and Benchmarks**

* Both models outperform previous ones, including Llama 2
* Text-only models trained on over 15 trillion tokens
* Competitive performance in benchmarks against other leading models
* Enhanced with a new tokenizer and grouped query attention (GQA)

**Context Length and Training Details**

* Context length: 8192
* Trained using dense autoregressive transformers with GQA

**Human Evaluation and Safety**

* Human evaluation shows Llama 3 outperforms previous models
* System-level safety approach to ensure safe and useful responses
* Conducted red teaming and benchmarked against Cyberseceval 2 and mlcommons AI safety benchmarks

**Intended Use**

* Designed for commercial and research use in English
* Available on major cloud providers: AWS, GCP, Azure, and others

